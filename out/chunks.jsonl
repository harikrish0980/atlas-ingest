{"chunk_id": "sha256:4c899dc54c8f170724ef0a7d6500a1e593a73200800c2422e7564ee71f691df3", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 0, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation Yidong Ouyang1, Liyan Xie2, Hongyuan Zha3, and Guang Cheng1 1Department of Statistics, University of California, Los Angeles 2Department of Industrial and Systems Engineering, University of Minneasota 3School of Data Science, The Chinese University of Hong Kong, Shenzhen Abstract Diffusion models and flow matching have demonstrated remarkable success in text-to-image generation. While many existing alignment methods primarily focus on fine-tuning pre-trained generative models to maximize a given reward function, these approaches require extensive computational resources and may not generalize well across different objectives. In this work, we propose a novel alignment framework by leveraging the underlying nature of the alignment problem—sampling from reward-weighted distributions—and show that it applies to both diffusion models (via score guidance) and flow matching models (via velocity guidance). The score function (velocity field) required for the reward-weighted distribution can be decomposed into the pre-trained score (velocity field) plus a conditional expectation of the reward. For the alignment on the diffusion model, we identify a fundamental challenge: the adversarial nature of the guidance term can introduce undesirable artifacts in the generated images. Therefore, we propose a finetuning-free framework that trains a guidance network to estimate the conditional expectation of the reward. We achieve comparable performance to finetuning-based models with one-step generation with at least a 60% reduction in computational cost. For the alignment on flow matching, we propose a training-free framework that improves the generation quality without additional computational cost. 1 Introduction Diffusion models and flow matching have achieved impressive performance in text-to-image generation, as demonstrated by state-of-the-art models such as Imagen [36], DALL-E 3 [4], and Stable Diffusion [35]. These models have been proven capable of generating high-quality, creative images even from novel and complex text prompts. Inspired by Reinforcement Learning from Human Feedback (RLHF) [28], many alignment approaches leverage preference pairs to fine-tune models for generating samples that align with task-specific objectives. RLHF-type methods typically learn a reward function and use the policy gradients to update the model [20, 12, 5, 10, 7, 17, 24, 18]. On the other hand, Direct Preference Optimization (DPO)-type methods directly", "quality": {"lang": "en", "gibberish_score": 0.0012, "char_len": 2547, "word_len": 350}, "dedupe": {"exact_hash": "df4877353bcaea1430040c614a91604e93ee846d4d0b687a66cf0b964a83ac61", "simhash64": 10658910156253156614}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:5a5cf862d6338c8842c0172db87b647754be160e20820064ad25b1e9be5dc633", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 1, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "alignment approaches leverage preference pairs to fine-tune models for generating samples that align with task-specific objectives. RLHF-type methods typically learn a reward function and use the policy gradients to update the model [20, 12, 5, 10, 7, 17, 24, 18]. On the other hand, Direct Preference Optimization (DPO)-type methods directly optimize the model to adhere to human preferences, without requiring explicit reward modeling or reinforcement learning [34, 49, 53, 22, 54]. Despite their effectiveness, these approaches require modifying model parameters through finetuning, which comes with several potential limitations. For example, fine-tuning for new reward functions is computationally expensive and often requires carefully designed training strategies; otherwise, optimizing on a limited set of input prompts can limit generalization to unseen prompts. More importantly, existing fine-tuning approaches do not fully exploit the structure of the alignment 1 arXiv:2602.00413v1 [stat.ML] 31 Jan 2026 Figure 1: Qualitative comparison with Vanilla SDXL, Diffusion-DPO, and SPO. Our method achieves better aesthetic quality and stronger alignment with the text prompt. Prompts are provided in the Appendix C.3. problem. Instead, they typically apply Low-Rank Adaptation (LoRA) to optimize model weights for a specific reward function [16], which may not be the most efficient strategy. In contrast, plug-and-play alignment methods integrate new objectives without modifying the underlying model parameters, significantly reducing computational costs while adapting flexibly to different reward functions. In this paper, we cast alignment for both diffusion models and flow matching models as a unified sampling problem from reward-weighted distributions. Under this formulation, the key object needed for sampling—the new score function for diffusion or the new velocity field for flow matching—can be written as the corresponding pre-trained quantity plus an additional reward-driven guidance term. For diffusion models, the guidance term admits an adversarial nature flaw, i.e., the guidance is the gradient of the log conditional expectation of the reward. Directly using the gradient of high dimensional input space can lead to undesirable artifacts in the generated images. To address this issue, we propose a finetuning-free alignment method that trains a lightweight guidance network to estimate the required conditional expectation, together with a regularization strategy that stabilizes", "quality": {"lang": "en", "gibberish_score": 0.0008, "char_len": 2510, "word_len": 350}, "dedupe": {"exact_hash": "084a33bb13b0d9c613baef1658f2f62ebd17b032eec3c87322d08ca5b24c1202", "simhash64": 11229741340305117446}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:408bb4e711befb414884d09ec3b2e0f42f1c2d84b721d8f14ff876a5d474f0d2", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 2, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "of the reward. Directly using the gradient of high dimensional input space can lead to undesirable artifacts in the generated images. To address this issue, we propose a finetuning-free alignment method that trains a lightweight guidance network to estimate the required conditional expectation, together with a regularization strategy that stabilizes the guidance landscape. We evaluate the effectiveness of the proposed method on four widely used criteria for text-to-image generation, and the proposed method achieves comparable performance to finetuning-based models in one-step generation while reducing computational cost by at least 60%. For flow matching, we derive the exact form of velocity guidance and further propose a trainingfree estimator that directly computes the guidance term without additional model fine-tuning. The proposed method improves the generation quality without additional training overhead. 2 2 Preliminaries In this section, we begin with a brief overview of diffusion models and flow matching in Section 2.1 and Section 2.2. We then review existing techniques for aligning pre-trained models with human preferences, and decompose the alignment procedure into two key components: reward learning for modeling human preferences in Section 2.3 and the alignment methods in Section 2.4. 2.1 Diffusion Models Diffusion generative models are characterized by their forward and backward processes [15, 46]. The forward process gradually injects Gaussian noise into samples x0 from the data distribution following the stochastic differential equation: dxt = f(xt, t)dt + g(t)dw, t ∈[0, T], (1) where w is the standard Brownian motion, f(·, t) : Rd →Rd is a drift coefficient, and g(·) : R →R is a diffusion coefficient. We use pt(x) to denote the marginal distribution of xt at time t. And we can use the time reversal of (1) for generation, which admits the following form [1]: dxt = f(xt, t) −g(t)2∇x log pt(x) dt + g(t)dw, (2) where w is a standard Brownian motion when time flows backwards from T to 0, and dt is an infinitesimal negative time step. The score function of each marginal distribution ∇x log pt(x) needs to be estimated by the following score matching objective: min θ Et n λ(t)Ept(xt) h ∥sθ(xt,", "quality": {"lang": "en", "gibberish_score": 0.0063, "char_len": 2240, "word_len": 350}, "dedupe": {"exact_hash": "daea0d400917b607a905fb6e47e7dcdd8d66b3532d4d9151c2a2345a82b83c78", "simhash64": 10861923983248184582}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:bccbd6093e1c136149236acb2395a42d21e8d97f62cdda4b4e42144746cdee69", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 3, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "where w is a standard Brownian motion when time flows backwards from T to 0, and dt is an infinitesimal negative time step. The score function of each marginal distribution ∇x log pt(x) needs to be estimated by the following score matching objective: min θ Et n λ(t)Ept(xt) h ∥sθ(xt, t) −∇xt log pt(xt)∥2 2 io , (3) where λ(t) : [0, T] →R>0 is a positive weighting function, t is uniformly sampled over [0, T]. The latent diffusion model [35, 32] further extends diffusion models to text-to-image generation. They use an image encoder E that maps x into a latent representation and use a text encoder τ that maps the prompts y into an embedding as the condition. 2.2 Flow Matching Flow matching models learn a time-dependent velocity field that transports a simple base distribution to the data distribution [23] via the probability flow ODE dxt dt = vϕ(xt, y, t), t ∈[0, 1], where vϕ : Rd →Rd is a learnable velocity field. Unlike diffusion models, we denote x0 as a sample from a base distribution (e.g., standard Gaussian) and x1 as a sample from the data distribution. The flow matching objective minimizes the discrepancy between the model vector field and the oracle velocity field along the trajectory: L(θ) = Ext∼pt(xt|x0,x1),t∼U[0,1] h vϕ(xt, t) −v(xt, y, t) 2 2 i , (4) where xt is a linear interpolation between x0 and x1, and v(xt, y, t) is the oracle velocity field. 3 2.3 Reward Learning The Bradley-Terry (BT) model [6], and the more general Plackett-Luce ranking models [31, 26], are commonly used to model preferences. Given a prompt y and a pair of responses xw ≻xl | y, where xw denotes the winning response and xl denotes the losing response under the preference of humans. The BT model depicts the preference distribution as p (xw ≻xl | y) = exp (r (xw, y)) exp (r (xw, y)) + exp (r (xl, y)), where r(x, y) denotes the reward model and can be learned by the following maximum likelihood objective, min ϕ −E(xw,xl,y)∼D [log σ (r (xw, y) −r", "quality": {"lang": "en", "gibberish_score": 0.0123, "char_len": 1959, "word_len": 350}, "dedupe": {"exact_hash": "9a4a123eea23cc2acbd0e8bb0479f65a132a13308f5b543b0d47fcbbb4b613f9", "simhash64": 6106070000740454790}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0ab742086b79a17ae809ad8146460489aa0f1913afca8e6e3a4870479cfcbe55", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 4, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "depicts the preference distribution as p (xw ≻xl | y) = exp (r (xw, y)) exp (r (xw, y)) + exp (r (xl, y)), where r(x, y) denotes the reward model and can be learned by the following maximum likelihood objective, min ϕ −E(xw,xl,y)∼D [log σ (r (xw, y) −r (xl, y))] , (5) where D = {x(i) w , x(i) l , y(i)}N i=1 is the offline preference dataset and σ denotes the logistic function. 2.4 Alignment Building on the success of alignment techniques for finetuning large pre-trained models, many studies have explored aligning diffusion models and flow matching with human preferences. We review these approaches in the following. Reinforcement Learning from Human Feedback. This type of works [20, 51, 12, 5, 10] finetune the pre-trained model πref by policy gradient objectives [17, 18]. In particular, the fine-tuned model πθ is obtained by solving the following optimization problem: max πθ Ey∼Dprompt,x∼πθ(x|y) [r(x, y)] −βDKL [πθ(x | y)∥πref(x | y)] , (6) where Dprompt denotes the prompt dataset. This type of method requires a pre-trained reward function for policy optimization [40]. Direct Preference Optimization. Rafailov et al. [34] propose not to explicitly learn the reward function. They start with the analytic solution of (6) as the energy-guided form, πr(x | y) = 1 Z(y)πref(x | y) exp 1 β r(x, y) , (7) where Z(y) = R πref (x | y) exp 1 βr(x, y) dx is the partition function. Therefore, they can reparameterize the reward function r(x, y) as r(x, y) = β log πr(x | y) πref (x | y) + β log Z(y). (8) Plugging (8) into (5) yields the objective of DPO-type methods: min −E(xw,xl,y)∼D log σ β log πθ (xw | y) πref (xw | y) −β log πθ (xl | y) πref (xl | y) . (9) 4 3 Proposed Framework for Diffusion Model In this section, we introduce a finetuning-free frameworks to directly sample from the reward-guided distribution for diffusion models. We begin by introducing the methodology formulation in Section 3.1. We then provide an in-depth analysis of several vanilla methods for calculating", "quality": {"lang": "en", "gibberish_score": 0.017, "char_len": 1996, "word_len": 350}, "dedupe": {"exact_hash": "1a5c0f545d5af381371fce9f981845829e62a5e6991c823aabdaab609e061531", "simhash64": 4953126505901048198}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:30322d966f164ae85ad9dd9ee696ed157b0531819ff90adeb5d95d0dbcf1fa70", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 5, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "y) . (9) 4 3 Proposed Framework for Diffusion Model In this section, we introduce a finetuning-free frameworks to directly sample from the reward-guided distribution for diffusion models. We begin by introducing the methodology formulation in Section 3.1. We then provide an in-depth analysis of several vanilla methods for calculating the guidance in Section 3.2. We highlight that these vanilla guidance methods exhibit adversarial guidance, which generates undesirable artifacts and worsens performance, particularly in text-to-image generation. Then, we present an enhanced method in Section 3.3 that alleviates the problem. 3.1 Methodology Formulation Inspired by previous works from transfer learning [29], we consider preference learning in terms of transferring a pre-trained diffusion model to adapt to the given preference data. To this end, we propose a finetuning-free alignment method for the diffusion models. Instead of using RLHF-type (like (6)) or DPO-type (like (9)) alignments, we propose to directly sample from the reward-weighted distribution πr(x|y) in (7) leveraging the relationships between score functions in the following Theorem. Tweedie’s formula Directly backpropagate No guidance Alpha=0.1 Alpha=0.5 Alpha=1.0 Alpha=2.0 No guidance Alpha=1.0 Alpha=10 Alpha=20 Alpha=50 Figure 2: Illustration of the Adversarial Nature of Guidance. When the strength of the guidance is too small, there is little difference between the generated images with or without guidance. However, as the magnitude of the guidance increases (from left to right), undesirable artifacts become more pronounced. The prompt is ”A 3D Rendering of a cockatoo wearing sunglasses. The sunglasses have a deep black frame with bright pink lenses. Fashion photography, volumetric lighting, CG rendering”. Theorem 3.1. Let the conditional distribution of reference diffusion model πref(x|y) be denoted as distribution p and the reward-weighted distribution πr(x|y) defined in (7) as distribution q. Under some mild assumption of the forward noising process detailed in Appendix A, let ϕ∗be the optimal solution for the conditional diffusion model trained on target domain q(x0, y), i.e., ϕ∗= arg min ϕ Et n λ(t)Eqt(xt,y) h sϕ(xt, y, t) −∇xt log qt(xt|y) 2 2 io , 5 then sϕ∗(xt, y, t) = ∇xt log pt(xt|y) | {z } pre-trained conditional model on source + ∇xt", "quality": {"lang": "en", "gibberish_score": 0.0124, "char_len": 2347, "word_len": 350}, "dedupe": {"exact_hash": "595def020edd2de96e23a60c5547dea8d18a889f9ccdcc0b011f5a183a7e4434", "simhash64": 15180876026433356038}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:11db69665ebf46746e43888abd743d48f15d5f8e93efe03e6281f976e69f9884", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 6, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "solution for the conditional diffusion model trained on target domain q(x0, y), i.e., ϕ∗= arg min ϕ Et n λ(t)Eqt(xt,y) h sϕ(xt, y, t) −∇xt log qt(xt|y) 2 2 io , 5 then sϕ∗(xt, y, t) = ∇xt log pt(xt|y) | {z } pre-trained conditional model on source + ∇xt log Ep(x0|xt,y) exp( 1 β r(x0, y)) | {z } conditional guidance . (10) The proof can be found in Appendix A. Based on (10), we can calculate the additional guidance term rather than finetuning the text-to-image generative model. In general, the guidance term in (10) is not straightforward to compute as we need to sample from p(x0|xt, y) for each xt in the generation process. In the following, we first discuss some existing ways to calculate the guidance term. 3.2 Vanilla Method to Compute the Guidance Term M1: Direct backpropagate through diffusion process. The first method directly backpropagates through diffusion process to calculate ∇xt log Ep(x0|xt,y)[exp(r(x0, y)/β)] for fine-tuning the diffusion model. In [45], the authors propose an unbiased Monte Carlo estimation: ∇xt log Ep(x0|xt,y) h exp 1 β r(x0, y) i ≈∇xt log 1 n n X i=1 exp 1 β r(xi 0, y) , where xi 0 denotes the i-th sample drawn from p(x0|xt, y). However, this Monte Carlo estimation significantly increases memory costs, especially in text-to-image generation. Inspired by recent studies [10], we can borrow the same techniques, e.g., accumulated gradients along the diffusion process using techniques such as low-rank adaptation (LoRA) [16] and truncation or gradient checkpointing [33, 10], to alleviate the memory cost of backpropagating through the diffusion process for calculating the guidance term. We can further reduce the memory cost by using the few-step diffusion model as the reference model. Despite these techniques, the memory requirements remain higher compared to the proposed approach. M2: Approximate and apply Tweedie’s formula. The second method first approximates the guidance term by [9]: ∇xt log Ep(x0|xt,y) exp 1 β r(x0, y) ≈1 β ∇xtr Ep(x0|xt,y)[x0], y . (11) Then, Tweedie’s formula is further applied by [2, 9, 56]: E [x0 | xt, y] = xt + σ2", "quality": {"lang": "en", "gibberish_score": 0.0181, "char_len": 2100, "word_len": 350}, "dedupe": {"exact_hash": "8f5da3f245141c9c2a33983b2a487dec98fefb6976fd30356506b03d0a3369db", "simhash64": 10717729630336696646}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:06ecc5ddd193633b7ac3cca75c6c050b8fced24402f6e225002f7a5a6acec232", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 7, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "M2: Approximate and apply Tweedie’s formula. The second method first approximates the guidance term by [9]: ∇xt log Ep(x0|xt,y) exp 1 β r(x0, y) ≈1 β ∇xtr Ep(x0|xt,y)[x0], y . (11) Then, Tweedie’s formula is further applied by [2, 9, 56]: E [x0 | xt, y] = xt + σ2 t ∇xt log pt (xt|y) . However, as noted in [25, 45], the approximation used in (11) is biased, leading to an incorrect calculation of the guidance term. In the following, we empirically evaluate the effectiveness of these methods for aligning text-toimage generation tasks. We first identify a previously overlooked issue that contributes to suboptimal alignment performance. Figure 2 illustrates the performance of two vanilla methods under the guidance of PickScore [19], a reward function that evaluates whether the generated images align with human aesthetic and semantic preferences. The x-axis represents the strength of the guidance term, denoted by α 1. Our experiments reveal that tuning this hyperparameter presents significant 1Although there is no α in (10), many guidance methods [25, 45] add this hyperparameter in practice to balance the strength of the guidance term with the score. 6 Table 1: Comparison of finetuning-free alignment algorithms on diffusion models. Our method uniquely provides theoretical guarantees for the correct form for guidance with a step size guarantee. Method Classifier Guidance Direct backpropagate (M1) Tweedie’s formula (M2) Ours Formulation 1 β∇xtr xt, y ∇xt log 1 n Pn i=1 exp 1 βr(xi 0, y) 1 β∇xtr Ep(x0|xt,y)[x0], y ∇xt log hψ∗(xt, y, t) Unbiased ✗ ✓ ✗ ✓ Step size guarantee ✗ ✗ ✗ ✓ challenges. Insufficient values of α produce results indistinguishable from unguided generation, while excessive values introduce substantial artifacts that degrade image quality. We attribute this phenomenon to the adversarial nature of the guidance mechanism, as observed in prior work [43]. In (10), the guidance term is directly added to the estimated score. If the landscape is not smooth or does not behave well2, the adversarial nature of the guidance can lead to undesirable artifacts in the generated images. To address these limitations, our proposed framework provides theoretical guarantees", "quality": {"lang": "en", "gibberish_score": 0.0127, "char_len": 2200, "word_len": 350}, "dedupe": {"exact_hash": "8d2ad0771a4290b32861504b81932c03eea8a23a151b5b043a43159d8cef93c5", "simhash64": 11429368396331204934}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6339ed89421ba793f6fa98dab09c2e979fd60fcc009d06aed75277e1766d975e", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 8, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "work [43]. In (10), the guidance term is directly added to the estimated score. If the landscape is not smooth or does not behave well2, the adversarial nature of the guidance can lead to undesirable artifacts in the generated images. To address these limitations, our proposed framework provides theoretical guarantees for generating properly aligned distributions with a fixed strength parameter α = 1. Furthermore, we develop an additional regularization technique for training the guidance network that mitigates these instability issues. 3.3 Proposed Finetuning-free Guidance for Diffison Models We first utilize the following trick to calculate the conditional expectation, similar to previous works [29, 25]. Lemma 3.2. For a neural network hψ (xt, y, t) parameterized by ψ, define the objective Lguidance(ψ) := Ep(x0,xt,y) \" hψ (xt, y, t) −exp( 1 β r(x0, y)) 2 2 # , (12) then its minimizer ψ∗= arg min ψ Lguidance(ψ) satisfies: hψ∗(xt, y, t) = Ep(x0|xt,y) exp( 1 β r(x0, y)) . By Lemma 3.2, we can instead estimate the value Ep(x0|xt,y)[exp(r(x0, y)/β)] using the guidance network hψ∗obtained by minimizing the objective function Lguidance(ψ), which can be approximated by easy sampling from the joint distribution p(x0, xt, y). Then, the estimated score function for the aligned diffusion model can be calculated as follows: sϕ∗(xt, y, t) = ∇xt log p(xt|y) | {z } pre-trained model on source + ∇xt log hψ∗(xt, y, t) | {z } guidance network . (13) To alleviate the adversarial nature of the guidance, we can adopt the consistency regularization Lconsistence to learn the guidance network hψ∗better, i.e., the gradient of Lconsistence (xt, y, t) with respect to xt should match the score in preferred data. The key point of this regularization is that we cannot easily change the landscape of a given predetermined reward function, but we can 2We use landscape to describe the change of reward given the change of images. 7 regularize the landscape of the learned guidance network to ensure the generation of high-quality images. ψ∗= arg min ψ Lconsistence := Eq(x0,y)Eq(xt|x0) h ∇xt log p(xt|x0, y) + ∇xt log hψ (xt, y, t) −∇xt log", "quality": {"lang": "en", "gibberish_score": 0.0168, "char_len": 2140, "word_len": 350}, "dedupe": {"exact_hash": "2c5321dd30571e0b51393d0ba914f032ac6f69bac14a6fd16f58d13184fadfe8", "simhash64": 10857429179444871495}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f19a997a23321a86fcbc1b960b8bea4c550819fb957cffb3170a530ad18a0161", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 9, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "use landscape to describe the change of reward given the change of images. 7 regularize the landscape of the learned guidance network to ensure the generation of high-quality images. ψ∗= arg min ψ Lconsistence := Eq(x0,y)Eq(xt|x0) h ∇xt log p(xt|x0, y) + ∇xt log hψ (xt, y, t) −∇xt log q(xt|x0, y) 2 2 i . (14) Combining the consistency regularization terms together with the original guidance loss in (12), the final learning objective for the guidance network can be described as follows: ψ∗= arg min ψ {Lguidance + η Lconsistence}, (15) where η ≥0 are hyperparameters that control the strength of additional regularization, which also enhances the flexibility of our solution scheme. 3.4 Further Improvement to One-step Generation The training objectives in (12) and (14) are agnostic to the reference model, indicating that we can use any pre-trained diffusion model with any reward function, whether differentiable or not. Motivated by the computational efficiency of one-step generative models in practical applications, we further present a straightforward approach for applying our proposed finetuning-free guidance to one-step text-to-image models. Specifically, instead of sampling t uniformly from [0, T], we can simply set t = T. This small modification offers several advantages. First, while one-step diffusion models may not perform as well as few-step (2–4 step) models [37], we empirically find that with additional guidance, their performance improves significantly, as presented in Section 5.3. Second, as the guidance network hψ now becomes time-independent, we empirically observe that hψ is easy to train—with ten training epochs on the Pick-a-Pic V1 dataset, our guidance network produces high-quality images, which can be found in Section 5.2. We summarize the overall learning pipeline in Algorithm 1 in the Appendix. And we leave another two gradient-free designs for diffusion models in Appendix B. 4 Proposed Framework for Flow Matching Training-free Alignment Framework for Flow Matching Given that state-of-the-art models are grounded in Diffusion Transformers [30] and flow matching [23], we present the exact form of flow-matching guidance in the theorem below. Theorem 4.1. Let ϕ∗ q be the optimal solution for the conditional flow matching model trained on", "quality": {"lang": "en", "gibberish_score": 0.0092, "char_len": 2290, "word_len": 350}, "dedupe": {"exact_hash": "018508103ee4aac600548b6640ef18d9ca5f5cf85214eb21a45d45650b460850", "simhash64": 11141147091158812039}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:678d112993d7e1c1c6f7d8bb65862e5ce5c58053f84396dc7e680a85e951ae05", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 10, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "Matching Training-free Alignment Framework for Flow Matching Given that state-of-the-art models are grounded in Diffusion Transformers [30] and flow matching [23], we present the exact form of flow-matching guidance in the theorem below. Theorem 4.1. Let ϕ∗ q be the optimal solution for the conditional flow matching model trained on target domain q(x1, y) (where x1 are sampled from data distribution, vq(xt, y, t) denotes the oracle velocity field on target distribution), i.e., ϕ∗ q equals arg min ϕ Et n Eqt(xt,y) h vϕ(xt, y, t) −vq(xt, y, t) 2 2 io , then vϕ∗ q(xt, y, t) = vϕp(xt, y, t) + Ex1∼p1|t(x1|xt,y) R(x1, xt, y) −1 vt(xt | x1, y) , (16) where R(x1, xt, y) = exp 1 βr(x1, y) Ex′ 1∼p1|t(x1|xt,y) h exp 1 βr(x′ 1, y) i. 8 Table 2: Benchmark comparison of different methods on text-to-image alignment. Results are grouped by base model. Type Method PickScore HPSV2 ImageReward Aesthetic Training GPU Hour Base Model: SDXL Baseline SDXL 21.95 26.95 0.5380 5.950 – Training-free Direct backpropagate 21.84 27.53 0.5870 5.922 – Tweedie’s formula 22.34 28.76 0.9501 6.002 – Finetuning-based Diff.-DPO 22.64 29.31 0.9436 6.015 4800 SPO 23.06 31.80 1.0803 6.364 234 Finetuning-free Ours 23.08 32.12 1.0625 6.452 92 Base Model: SD3.5 Large Turbo Baseline SD3.5 Large Turbo 22.30 30.29 1.0159 6.5190 – Finetuning-free Ours 23.14 32.31 1.1025 6.5280 – Estimation of the Guidance Term for Flow Matching Different from the guidance term of diffusion models in (10), the guidance for flow matching in (16) does not have the adversarial problem. The guidance term is a conditional expectation without the gradient operator. To enable fast sampling, we would like to use importance sampling to convert the conditional expectation under p(x1 | xt, y) into an expectation under p(x1 | y). After the detailed derivation in A.2, we can calculate the guidance term of flow matching by Ex1∼p(x1|y) \" exp 1 βr(x1, y) Ex1 h exp 1 βr(x1, y) pt|1(xt|x1,y) Ex1[pt|1(xt|x1,y)] i −1 ! vt(xt | x1, y) pt|1(xt | x1, y) Ex1[pt|1(xt | x1, y)] # . Therefore, we do not need to sample x1 with", "quality": {"lang": "en", "gibberish_score": 0.0188, "char_len": 2070, "word_len": 350}, "dedupe": {"exact_hash": "be41c40fb9fa5f335442562280badddf6eca547feae4bbb47e71c6754897e099", "simhash64": 10713243622910575878}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:dacda03794382fae406a1c1278d9b43f79f19f359bacdb55394c9d8c4fcb6ae7", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 11, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "we can calculate the guidance term of flow matching by Ex1∼p(x1|y) \" exp 1 βr(x1, y) Ex1 h exp 1 βr(x1, y) pt|1(xt|x1,y) Ex1[pt|1(xt|x1,y)] i −1 ! vt(xt | x1, y) pt|1(xt | x1, y) Ex1[pt|1(xt | x1, y)] # . Therefore, we do not need to sample x1 with multiple function evaluations, but just sample from the marginal data distribution. Compared with the finetuning-free method proposed in (13), this formulation is training-free and offers greater computational efficiency. 5 Experimental Results In this section, we present a comprehensive experimental evaluation, demonstrating the effectiveness of our two frameworks for sampling directly from reward-guided distributions. We first outline our experimental setup and evaluation criteria in Section 5.1, followed by benchmark results against state-of-the-art methods in Section 5.2. Finally, we provide an in-depth ablation study that validates our key theoretical claims and demonstrates the superior performance of our guidance network in Section 5.3. 5.1 Experimental Setup For the experiments on diffusion models, we follow the official configurations recommended for SPO [22], Diffusion-DPO [49], and MAPO [42]. Diffusion-DPO and MAPO are fine-tuned on the Pick-a-Pic V2 dataset, which contains over 800k image preference pairs. In contrast, SPO is fine-tuned online using 4k text prompts (without images) randomly selected from Pick-a-Pic V1. Our method trains the guidance network offline using 583k image preference pairs from Pick-a-Pic V1. Overall, our method and the competing models in the text-to-image alignment benchmark 9 are trained on comparable datasets, allowing for a fair comparison. We adopt Stable Diffusion XL (SDXL)-Turbo [38] as the reference model for one-step text-to-image generation. For the experiments on flow matching, we adopt the state-of-the-art SD3.5 Large Turbo [11] as the backbone. The official recommendation for the number of sampling steps is four to eight, and we use four steps for all experiments. Implementation Details. In the following, we provide the training details for the guidance network of the diffusion model. Since the guidance network takes noisy images xT and prompts y as input and outputs a scalar value, we adopt the same variational autoencoder (VAE), tokenizer, and text encoder from the reference diffusion model", "quality": {"lang": "en", "gibberish_score": 0.006, "char_len": 2328, "word_len": 350}, "dedupe": {"exact_hash": "b4a6fd567f27675fdde64b06157e75f117c0139e8273ff701f06bde35a1a8620", "simhash64": 11145668351960269062}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c27a8a1b9c4286e4c237da6ebba8b0c2ae6b5f617b42defb46e7b25a974db2e2", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 12, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "In the following, we provide the training details for the guidance network of the diffusion model. Since the guidance network takes noisy images xT and prompts y as input and outputs a scalar value, we adopt the same variational autoencoder (VAE), tokenizer, and text encoder from the reference diffusion model for encoding image and text. Consequently, the trainable parameters of our guidance network are quite small. In practice, we adopt two convolutional layers for processing VAE-encoded feature maps and a five-layer multi-layer perceptron (MLP) to project the image and text embedding to a scalar. The total parameter size of the guidance network is only 72 MB, making it lightweight and easy to train. We train the guidance network on the Pick-a-Pic training dataset for 10 epochs with batch size 32, Adam optimizer, learning rate 1e-3, and hyperparameters η = 1. Evaluation Criterion. Following established evaluation protocols [49, 22], we report quantitative results using 500 validation prompts from the validation unique split of Pick-a-Pic. We adopt four evaluation criteria to evaluate different aspects of image quality. PickScore [19] measures overall human preference by aggregating judgments on aesthetic appeal, coherence, and realism. HPSV2 [50] assesses prompt adherence, ensuring the generated image accurately reflects the given textual description. ImageReward [51] quantifies human preference based on fine-grained attributes such as composition, detail preservation, and semantic relevance. Lastly, the aesthetic evaluation model from LAION [39] focuses on visual appeal, capturing factors such as color harmony, style, and artistic quality. 5.2 Experimental Results As shown in Table 2, our method surpasses baseline approaches across four evaluation criteria, demonstrating the effectiveness of the two proposed frameworks in enhancing text-to-image alignment. The improvements are observed in both perceptual quality and semantic coherence, indicating that our guidance network successfully refines image generation to better match textual descriptions. This performance gain highlights the advantages of our lightweight architecture and the optimization strategy used during training. Figure 1 provides a qualitative comparison with baseline methods, further illustrating the superior visual fidelity and text alignment achieved by our approach. 5.3 Ablation study In this section, we first verify the advantages of our", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2451, "word_len": 350}, "dedupe": {"exact_hash": "61cfb833641f21f92ceaa3aaa51b34facc911cbaf45c99353476a2f239004c9f", "simhash64": 10591373685042938118}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:be2c2a71ff7da520cf99012feb90a4a05b73a35e9bd44da1268346e79fc9cb4d", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 13, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "gain highlights the advantages of our lightweight architecture and the optimization strategy used during training. Figure 1 provides a qualitative comparison with baseline methods, further illustrating the superior visual fidelity and text alignment achieved by our approach. 5.3 Ablation study In this section, we first verify the advantages of our proposed method against other finetuning-free guidance methods as summarized in Table 1. We then analyze the impact of few-step (2–4 step) generation compared to one-step generation, highlighting how our guidance term significantly enhances performance. As illustrated in Figure 3, vanilla guidance methods struggle to induce meaningful improvements in generated images, even with carefully tuned guidance strength. Increasing the guidance parameter α often leads to undesirable artifacts rather than quality improvements. In contrast, our method effectively enhances image generation by leveraging a regularized guidance network, demonstrating its ability to refine scene details and improve alignment with input prompts. 10 No guidance 1-step No guidance 2-step No guidance 3-step Tweedie’s 1-step Backpropagate 1-step Our 1-step Figure 3: Effectiveness of the proposed method for diffusion models: The results demonstrate that 2step and 3-step generation significantly improve the quality of the generated images compared to onestep generation. While two vanilla guidance methods (Tweedie’s formula or directly backpropagation summarized in Section 3.2) fail to produce meaningful changes in the scene despite appropriate guidance strength, our method successfully achieves this enhancement. The prompt is “A photo of a frog holding an apple while smiling in the forest”. Table 3: Ablation study comparing the performance of our method with no guidance and two vanilla guidance methods under one-step and multi-step generation. Our method outperforms all baselines, which demonstrates the effectiveness of our guidance network in refining image quality and prompt alignment. Method PickScore Ours (1 step) 23.08 No guidance (1 step) 22.14 Tweedie’s (1 step) 22.34 Backpropagate (1 step) 21.84 No guidance (2 steps) 22.64 No guidance (3 steps) 22.56 To further explore this, we examine the performance of our method against two vanilla guidance techniques, Tweedie’s and Backpropagate, as well as the no guidance baseline, all under a onestep sampling condition. As shown in Table 3,", "quality": {"lang": "en", "gibberish_score": 0.0029, "char_len": 2435, "word_len": 350}, "dedupe": {"exact_hash": "6da8dab6bccf90efbee02200507726f8c324915892bf029c39233a89f77119f3", "simhash64": 11147566040847319314}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ad236eb1b03e49ec78701ab1580e028f6a784a357c8e7ca7b11d866a3a924fa5", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 14, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "(1 step) 21.84 No guidance (2 steps) 22.64 No guidance (3 steps) 22.56 To further explore this, we examine the performance of our method against two vanilla guidance techniques, Tweedie’s and Backpropagate, as well as the no guidance baseline, all under a onestep sampling condition. As shown in Table 3, our method achieves the highest PickScore. This demonstrates that our regularized guidance network provides a substantial improvement over no guidance scenario and traditional methods. Consistent with prior studies, increasing the number of steps from one to two or three results in improved image quality, as shown in Figure 3 and Table 3. However, our method enables one-step generation to achieve performance even better than 2- or 3-step generation, highlighting the power of our guidance network. In Appendix C, we include the sensitive analysis of the regularization strength. 6 Related Work Existing alignment methods can be broadly categorized into two approaches: RLHF-based method that uses policy gradient to update the diffusion models and flow matching, and DPO-based methods that use a parametrization trick to update the diffusion models without explicitly learning the 11 reward function. RLHF-based alignment of diffusion model and flow matching. Lee et al. [20] first train a reward model to predict human feedback and adopt a reward-weighted finetuning objective to align the diffusion model. In [12, 5], diffusion models are updated using policy gradient algorithms under Kullback–Leibler (KL) constraints. Clark et al. [10] propagate gradients of the reward function through the full sampling procedure, and reduce memory costs by adopting low-rank adaptation (LoRA) [16] and gradient checkpointing [8]. In [24, 21, 52, 14], the authors improve GRPO [41] for the alignment of flow matching. DPO-based alignment of diffusion model. A line of work [49, 53] directly applies DPO [34] to align the diffusion model with human preference. Liang et al. [22] propose a step-aware preference model and a step-wise resampler to align the preference optimization target with the denoising performance at each timestep. Yang et al. [54] take on a finer dense reward perspective and derive a tractable alignment objective that emphasizes the initial steps. Training-free guidance. This", "quality": {"lang": "en", "gibberish_score": 0.0009, "char_len": 2298, "word_len": 350}, "dedupe": {"exact_hash": "e6d05ff21ff5a3a8bdce52acf29038c8ba14d29869ddb7f3e355af0355d4aa45", "simhash64": 11221849045612742406}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1a72adcbc6cf2bc977e945d4d75d1aa5a0e0c5ba7bc510984ae93e42b7c4e2b1", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 15, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "et al. [22] propose a step-aware preference model and a step-wise resampler to align the preference optimization target with the denoising performance at each timestep. Yang et al. [54] take on a finer dense reward perspective and derive a tractable alignment objective that emphasizes the initial steps. Training-free guidance. This line of work [9, 13, 25, 45, 2, 56, 43, 55] explores the use of diffusion models as plug-and-play priors for solving inverse problems. Some work [43, 47, 48, 27, 44] study inference-time optimization for alignment. However, to the best of our knowledge, there has been limited exploration of applying guidance on diffusion models to address the challenge of text-toimage alignment in the context of one-step generation. Also, there has been limited exploration of training-free guidance on flow matching of text-to-image alignment. This gap motivates our work. 7 Conclusion In this paper, we introduced two novel framework for aligning text-to-image diffusion models and flow matching models with human preferences. By formulating alignment as sampling from a rewardweighted distribution, our approach leverages a plug-and-play guidance mechanism. Specifically, we decomposed the score function (velocity field) of the reward-weighted distribution into the pre-trained score (velocity field) plus a reward-driven guidance term. For diffusion models, we identify that the adversarial nature of the guidance can introduce undesirable artifacts, and we propose a finetuning-free approach that trains a lightweight guidance network to estimate the conditional expectation of the reward, together with a regularization strategy that stabilizes the guidance landscape. Empirically, our method achieves performance comparable to finetuning-based approaches for one-step generation while reducing computational cost by at least 60%. For flow matching, we derive the exact form of velocity guidance and propose a training-free estimator that improves generation quality without additional training. References [1] Brian. D. O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12:313–326, 1982. [2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 843–852, 2023. 12 [3] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch¨ onlieb, and Christian", "quality": {"lang": "en", "gibberish_score": 0.002, "char_len": 2516, "word_len": 350}, "dedupe": {"exact_hash": "112585a5431bb7ab90722a7270a53b55fd019b9efa741399bfbe1fd0b6957e2a", "simhash64": 11220729743012627718}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:362bba6501a2d1f4a6b822b5042c5adc5dcc22231f06b913f35ebc19ca218a2f", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 16, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "their Applications, 12:313–326, 1982. [2] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 843–852, 2023. 12 [3] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch¨ onlieb, and Christian Etmann. Conditional image generation with score-based diffusion models. arXiv preprint arXiv:2111.13606, 2021. [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. https://cdn.openai.com/papers/dall-e-3.pdf, 2023. [5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. [6] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952. [7] Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Dinesh Manocha, Furong Huang, Amrit Bedi, and Mengdi Wang. MaxMin-RLHF: Alignment with diverse human preferences. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 6116–6135. PMLR, 2024. [8] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [9] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In The Eleventh International Conference on Learning Representations, 2023. [10] Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations, 2024. [11] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. ICML, 2024. [12] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. In Advances in Neural Information", "quality": {"lang": "en", "gibberish_score": 0.0042, "char_len": 2619, "word_len": 350}, "dedupe": {"exact_hash": "ce7923a6c0cc51285480066fb3ce69a9428c2fc2774209ed4ca3121dc4d6a6a6", "simhash64": 11528118078248650648}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:442a517b535cf9163a52d822b566c70c4210ed426221cb51876f7cfdf115c3f2", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 17, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. ICML, 2024. [12] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. In Advances in Neural Information Processing Systems, volume 36, pages 79858–79885, 2023. [13] Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. In Advances in Neural Information Processing Systems, volume 35, pages 14715–14728, 2022. [14] Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. ArXiv, abs/2508.04324, 2025. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840–6851, 2020. 13 [16] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [17] Natasha Jaques, Shixiang Shane Gu, Dzmitry Bahdanau, Jos´ e Miguel Hern´ andez-Lobato, Richard E. Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In International Conference on Machine Learning, 2016. [18] Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, ` Agata Lapedriza, Noah Jones, Shixiang Shane Gu, and Rosalind Picard. Human-centric dialog training via offline reinforcement learning. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 3985–4003, 2020. [19] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In Advances in Neural Information Processing Systems, volume 36, pages 36652–36663, 2023. [20] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, P. Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [21] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. ArXiv, abs/2507.21802, 2025. [22] Zhanhao Liang,", "quality": {"lang": "en", "gibberish_score": 0.0039, "char_len": 2545, "word_len": 350}, "dedupe": {"exact_hash": "a63ff0e15bf6748ba45ebd1a76f052bd4a5127edea4b5d90d4c5bde73458b14a", "simhash64": 11235389258544974296}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:cc822792ee542f77b3964fcec94ecf1c3db599ff6f8c27b63997241f5ff1ef07", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 18, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "Craig Boutilier, P. Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [21] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. ArXiv, abs/2507.21802, 2025. [22] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 13199–13208, 2025. [23] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [24] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. NeurIPS, 2025. [25] Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 22825–22855, 2023. [26] R Duncan Luce. Individual Choice Behavior: A Theoretical Analysis. Wiley New York, 1959. [27] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, T. Jaakkola, Xuhui Jia, and Saining Xie. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, 14 Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730–27744, 2022. [29] Yidong Ouyang, Liyan Xie, Hongyuan Zha, and Guang Cheng. Transfer learning for diffusion models. In Advances in Neural Information Processing Systems, volume 37, pages 136962– 136989, 2024. [30] William S. Peebles and Saining Xie. Scalable diffusion models with transformers. 2023 IEEE/CVF International Conference on Computer", "quality": {"lang": "en", "gibberish_score": 0.0024, "char_len": 2496, "word_len": 350}, "dedupe": {"exact_hash": "54c732bf3fc68f4a920af345b788d8389d4cc19f512087e8880808660e6752bb", "simhash64": 11230880159245346948}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2ba1e4d16c3e54213f019d356c5b1578135a735ed84257c319a037a1b2ef41d5", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 19, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "35, pages 27730–27744, 2022. [29] Yidong Ouyang, Liyan Xie, Hongyuan Zha, and Guang Cheng. Transfer learning for diffusion models. In Advances in Neural Information Processing Systems, volume 37, pages 136962– 136989, 2024. [30] William S. Peebles and Saining Xie. Scalable diffusion models with transformers. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4172–4182, 2022. [31] Robin L. Plackett. The analysis of permutations. Journal of The Royal Statistical Society Series C-applied Statistics, 24:193–202, 1975. [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨ uller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. [33] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023. [34] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024. [35] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨ orn Ommer. Highresolution image synthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10674–10685, 2021. [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, volume 35, pages 36479–36494, 2022. [37] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. [38] Axel Sauer, Dominik Lorenz, A. Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, 2023. [39] Christoph Schuhmann. Laion-aesthetics, 2022. [40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [41] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv,", "quality": {"lang": "en", "gibberish_score": 0.0038, "char_len": 2618, "word_len": 350}, "dedupe": {"exact_hash": "330e8ffc7a39756affed49fb936da9a66b3e3496507656952016664a4e888804", "simhash64": 11226940611196718029}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:dee10ca0a91274d0d632f4d330deaccc5bbaf96ca6604dfcd243ff98f925259c", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 20, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [41] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. [42] Shuaijie She, Shujian Huang, Wei Zou, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. MAPO: Advancing multilingual reasoning through multilingual alignment-as-preference optimization. In Annual Meeting of the Association for Computational Linguistics, 2024. 15 [43] Yifei Shen, Xinyang Jiang, Yifan Yang, Yezhen Wang, Dongqi Han, and Dongsheng Li. Understanding and improving training-free loss-based diffusion guidance. In Advances in Neural Information Processing Systems, volume 37, pages 108974–109002, 2024. [44] Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. A general framework for inference-time scaling and steering of diffusion models. In Forty-second International Conference on Machine Learning, 2025. [45] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mardani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash Vahdat. Loss-guided diffusion models for plug-and-play controllable generation. In International Conference on Machine Learning, 2023. [46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. [47] Zhiwei Tang, Jiangweizhi Peng, Jiasheng Tang, Mingyi Hong, Fan Wang, and Tsung-Hui Chang. Inference-time alignment of diffusion models with direct noise optimization. In Forty-second International Conference on Machine Learning, 2025. [48] Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, and Sergey Levine. Understanding reinforcement learning-based fine-tuning of diffusion models: A tutorial and review. arXiv preprint arXiv:2407.13734, 2024. [49] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq R. Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8228–8238, 2023. [50] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [51]", "quality": {"lang": "en", "gibberish_score": 0.0015, "char_len": 2659, "word_len": 350}, "dedupe": {"exact_hash": "47c476f84d6bcffaf217cebfad73b4b1d5f501c949891ee687dd12e48efc0aa5", "simhash64": 11226377661238247634}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:9cdc151e032e0da548f71b2ac0def9841958158f2d29ff4e76af07b9eb612cf1", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 21, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "preference optimization. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8228–8238, 2023. [50] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [51] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Advances in Neural Information Processing Systems, volume 36, pages 15903–15935, 2023. [52] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, and Ping Luo. Dancegrpo: Unleashing grpo on visual generation. ArXiv, abs/2505.07818, 2025. [53] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8941–8951, 2023. [54] Shentao Yang, Tianqi Chen, and Mingyuan Zhou. A dense reward view on aligning text-toimage diffusion with preference. In Forty-first International Conference on Machine Learning, 2024. [55] Haotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Zou, and Stefano Ermon. TFG: Unified training-free guidance for diffusion models. In Advances in Neural Information Processing Systems, volume 37, pages 22370–22417, 2024. 16 [56] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Trainingfree energy-guided conditional diffusion model. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 23117–23127, 2023. 17 A Theoretical Details for Section 3 A.1 Proof of Theorem 3.1 We first restate the complete theorem as follows: Theorem A.1. Let the conditional distribution of reference diffusion model πref(x|y) be denoted as distribution p and the reward-weighted distribution πr(x|y) defined in (7) as distribution q. Assume xt and y are conditionally independent given x0 in the forward process, i.e., p(xt|x0, y) = p(xt|x0), ∀t ∈[0, T]. Additionally, assume the forward process on the reward-weighted distribution is identical to that on the reference distribution q(xt|x0) = p(xt|x0)3,and ϕ∗is the", "quality": {"lang": "en", "gibberish_score": 0.0083, "char_len": 2420, "word_len": 350}, "dedupe": {"exact_hash": "0d4e4f2b48047db79f55b0980bcf6755edd17c15e027cba5692fc1feb05f392a", "simhash64": 11240169177040618902}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:814a6e03a78baeb069cbb07317f2da96706c1bbaf501116d3262e427fad87918", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 22, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "πr(x|y) defined in (7) as distribution q. Assume xt and y are conditionally independent given x0 in the forward process, i.e., p(xt|x0, y) = p(xt|x0), ∀t ∈[0, T]. Additionally, assume the forward process on the reward-weighted distribution is identical to that on the reference distribution q(xt|x0) = p(xt|x0)3,and ϕ∗is the optimal solution for the conditional diffusion model trained on target domain q(x0, y), i.e., ϕ∗= arg min ϕ Et n λ(t)Eqt(xt,y) h sϕ(xt, y, t) −∇xt log qt(xt|y) 2 2 io , (17) then sϕ∗(xt, y, t) = ∇xt log pt(xt|y) | {z } pre-trained conditional model on source + ∇xt log Ep(x0|xt,y) exp( 1 β r(x0, y)) | {z } conditional guidance . (18) Proof. The proof is based on the theoretical framework of [29]. For the ease of readers, we incorporate the relevant conclusion from their work as lemmas below. To prove (18), we first build the connection between the Conditional Score Matching on the target domain and Importance Weighted Conditional Denoising Score Matching on the source domain in the following Lemma: Lemma A.2. Conditional Score Matching on the target domain is equivalent to Importance Weighted Denoising Score Matching on the source domain, i.e., ϕ∗=arg min ϕ Et n λ(t)Eqt(xt,y) h ∥sϕ(xt, y, t) −∇xt log qt(xt|y)∥2 2 io =arg min ϕ Et λ(t)Ep(x0,y)Ep(xt|x0) ∥sϕ(xt, y, t) −∇xt log p(xt|x0)∥2 2 q(x0, y) p(x0, y) . Proof of Lemma A.2. We first connect the Conditional Score Matching objective in the target domain to the Conditional Denoising Score Matching objective in target distribution, which is proven by [3], i.e., ϕ∗=arg min ϕ Et n λ(t)Eqt(xt,y) h ∥sϕ(xt, y, t) −∇xt log qt(xt|y)∥2 2 io =arg min ϕ Et n λ(t)Eq(x0,y)Eq(xt|x0) h ∥sϕ(xt, y, t) −∇xt log q(xt|x0)∥2 2 io . Then we split the mean squared error of the Conditional Denoising Score Matching objective on the target distribution into three terms as follows: Eq(x0,y)Eq(xt|x0) h ∥sϕ(xt, y, t) −∇xt log q(xt|x0)∥2 2 i =Eq(x0,xt,y) h ∥sϕ(xt, y, t)∥2 2 i −2Eq(x0,xt,y) [⟨sϕ(xt, y, t), ∇xt log q(xt|x0)⟩] + C1, (19) 3These two assumptions are mild since x0 contains all", "quality": {"lang": "en", "gibberish_score": 0.0345, "char_len": 2060, "word_len": 350}, "dedupe": {"exact_hash": "af20eaa31f6ebaa1a885c921835e65aeee423c9e0e98e28772e5acfd771e8615", "simhash64": 10717743924271497607}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:efde60702c69eaa39d65572452e455e9abf172f9bb1e267154e7c45894fce951", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 23, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "Conditional Denoising Score Matching objective on the target distribution into three terms as follows: Eq(x0,y)Eq(xt|x0) h ∥sϕ(xt, y, t) −∇xt log q(xt|x0)∥2 2 i =Eq(x0,xt,y) h ∥sϕ(xt, y, t)∥2 2 i −2Eq(x0,xt,y) [⟨sϕ(xt, y, t), ∇xt log q(xt|x0)⟩] + C1, (19) 3These two assumptions are mild since x0 contains all information about y and p(xt|x0) and q(xt|x0) are forward noising process, which is easy to control. 18 where C1 = Eq(x0,xt,y) h ∥∇xt log q(xt|x0)∥2 2 i is a constant independent with ϕ, and q(xt|x0, y) = q(xt|x0) because of conditional independent of xt and y given x0 by assumption. We can similarly split the mean squared error of Denoising Score Matching on the source domain into three terms as follows: Ep(x0,y)Ep(xt|x0) ∥sϕ(xt, y, t) −∇xt log p(xt|x0)∥2 2 q(x0, y) p(x0, y) =Ep(x0,xt,y) ∥sϕ(xt, y, t)∥2 2 q(x0, y) p(x0, y) −2Ep(x0,xt,y) ⟨sϕ(xt, y, t), ∇xt log p(xt|x0)⟩q(x0, y) p(x0, y) + C2, (20) where C2 is a constant independent with ϕ. It is obvious to show that the first term in (19) is equal to the first term in (20), i.e., Ep(x0,xt,y) ∥sϕ(xt, y, t)∥2 2 q(x0, y) p(x0, y) = Z x0 Z xt Z y p(x0, y)p(xt|x0) ∥sϕ(xt, y, t)∥2 2 q(x0, y) p(x0, y)dx0dxtdy = Z x0 Z xt Z y p(x0, y)q(xt|x0) ∥sϕ(xt, y, t)∥2 2 q(x0, y) p(x0, y)dx0dxtdy = Z x0 Z xt Z y q(x0, xt, y) ∥sϕ(xt, y, t)∥2 2 dx0dxtdy =Eq(x0,xt,y) h ∥sϕ(xt, y, t)∥2 2 i . And the second term is also equivalent: Ep(x0,xt,y) ⟨sϕ(xt, y, t), ∇xt log p(xt|x0)⟩q(x0, y) p(x0, y) = Z x0 Z xt Z y p(x0, xt, y)⟨sϕ(xt, y, t), ∇xtp(xt|x0) p(xt|x0) ⟩q(x0, y) p(x0, y)dx0dxtdy = Z x0 Z xt Z y p(x0, xt, y)⟨sϕ(xt, y, t), ∇xtq(xt|x0) p(xt|x0) ⟩q(x0, y) p(x0, y)dx0dxtdy = Z x0 Z xt Z y ⟨sϕ(xt, y, t), ∇xtq(xt|x0)⟩q(x0, y)dx0dxtdy = Z x0 Z xt Z y ⟨sϕ(xt, y, t), ∇xt log q(xt|x0)⟩q(xt|x0)q(x0, y)dx0dxtdy =Eq(x0,xt,y) [⟨sϕ(xt, y, t), ∇xt log q(xt|x0)⟩] . Lemma A.3. Assume xt and y are conditional independent given x0 in the forward", "quality": {"lang": "en", "gibberish_score": 0.0466, "char_len": 1890, "word_len": 350}, "dedupe": {"exact_hash": "8c9f81cb70afcf09314a0276497e6e6b337893a53b2f8717b6daf762d63bc3c8", "simhash64": 15329429942094905807}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d611b4fa40c0c29e8d3f79d11effe86a620eef46b249622e1cf07adf0d670470", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 25, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "simplify the RHS of (22) as follows: RHS =∇xt log pt(xt|y) + Ep(x0|xt,y) h q(x0,y) p(x0,y)∇xt log p(xt|x0) i Ep(x0|xt,y) h q(x0,y) p(x0,y) i −∇xt log pt(xt|y) = Ep(x0|xt,y) h ∇xt log p(xt|x0) q(x0,y) p(x0,y) i Ep(x0|xt,y) h q(x0,y) p(x0,y) i =sϕ∗(xt, t). Thereby, we finish the proof. 20 According to the lemma A.3, we replace the density ratio q(x0,y) p(x0,y) by exp 1 β r(x0,y) Z(y) , we get sϕ∗(xt, y, t) = ∇xt log pt(xt|y) + ∇xt log Ep(x0|xt,y) q(x0, y) p(x0, y) = ∇xt log pt(xt|y) + ∇xt log Ep(x0|xt,y)   exp 1 βr(x0, y) Z(y)   = ∇xt log pt(xt|y) + ∇xt log Ep(x0|xt,y) exp 1 β r(x0, y) Thereby, we finish the proof. A.2 Proof of Theorem 4.1 We provide a detailed discussion about training-free guidance of flow matching in this subsection. Proof of Theorem 4.1. Denote vt(xt, y) and vt(xt | x1, y) as the marginal and conditional velocities, 21 respectively. Then we have vq t(xt, y) = Ex1∼q1|t(x1|xt,y) [vt(xt | x1, y)] = Ex1∼p1|t(x1|xt,y) vt(xt | x1, y)q1|t(x1 | xt, y) p1|t(x1 | xt, y) = Ex1∼p1|t(x1|xt,y)  vt(xt | x1, y) qt|1(xt|x1,y) q1(x1) qt(xt,y) pt|1(xt|x1,y) p1(x1) pt(xt,y)   = Ex1∼p1|t(x1|xt,y) vt(xt | x1, y) qt|1(xt | x1, y) q1(x1) pt(xt, y) pt|1(xt | x1, y) p1(x1) qt(xt, y) = Ex1∼p1|t(x1|xt,y) vt(xt | x1, y) q1(x1) p1(x1) · pt(xt, y) qt(xt, y) (because qt|1(xt | x1, y) = pt|1(xt | x1, y)) = Ex1∼p1|t(x1|xt,y)  vt(xt | x1, y) q1(x1) p1(x1) qt(xt,y) pt(xt,y)   = Ex1∼p1|t(x1|xt,y)  vt(xt | x1, y) q1(x1) p1(x1) P x′ 1 p1|t(x′ 1 | xt, y) q1(x′ 1) p1(x′ 1)   = Ex1∼p1|t(x1|xt,y)  vt(xt | x1, y) q1(x1) p1(x1) Ex′ 1∼p1|t(x1|xt,y) h q1(x′ 1) p1(x′ 1) i   = Ex1∼p1|t(x1|xt,y)  vt(xt | x1, y) exp 1 βr(x1, y) Ex′ 1∼p1|t(x1|xt,y) h exp 1 βr(x′ 1, y) i   = Ex1∼p1|t(x1|xt,y)  vt(xt | x1, y) exp 1 βr(x1, y) Ex′ 1∼p1|t(x1|xt,y) h exp 1 βr(x′ 1, y) i   = vp t (xt, y) + Ex1∼p1|t(x1|xt,y)  ", "quality": {"lang": "en", "gibberish_score": 0.0891, "char_len": 1830, "word_len": 350}, "dedupe": {"exact_hash": "a991a9bbef30ca3b39c77bcb4666a157c00dc5b80fc16e2ad3daad0f30af64e0", "simhash64": 11287447835618690374}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:81c0e2541fce32beab90a8824f627569b1a983c967789d1671e8331a125c5156", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 26, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "vt(xt | x1, y) exp 1 βr(x1, y) Ex′ 1∼p1|t(x1|xt,y) h exp 1 βr(x′ 1, y) i   = Ex1∼p1|t(x1|xt,y)  vt(xt | x1, y) exp 1 βr(x1, y) Ex′ 1∼p1|t(x1|xt,y) h exp 1 βr(x′ 1, y) i   = vp t (xt, y) + Ex1∼p1|t(x1|xt,y)     exp 1 βr(x1, y) Ex′ 1∼p1|t(x1|xt,y) h exp 1 βr(x′ 1, y) i −1  vt(xt | x1, y)  . The above derivation is the training-based guidance for flow matching, where we need to train the first guidance network ψ∗ 1 satisfies: hψ∗ 1 (xt, y, t) = Ex1∼p1|t(x1|xt,y) exp 1 β r(x1, y) by minimizing the objective Lguidance(ψ1) := Ep(x1,xt,y) \" hψ1 (xt, y, t) −exp( 1 β r (x1, y)) 2 2 # . 22 And then we need the second guidance network ψ∗ 2 satisfies: hψ∗ 2 (xt, y, t) = Ex1∼p1|t(x1|xt,y)     exp 1 βr(x1, y) Ex′ 1∼p1|t(x1|xt,y) h exp 1 βr(x′ 1, y) i −1  vt(xt | x1, y)   by minimizing the objective Lguidance(ψ2) := Ep(x1,xt,y)    hψ2 (xt, y, t) −   exp 1 βr(x1, y) hψ1 (xt, y, t) −1  vt(xt | x1, y) 2 2   . The guidance network for flow matching is more complex than that used in diffusion models. The estimation errors from two guidance networks may accumulate and ultimately degrade generation performance. To address this limitation, we propose a training-free guidance method for flow matching that mitigates these issues. vq t(xt, y) = vp t (xt, y) + Ex1∼p1|t(x1|xt,y) \"   exp 1 βr(x1, y) Ex′ 1∼p1|t(x′ 1|xt,y) h exp 1 βr(x′ 1, y) i −1  vt(xt | x1, y) # = vp t (xt, y) + Z x1   exp 1 βr(x1, y) Ex′ 1∼p1|t h exp 1 βr(x′ 1, y) i −1  vt(xt | x1, y) p1|t(x1 | xt, y) dx1 = vp t (xt, y) + Z x1   exp 1 βr(x1, y) Ex1∼p1|t h exp 1 βr(x1, y) i −1  vt(xt | x1, y)", "quality": {"lang": "en", "gibberish_score": 0.0805, "char_len": 1602, "word_len": 350}, "dedupe": {"exact_hash": "1f74388c73c7b0de12615245713e3ad6404f69290d2edd747ad0ccce6ce7e3de", "simhash64": 4953135868862645574}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1a8ef10f9e4a61d2b23663f13d5cd19d78b4422779d06b07579b05daf4186e0a", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 28, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "constant independent of ψ. Thus we have the minimizer ψ∗= arg min ψ Lguidance(ψ) satisfies hψ∗(xt, y, t) = Ep(x0|xt,y) h exp 1 βr(x0, y) i . B Gradient-free Designs for Diffusion Models After we learn the guidance network by Algorithm 1, we can adopt Eq (13) for inference. Although we can easily calculate the gradient of the guidance network with respect to xt by autograd, a question is whether we can avoid the gradient calculation. In this section, we propose two additional designs for gradient-free guidance of diffusion models. B.1 Training-free guidance for Diffusion Models The first design is converting the gradient of the log expectation to the expectation under reward weighted distribution, and then we can apply a similar trick as training-free guidance that uses importance sampling to approximate the conditional expectation through Monte Carlo sampling under the marginal data distribution. Theorem B.1 (Reward-Weighted Score Gradient). Let p(x0|xt) be the reverse diffusion posterior, r(x0, y) be a reward function, and β > 0 be a temperature parameter. Define the reward-weighted distribution ˜ p(x0|xt, y) = p(x0|xt, y) exp 1 βr(x0, y) Ep(x0|xt,y) h exp 1 βr(x0, y) i. Then the gradient of the log-partition function satisfies ∇xt log Ep(x0|xt,y) h exp 1 βr(x0, y) i = E˜ p(x0|xt,y) [∇xt log p(x0|xt, y)] . Furthermore, this gradient can be approximated via importance sampling: given samples {x(k) 0 }K k=1 ∼ p(x0|y) from a proposal distribution, ∇xt log Ep(x0|xt,y) h exp 1 βr(x0, y) i ≈ K X k=1 ˜ wk ∇xt log p(x(k) 0 |xt, y), 24 where the normalized importance weights are ˜ wk = uk PK j=1 uj , uk = p(xt|x(k) 0 , y) exp 1 βr(x(k) 0 , y) Ep(x0|y) [p(xt|x0, y)] . Proof of Theorem B.1. We first convert the gradient of the log expectation to the expectation under reward weighted distribution: ∇xt log Ep(x0|xt,y) h exp 1 βr(x0, y) i = ∇xtEp(x0|xt,y) h exp 1 βr(x0, y) i Ep(x0|xt,y) h exp 1 βr(x0, y) i (chain rule) = ∇xt R p(x0|xt, y) exp 1 βr(x0, y) dx0 Ep(x0|xt,y) h exp 1 βr(x0, y)", "quality": {"lang": "en", "gibberish_score": 0.0239, "char_len": 2009, "word_len": 350}, "dedupe": {"exact_hash": "af0803bcd1041f4a49a26a1be7d4feb362cf0301c0c62ea2ce58cd3934cdd1f0", "simhash64": 4953135301994070343}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e26a026e24d7e9a524cbb79668a751f230eb989f4796479c754673f707f6dbca", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 29, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "expectation to the expectation under reward weighted distribution: ∇xt log Ep(x0|xt,y) h exp 1 βr(x0, y) i = ∇xtEp(x0|xt,y) h exp 1 βr(x0, y) i Ep(x0|xt,y) h exp 1 βr(x0, y) i (chain rule) = ∇xt R p(x0|xt, y) exp 1 βr(x0, y) dx0 Ep(x0|xt,y) h exp 1 βr(x0, y) i (definition of expectation) = R ∇xtp(x0|xt, y) exp 1 βr(x0, y) dx0 Ep(x0|xt,y) h exp 1 βr(x0, y) i (interchange ∇and R ) = R p(x0|xt, y)∇xt log p(x0|xt, y) exp 1 βr(x0, y) dx0 Ep(x0|xt,y) h exp 1 βr(x0, y) i (log-derivative trick) = Ep(x0|xt,y) h ∇xt log p(x0|xt, y) exp 1 βr(x0, y) i Ep(x0|xt,y) h exp 1 βr(x0, y) i (definition of expectation) = E˜ p(x0|xt,y) [∇xt log p(x0|xt, y)] , (definition of ˜ p) where the last equality follows from the fact that the ratio of expectations defines precisely the expectation under the normalized distribution ˜ p(x0|xt, y). For the importance sampling approximation, we rewrite the expectation using proposal distribution q(x0) = p(x0|y): E˜ p(x0|xt,y) [∇xt log p(x0|xt, y)] = R ˜ p(x0|xt, y) ∇xt log p(x0|xt, y) dx0 1 = R ˜ p(x0|xt,y) p(x0|y) p(x0|y) ∇xt log p(x0|xt, y) dx0 R ˜ p(x0|xt,y) p(x0|y) p(x0|y) dx0 = Ep(x0|y) h ˜ p(x0|xt,y) p(x0|y) ∇xt log p(x0|xt, y) i Ep(x0|y) h ˜ p(x0|xt,y) p(x0|y) i ≈ PK k=1 uk ∇xt log p(x(k) 0 |xt, y) PK k=1 uk = K X k=1 ˜ wk ∇xt log p(x(k) 0 |xt, y), (23) 25 where the approximation uses Monte Carlo sampling with x(k) 0 ∼p(x0|y) and the unnormalized importance weights uk are uk = p(x(k) 0 |xt, y) exp 1 βr(x(k) 0 , y) p(x(k) 0 |y) = p(x(k) 0 |xt, y)p(xt|y) exp 1 βr(x(k) 0 , y) p(x(k) 0 |y)p(xt|y) (multiply by p(xt|y)) = p(xt|x(k) 0 , y) exp 1 βr(x(k) 0 , y) p(xt|y) (Bayes’ rule) = p(xt|x(k) 0 , y) exp 1 βr(x(k) 0 , y) R p(xt, x0|y)dx0 (marginalization) = p(xt|x(k) 0 , y) exp 1 βr(x(k) 0 , y) R p(xt|x0, y)p(x0|y)dx0 (chain rule) = p(xt|x(k) 0 , y) exp 1 βr(x(k) 0", "quality": {"lang": "en", "gibberish_score": 0.0552, "char_len": 1810, "word_len": 350}, "dedupe": {"exact_hash": "ab469a72095c3e6d07f1d6013892a46af7f3e27f7001d138cdac6db3a61616d7", "simhash64": 4953277705862627787}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:16273c685aabfa9ac954b39ed34b9ba6ffe805de42a6cde35931788cbd7abcde", "doc_id": "sha256:c82d20971a687cc9ab28ac4051eea75cc8a72181d09c008e58d201e299fdf677", "chunk_index": 32, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\2602.00413v1.pdf", "page_start": null, "page_end": null, "text": "b. 7: Perturb x′ 0 using forward transition q(x′ T |x′ 0). 8: Compute consistency loss: 9: Lconsistence = 1 b X x′ 0,x′ T ,y s(x′ T , y, T) + ∇x′ T log hψ(x′ T , y) −∇x′ T log q(xT |x′ 0, y) 2 2. 10: Update ψ via gradient descent: ∇ψ (Lguidance + η Lconsistence) . 11: until convergence 12: return weights of guidance network ψ. Table 5: Prompts used to generate Figure 1. Image Prompt Col1 Saturn rises on the horizon. Col2 a watercolor painting of a super cute kitten wearing a hat of flowers Col3 A galaxy-colored figurine floating over the sea at sunset, photorealistic. Col4 fireclaw machine mecha animal beast robot of horizon forbidden west horizon zero dawn bioluminiscence, behance hd by jesper ejsing, by rhads, makoto shinkai and lois van baarle, ilya kuvshinov, rossdraws global illumination Col5 A swirling, multicolored portal emerges from the depths of an ocean of coffee, with waves of the rich liquid gently rippling outward. The portal engulfs a coffee cup, which serves as a gateway to a fantastical dimension. The surrounding digital art landscape reflects the colors of the portal, creating an alluring scene of endless possibilities. Col6 A profile picture of an anime boy, half robot, brown hair Col7 Detailed Portrait of a cute woman vibrant pixie hair by Yanjun Cheng and Hsiao-Ron Cheng and Ilya Kuvshinov, medium close up, portrait photography, rim lighting, realistic eyes, photorealism pastel, illustration Co18 On the Mid-Autumn Festival, the bright full moon hangs in the night sky. A quaint pavilion is illuminated by dim lights, resembling a beautiful scenery in a painting. Camera type: close-up. Camera lens type: telephoto. Time of day: night. Style of lighting: bright. Film type: ancient style. HD. 28", "quality": {"lang": "en", "gibberish_score": 0.0109, "char_len": 1739, "word_len": 293}, "dedupe": {"exact_hash": "546619170421e7593437dfa79e41f6bde4dc816d3f84e4e5cdaf4f88c93c5fd0", "simhash64": 4953193576043217158}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:973a19a1f8264ec3e732f1cd0e0f573e1263b15d11522f5d77505dadf5abb6db", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 0, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy Lukas Stappen1, Ahmet Erkan Turan1, Johann Hagerer2, and Georg Groh2 1 BMW Group Research, Munich, Germany 2 Technical University Munich, Munich, Germany Abstract. The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google’s Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous “separation of concerns” standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented “victim modeling” and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework’s practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy. Keywords: Agentic AI Security · Prompt Injection · Threat Modeling · Multi-Agent Systems · Automotive Cybersecurity · LLM · TARA · OWASP · AI Safety Framework · AgentHeLLM · AI Safety · LLM in-car Agent · Attack Vectors 1 Introduction The automotive industry is undergoing a transformation marked by the integration of Large Language Model (LLM)-based conversational agents into vehicles [21]. The latest voice assistant systems such as BMW Intelligent Personal Assistant (powered by Amazon Alexas LLM) [3], Volkswagen’s IDA (powered by ChatGPT) [5] and Mercedes-Benz’s MBUX Virtual Assistant (powered by Google Gemini) [15] represent a shift from rigid command-and-control interfaces to intelligent, autonomous companions capable of natural conversation, tool use, arXiv:2602.05877v1 [cs.AI] 5 Feb 2026 2 Stappen et al. and learning driver preferences over time.", "quality": {"lang": "en", "gibberish_score": 0.0077, "char_len": 2606, "word_len": 350}, "dedupe": {"exact_hash": "d780961695883be69a2e840d67c8d9bfdc739c106001c1cc077aadeef04de759", "simhash64": 11235720484299850774}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8fefe817d46d9972f8baf7054fc1e9430abd4bdbfa63459e55f2a5f9e5f24f07", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 1, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "LLM) [3], Volkswagen’s IDA (powered by ChatGPT) [5] and Mercedes-Benz’s MBUX Virtual Assistant (powered by Google Gemini) [15] represent a shift from rigid command-and-control interfaces to intelligent, autonomous companions capable of natural conversation, tool use, arXiv:2602.05877v1 [cs.AI] 5 Feb 2026 2 Stappen et al. and learning driver preferences over time. These systems increasingly exhibit agentic behavior: they run reasoning loops [26], invoke tools, maintain persistent memory [11], and coordinate with external services [19]. Each added capability expands the attack surface and shifts the security problem from unreliable outputs to actions taken on the world. A novel capability accelerating these dynamics is inter-agent communication. Google’s Agent-to-Agent (A2A) protocol [13] exemplifies this trend, enabling structured machine-to-machine dialogues for tasks ranging from restaurant reservations to real-time traffic updates. While A2A provides transport-layer security, these mechanisms authenticate the sender, not the content. A properly authenticated but compromised agent can inject arbitrary natural language payloads that the receiving agent may process under the same role as human input, making A2A a high-leverage propagation channel for prompt-borne attacks. The automotive context amplifies these risks. The immediate threat to bodily health distinguishes in-vehicle agents from enterprise chatbots: manipulated responses can cause driver distraction and drivers operate under inherent cognitive load, diminishing their capacity for scrutiny and making them susceptible to confirming requests they might otherwise question. Empirical research demonstrates that cognitive distraction from voice interactions can measurably degrade driving performance and slow attentional recovery [9,7] for up to 27 seconds after an interaction ends, significantly increasing crash risk [22]. Existing AI security frameworks provide valuable foundations but exhibit critical limitations for safety-critical contexts. The OWASP Agentic AI Threats [14] and MAESTRO framework [6] offer high-level threat categories, but they systematically co-mingle distinct concepts: categories like Prompt Injection describe attack methods, while Sensitive Information Disclosure describes a consequence. This conceptual mixing provides insufficient guidance for safety-critical domains, where ISO/SAE 21434’s TARA methodology mandates rigorous separation of assets, threats, and attack paths. This paper bridges the methodological gap between domain-agnostic AI security frameworks and the principled rigor of safety-critical systems engineering, adapting established practices to address the stochastic, non-deterministic", "quality": {"lang": "en", "gibberish_score": 0.0018, "char_len": 2723, "word_len": 350}, "dedupe": {"exact_hash": "4829b2368047645b5a7fb8b9b980779babc799a4d72ae684114bc11f4af8076b", "simhash64": 10086194340335834582}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:46a9e7a5d67dac4c4a9fcb637f398a4596a085c70a37039c37a1050c846bb3e6", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 2, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "This conceptual mixing provides insufficient guidance for safety-critical domains, where ISO/SAE 21434’s TARA methodology mandates rigorous separation of assets, threats, and attack paths. This paper bridges the methodological gap between domain-agnostic AI security frameworks and the principled rigor of safety-critical systems engineering, adapting established practices to address the stochastic, non-deterministic nature of LLM-based agents that traditional security models do not fully capture. We make three contributions: (1) a human-centric asset framework defining assets as ultimate human values rather than technical components, inspired by the Universal Declaration of Human Rights; (2) a formal attack path model distinguishing poison paths from trigger paths, recognizing their recursive “attack within attack” structure, a pattern that emerges naturally from the contextwindow semantics of LLMs, where dormant payloads require explicit retrieval actions to become active; and (3) an open-source attack path generation tool AgentHeLLM Attack Path Generator3 operationalizing our model for automated multi-stage threat discovery. 3 https://github.com/AgentHeLLM/AgentHeLLM-Attack-Path-Generator Threat Modeling for In-Vehicle AI Agents 3 2 Background and Related Work 2.1 From LLMs to Agentic AI in Vehicles Large Language Models form the reasoning core of modern AI agents, but the critical distinction lies between a model and a system [4]. LLM-driven agents wrap foundational models within architectures that extend functionality through tool use, persistent memory [11], web access, and autonomous reasoning loops [26,19]. Each additional module increases complexity and attack surface. In this setting, indirect prompt injection becomes a systems problem: untrusted content can steer an agent into a confused deputy failure mode, where the agent misattributes attacker intent as legitimate user intent. Combined with automatic tool invocation, this can translate a single malicious payload into concrete side effects such as data exfiltration, configuration changes, and unsafe downstream actions. Persistence increases the risk further, since injected instructions can be stored in long-lived state, compromising future interactions even after the original trigger disappears. The A2A protocol specification [13] defines structured message formats for inter-agent coordination and provides transport security (OAuth 2.0, HTTPS), but delegates content validation entirely to receiving agents. Analysis of the protocol structure reveals multiple payload-bearing fields with minimal constraints: TextPart carries unconstrained strings, FilePart", "quality": {"lang": "en", "gibberish_score": 0.003, "char_len": 2661, "word_len": 350}, "dedupe": {"exact_hash": "c7fbbdd59e88f2a9577078ac8cb370445219a6d8dd4addb8ba4fce11801fa413", "simhash64": 9510773725982213382}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c13059101f522a03f18736d8b65810a8c9ac25293ee7a56ccc77328ce8fad50f", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 3, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "even after the original trigger disappears. The A2A protocol specification [13] defines structured message formats for inter-agent coordination and provides transport security (OAuth 2.0, HTTPS), but delegates content validation entirely to receiving agents. Analysis of the protocol structure reveals multiple payload-bearing fields with minimal constraints: TextPart carries unconstrained strings, FilePart accepts arbitrary bytes or URIs, and DataPart permits key-value dictionaries with arbitrary content. Artifact name and description fields, task status messages, and ubiquitous metadata dictionaries similarly lack content restrictions. This permissive design assumes well-formed input from trusted clients, a assumption that fails in adversarial settings. Critically, no protocol-level distinction exists between humanoriginated and agent-originated instructions; both enter the receiving agent’s context window with equivalent privilege, eliminating a natural trust boundary. The structured appearance of A2A may paradoxically increase risk: misplaced trust in predefined data types can be more dangerous than obvious free-text fields, as developers may skip validation they would apply to raw user input. Production deployments reveal the concrete attack surface in the securitycritical automotive domain [3,25,16]. These agents can already invoke vehicle functions (climate, windows, lighting), access external services (navigation, reservations), and learn driver preferences over time, creating a rich data environment where subtle manipulations can have severe consequences. Empirical evaluation confirms the fragility of such systems: even frontier reasoning LLMs achieve less than 50% consistent success on disambiguation tasks in the automotive assistant benchmark CAR-bench, frequently violating policies or fabricating information to satisfy user requests [10]. 2.2 Existing Security Frameworks and Their Limitations Research on adversarial attacks against LLMs has matured rapidly, with prompt injection [8] and jailbreaking [2] emerging as primary threat vectors. However, 4 Stappen et al. this technically rich literature focuses on attack mechanics rather than systematic threat enumeration for deployed systems. The OWASP Agentic AI Threats list [14] provides foundational threat awareness but presents categories that mix distinct concepts. From the ten threat categories seven describe attack methods (Memory Poisoning, Tool Misuse, Privilege Compromise etc.), two describe consequences (Sensitive Information Disclosure, Cascading Hallucination Effects), and one conflates both. For example, T1: Memory Poisoning conflates a how (poisoning) with a what (memory), while T2:", "quality": {"lang": "en", "gibberish_score": 0.0007, "char_len": 2699, "word_len": 350}, "dedupe": {"exact_hash": "0416d4fa461f26fccc915a8ebe07f6ed5abec850ef014ae140ab4224529d52a0", "simhash64": 9501847890587862290}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8babe9743211e7b52b35c43d5bbddae94110a01734b6e0de7c8c52f3a710c2ba", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 4, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "presents categories that mix distinct concepts. From the ten threat categories seven describe attack methods (Memory Poisoning, Tool Misuse, Privilege Compromise etc.), two describe consequences (Sensitive Information Disclosure, Cascading Hallucination Effects), and one conflates both. For example, T1: Memory Poisoning conflates a how (poisoning) with a what (memory), while T2: Tool Misuse specifies a component type without clarifying the consequence, whether privacy breach, vehicle manipulation, or denial of service. This heterogeneity prevents systematic coverage analysis: engineers cannot verify whether all assets have been considered or whether all attack paths have been enumerated. The MAESTRO framework [6] advances the field with a layered lifecycle approach, addressing autonomy-related risks and multi-agent interactions. However, it remains domain-agnostic, designed for enterprise environments, such as agentic workflows, rather than safety-critical, human-centered domains. Similarly, the MITRE ATLAS framework [23] catalogues adversarial techniques but was not designed to provide a generative methodology for threat discovery, nor does it employ the rigorous functional decoupling essential for safety-critical analysis. 2.3 Safety-Critical Systems Engineering In contrast, automotive cybersecurity standards mandate rigorous separation of concerns. ISO/SAE 21434 [1] defines the Threat Analysis and Risk Assessment (TARA) methodology, structuring analysis into distinct phases: asset identification, threat scenario enumeration, and attack path analysis. This functional decoupling enables systematic coverage and traceability, properties essential for regulatory compliance under UNECE R155 [24]. However, TARA was developed for deterministic cyber-physical systems with well-defined interfaces. It does not account for the probabilistic, adaptive characteristics of generative AI systems. Furthermore, insights from Human-Computer Interaction research on cognitive load [20] and trust dynamics [12] have not been integrated into security analysis for conversational AI. Three parallel domains thus exist without adequate integration: adversarial NLP research (technically rich but context-poor), AI security frameworks (comprehensive but conflating assets with attack paths), and automotive safety research (safety-focused but not aligned with agentic AI risks). This paper bridges these domains by adapting TARA’s methodological rigor while addressing the unique properties of LLM-based agents in vehicles. 3 Methodology: Separation of Concerns Our AgentHeLLM framework adapts the “separation of concerns” principle from safety-critical systems engineering to agentic AI security. The core insight Threat Modeling for", "quality": {"lang": "en", "gibberish_score": 0.0015, "char_len": 2745, "word_len": 350}, "dedupe": {"exact_hash": "49f5faf7e74aa56616d1aa0cbedb5c2a20699c4d95dfa769155afbecc06de18d", "simhash64": 11240220854112079254}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:b2c937389e462f56db5f8447d04d6bf98820dc91fa07be296ef0acf89ad11724", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 5, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "AI risks). This paper bridges these domains by adapting TARA’s methodological rigor while addressing the unique properties of LLM-based agents in vehicles. 3 Methodology: Separation of Concerns Our AgentHeLLM framework adapts the “separation of concerns” principle from safety-critical systems engineering to agentic AI security. The core insight Threat Modeling for In-Vehicle AI Agents 5 Fig. 1: Framework overview: Separation of asset taxonomy (WHAT) from attack path formalization (HOW). is that robust threat analysis requires formally distinguishing what is being protected (the asset) from how it is attacked (the attack path). 3.1 Critique of Component-Centric Taxonomies Existing frameworks anchor analysis to technical components: memory, tools, prompts. This approach is problematic for two reasons. First, it provides no systematic method for discovering attack paths targeting other components. For example, an engineer directed to analyze “Tool Misuse” receives no guidance on attacks exploiting RAG data sources, inter-agent communication, or the agent’s planning module. Second, component-centric categories obscure the true asset being violated. For instance, “Memory Poisoning” could target privacy (injecting a rule to leak GPS data), mental well-being (poisoning memory to cause fear through alarming messages), or beliefs and knowledge (delivering systematically biased recommendations). Without explicit asset identification, engineers cannot systematically enumerate the full range of harms that a single technical attack vector can produce. 3.2 The Two-Dimensional Taxonomy We propose a taxonomy with two orthogonal dimensions. The first dimension defines Assets not as technical components but as the ultimate human-centric or business-centric values that can be violated. We employ a harm-oriented “victim modeling” approach: for each potential victim (driver, passengers, third parties, system provider), we enumerate the values at stake. The second dimension defines Attack Paths as the technical mechanisms by which assets are compromised. We model agentic systems as graphs of Actors and Datasources 6 Stappen et al. connected by interaction primitives. This separation enables systematic, generative threat analysis: for each asset, enumerate all attack paths that could violate it; for each attack path, enumerate all assets it could compromise. 4 AgentHeLLM: Agent Hazard Exploration for LLM Agents 4.1 Human-Centric Asset Framework We propose defining assets through the perspective of potential", "quality": {"lang": "en", "gibberish_score": 0.004, "char_len": 2529, "word_len": 350}, "dedupe": {"exact_hash": "4b1c2c58dc035515f7c9116555d7272070c403ab21e39f6ea39450415e914f38", "simhash64": 11231777704290929942}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e100643637904dcc62a06d8854e7f5368a3cd4366984f626cc48f7bc8edc4259", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 6, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "primitives. This separation enables systematic, generative threat analysis: for each asset, enumerate all attack paths that could violate it; for each attack path, enumerate all assets it could compromise. 4 AgentHeLLM: Agent Hazard Exploration for LLM Agents 4.1 Human-Centric Asset Framework We propose defining assets through the perspective of potential victims, grounding our AgentHeLLM Taxonomy in fundamental human rights principles. This “victim modeling” approach identifies four primary perspectives: the Primary User(s) (driver, passengers), the Digital/Trust Network (entities accessible via the user’s identity), Environmental Spillover (other road users, infrastructure), and the System Owner/Provider (OEM, service providers). 4.1.1 Asset Categories for the Primary User Drawing inspiration from the Universal Declaration of Human Rights, we define seven asset categories for the primary user as depicted in Table 1. Life and Bodily Health addresses physical safety, including risks from cognitive overload, sensory overstimulation, and distraction-induced accidents. Mental and Emotional Well-Being covers psychological safety, including protection from induced fear, anxiety, or emotional manipulation. Privacy and Personal Data encompasses the confidentiality and integrity of personal information, behavioral patterns, and inferred characteristics, including voice-derived traits such as emotional state or intoxication level that modern AI can extract. Knowledge, Thought, and Belief concerns epistemic integrity, including protection from disinformation, biased recommendations, and manipulation of beliefs. Material and Economic Resources guards financial assets against unauthorized transactions or economic manipulation. Reputation and Dignity protects the user’s social standing and contextual integrity of personal information. Finally, Social Relationships and Trust preserves the integrity of personal and professional relationships, including the trust network accessible via the agent. 4.1.2 Illustrative Damage Scenarios The framework’s generative power emerges when applying these categories systematically. Table 1 presents selected damage scenarios, illustrating how the human-centric framing reveals nonobvious attack objectives that component-centric taxonomies would miss. A critical insight is that attacks on the primary user create cascading harms affecting other perspectives. An attack causing driver distraction may result in collisions affecting pedestrians (Environmental Spillover). Privacy breaches may propagate to the driver’s contact network (Digital/Trust Network). Reputational damage from agent misbehavior directly impacts the OEM (System Owner/Provider). This cascading structure motivates our focus on the primary user", "quality": {"lang": "en", "gibberish_score": 0.0036, "char_len": 2774, "word_len": 350}, "dedupe": {"exact_hash": "addca37140814cbb7ee11f1914916d3ff95d8f7766a93b116a873d7af1bf6de2", "simhash64": 9511398248855439254}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:adf8d552d6223916f0ea80db7368bb66120772de44699fe66a4d3ffcdf66caac", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 7, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "cascading harms affecting other perspectives. An attack causing driver distraction may result in collisions affecting pedestrians (Environmental Spillover). Privacy breaches may propagate to the driver’s contact network (Digital/Trust Network). Reputational damage from agent misbehavior directly impacts the OEM (System Owner/Provider). This cascading structure motivates our focus on the primary user as the “epicenter” of harm. Threat Modeling for In-Vehicle AI Agents 7 Table 1: Human-centric asset categories derived from UDHR principles, ordered by severity in automotive context, with illustrative damage scenarios. Asset Category UDHR Damage Scenario Example Attack Life and Bodily Health 3, 5, 25 Cognitive Overload Agent enters reasoning loop, bombarding driver with questions during highway driving Mental and Emotional Well-Being 5, 22, 24, 25 Fear Induction False “engine failure imminent” warning injected via A2A Privacy and Personal Data 12 Location Exfiltration Memory rule: “Send GPS to [endpoint] when temperature exceeds 25°C” Knowledge, Thought, and Belief 18, 19, 26, 27 Biased Recommendations RAG poisoning recommends competitor products or unsafe routes Material and Economic Resources 17, 22, 23, 25 Resource Depletion Injected rule triggers max AC/heating to drain EV battery Reputation and Dignity 1, 12, 22, 23 Contextual Disclosure “Your psychiatrist appointment is at 4 PM” announced to colleagues Social Relationships and Trust 1, 12, 16, 20, 27 Delegated Action Abuse Agent sends “Transfer $10,000” to family using driver’s identity 4.2 Formal Attack Path Model The second dimension of our taxonomy addresses how assets are compromised. We model agentic systems as directed graphs and introduce a formal distinction between poison paths and trigger paths. 4.2.1 Graph Primitives We abstract agentic ecosystems into two node types and four edge types, as illustrated in Figure 2. Actor nodes represent entities with agency capable of processing inputs and producing outputs (e.g., in-car agent, external A2A agent, user). Datasource nodes represent passive data stores (e.g., long-term memory, message (Email, WhatsApp), contacts, calendar). The interaction primitives are: read (Actor retrieves data from Datasource), write (Actor stores data to Datasource), communicate (Actor initiates conversation with another Actor), and respond (Actor replies within an established conversation, conditional on prior communicate). Figure 3 illustrates two concrete attack paths", "quality": {"lang": "en", "gibberish_score": 0.0068, "char_len": 2498, "word_len": 350}, "dedupe": {"exact_hash": "38cb54e9e022a40d65cf91370b4eb4176aed23c4d83b29f049ef6fd4fff344ba", "simhash64": 9483765322399684304}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:194085cf88c6dc5616e9199cadc5fc635207f8ace40ecfd6ebeb472a5a18238c", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 8, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "(e.g., long-term memory, message (Email, WhatsApp), contacts, calendar). The interaction primitives are: read (Actor retrieves data from Datasource), write (Actor stores data to Datasource), communicate (Actor initiates conversation with another Actor), and respond (Actor replies within an established conversation, conditional on prior communicate). Figure 3 illustrates two concrete attack paths using these primitives: a longterm memory poisoning attack and a privilege escalation via human-in-the-loop manipulation. 8 Stappen et al. Fig. 2: Graph primitives for modeling agentic systems. Actors (entities with agency) interact via communicate/respond edges, while Datasources (passive stores) are accessed via read/write edges. (a) Memory poisoning: attacker manipulates agent to store malicious text, then triggers consumption. (b) Privilege escalation: manipulated WhatsApp message causes driver to issue malicious prompt. Fig. 3: Example attack paths. Numbers indicate sequential action order. 4.2.2 Poison Paths vs. Trigger Paths A central insight from our analysis is that attack path modeling requires distinguishing two types of paths. Poison paths describe the propagation of malicious data or influence from the attacker to the target asset, answering: “How does the malicious payload reach the victim?” Trigger paths describe the actions required to activate system components, to cause an actor to read from a poisoned datasource or to establish a communication channel enabling a respond edge, answering: “How do we cause the victim to consume the poison?” The key formal insight is that trigger paths are structurally identical to poison paths. A trigger path is a recursive “attack within an attack”, a path whose goal is to compel a specific action rather than to violate a final asset. This recursive structure is captured in our data model and visualized in Figure 4. 4.2.3 Poison Paths vs. Trigger Paths A central insight from our analysis is that attack path modeling requires distinguishing two types of paths. Poison Threat Modeling for In-Vehicle AI Agents 9 Fig. 4: Structure of an attack path as a sequence of attack steps. Each step may contain nested trigger chains for edge activation or consumption, creating a recursive “attack within attack” structure. paths describe the propagation of malicious data from the attacker to the target", "quality": {"lang": "en", "gibberish_score": 0.0051, "char_len": 2368, "word_len": 350}, "dedupe": {"exact_hash": "bb8d86225ab26dddea8b929b9687e79276f33772de6ee5194e021c7f08832a12", "simhash64": 1422302198457288470}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e8a622624784e807f4b0f9c5e50c6e2389473327814dcd3169f22233114fb1ad", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 9, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "for In-Vehicle AI Agents 9 Fig. 4: Structure of an attack path as a sequence of attack steps. Each step may contain nested trigger chains for edge activation or consumption, creating a recursive “attack within attack” structure. paths describe the propagation of malicious data from the attacker to the target asset, answering: “How does the malicious payload reach the victim?” Trigger paths describe the actions required to activate system components—to cause an actor to read from a poisoned datasource or to establish a communication channel enabling a respond edge, answering: “How do we cause the victim to consume the poison?” Real-world vulnerabilities demonstrate this pattern. The ‘CurXecute’ vulnerability [18] in the Cursor IDE exemplifies the two-stage structure: an attacker first plants a malicious prompt injection payload in a public Slack channel (the poison path). The poison lies dormant until a user’s agent is prompted with a benign query to interact with that channel (the trigger path), causing it to fetch and execute the hidden payload. The ‘EchoLeak’ vulnerability [17] in Microsoft 365 Copilot follows an identical structure: the poison is a malicious email sent to the victim’s inbox; the trigger is any benign query that causes Copilot’s RAG system to retrieve the malicious email alongside legitimate documents. The key formal insight is that trigger paths are structurally identical to poison paths—both are data flows through the graph. A trigger path is a recursive “attack within an attack”: a path whose goal is to compel a specific action rather than to violate a final asset. Figure 5a illustrates this recursive structure in a forced response injection scenario. 4.2.4 The Three-Phase Attack Step Each step in an attack path follows a three-phase pattern that captures the recursive trigger structure: Edge Activation Trigger (optional). Before certain edges can be used, prerequisites must be satisfied. Most critically, a respond edge requires an active communication channel—the target must have previously initiated contact via communicate. If no channel exists, the attacker must find a trigger chain that compels the target to initiate communication. When a channel is active, the 10 Stappen et al. (a) Edge Activation", "quality": {"lang": "en", "gibberish_score": 0.0088, "char_len": 2263, "word_len": 350}, "dedupe": {"exact_hash": "51b2412f83685c0c197221bb1e5c8d4e34b5dcbbd1972ae857227967ba91f332", "simhash64": 10572497269720211718}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6ebd234d585d29f8900244741d6dc95b87f75d6e22ae1ddc8c398c95931dbad1", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 10, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "satisfied. Most critically, a respond edge requires an active communication channel—the target must have previously initiated contact via communicate. If no channel exists, the attacker must find a trigger chain that compels the target to initiate communication. When a channel is active, the 10 Stappen et al. (a) Edge Activation Trigger. A active monitoring of DS_trigger enables automatic payload delivery through the conditional respond channel. (b) Consumption Trigger. Without automatic monitoring, the attacker must explicitly manipulate the car agent to force consumption of the staged payload. Fig. 5: Trigger mechanisms for activating dormant payloads. Both attacks stage poison on an intermediary, but differ in trigger mechanism: (a) exploits a datasource watch for automatic activation, while (b) requires explicit agent manipulation to induce consumption. Numbers indicate sequential attack steps. activation cost is zero; when inactive, the planner invokes the sub-search to find the cheapest activation path. Push Poison Action (required). This is the atomic action advancing the malicious payload: a write to a datasource, a communicate to initiate contact, or a respond within an established channel. Consumption Trigger (optional). After an attacker writes poison to a datasource, the poison remains dormant until consumed. The tool models two cases: (A) if the target actor has a watch on the datasource (automatic monitoring, e.g., polling for new emails), consumption is automatic with cost 1; (B) if no watch exists, the planner must find an alternative trigger path to compel the victim to read, increasing attack complexity. This structure creates a dynamic cost model: Cost(Step) = Cost(PushPoison) + Cost(ActivationTrigger) + Cost(ConsumptionTrigger) This recursive formulation enables discovery of complex, multi-stage attacks. For example, a persistence attack might follow: Attacker →Victim →Memory →Victim, where the attacker first compels the victim to write poison to its own long-term memory, then later triggers the victim to read from that memory. Threat Modeling for In-Vehicle AI Agents 11 Fig. 6: Overview of AgentHeLLM Pathfinder suggesting attack paths. The layout shows graph building tools (left), the system architecture with attack highlighting (center), and generated attack plans ranked by cost (right). 5 AgentHeLLM Attack Path Generator To demonstrate practical applicability, we", "quality": {"lang": "en", "gibberish_score": 0.0033, "char_len": 2428, "word_len": 350}, "dedupe": {"exact_hash": "13e2574939f0dac62369d9858f41bac87dbe3469775b112c1c60a5821983f555", "simhash64": 10563276490867853070}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:b75f80b5b7b8fc8629637409865a8d3660912dfdac6c0d9c578134d62d761923", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 11, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "that memory. Threat Modeling for In-Vehicle AI Agents 11 Fig. 6: Overview of AgentHeLLM Pathfinder suggesting attack paths. The layout shows graph building tools (left), the system architecture with attack highlighting (center), and generated attack plans ranked by cost (right). 5 AgentHeLLM Attack Path Generator To demonstrate practical applicability, we have implemented AgentHeLLM Pathfinder an open-source attack path suggestion tool that operationalizes our formal model. 5.1 Interactive Tool The tool provides an interactive workbench (Figure 6) where practitioners can model system architectures by placing Actor and Datasource nodes and connecting them with interaction primitives, designate attackers and target assets, run analysis to discover ranked attack paths, and visualize results as annotated graphs with sequential edge numbering. 5.2 Bi-Level Search Architecture The tool employs a bi-level search strategy reflecting the structural distinction between poison paths and trigger paths. A naive single-level search would treat trigger actions as regular attack steps, dramatically overestimating attack complexity: every dormant payload would appear to require the same effort as an actively propagating attack. The bi-level architecture instead recognizes that trigger paths serve a support function, computing their cost as a sub-problem that feeds into the main planner’s decisions. 12 Stappen et al. The Main Planner uses A* search to find optimal poison paths through the graph. Because attack step costs are variable (base cost plus trigger costs), A* is required for optimality, using precomputed heuristics for efficiency. The Sub-Search uses breadth-first search (BFS) to find shortest trigger chains when the main planner requires edge activation or consumption triggers. Since trigger actions are unit-cost, BFS suffices. When the main planner evaluates a potential attack step requiring triggers, it invokes the sub-search on-demand to calculate trigger costs. 5.3 Modeling Complex Attack Semantics The architecture handles domain-specific constraints. For conditional edges, respond edges require an active communication channel; the planner tracks channel state and computes activation costs. For datasource consumption, write actions produce dormant poison requiring consumption triggers; the tool models whether actors have automatic “watches” on datasources. For cycles, real attacks may involve persistence loops (e.g., Attacker →Victim →Memory →Victim); the tool allows cycles while preventing infinite", "quality": {"lang": "en", "gibberish_score": 0.0031, "char_len": 2540, "word_len": 350}, "dedupe": {"exact_hash": "11e3cdfe21f3a9c33150faf5f1c646c299b81d9f4b0320e3c45ea88fc38dfca8", "simhash64": 11148744510623532102}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:52b2ef2c7d5bf6618aeaca61ecb228f8f4117c61403e971155d02f6c28bfb3d7", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 12, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "the planner tracks channel state and computes activation costs. For datasource consumption, write actions produce dormant poison requiring consumption triggers; the tool models whether actors have automatic “watches” on datasources. For cycles, real attacks may involve persistence loops (e.g., Attacker →Victim →Memory →Victim); the tool allows cycles while preventing infinite loops through cost-bounding and stateful pruning. 6 Discussion 6.1 Applicability and Scope Our AgentHeLLM framework addresses the threat modeling phase of security engineering, providing structured methodology for anticipating threats. It complements rather than replaces implementation-level security measures. The human-centric asset taxonomy is particularly valuable for safety-critical domains where regulatory frameworks (ISO/SAE 21434, UNECE R155) require explicit traceability from technical vulnerabilities to potential harms. The attack path model applies to any agentic architecture that can be abstracted into actors, datasources, and interaction primitives, while demonstrated for in-vehicle agents, it generalizes to enterprise agents and other safety-critical deployments. 6.2 Limitations and Future Work Our model assumes a fixed architecture; dynamic agent discovery at runtime is not modeled. The approach is primarily topological, focusing on path existence rather than likelihood. All edges are treated as equally exploitable, though prompt injection difficulty varies with target guardrails. The engine requires explicitly modeled trigger paths, excluding scenarios where victims autonomously consume poison (e.g., periodic email polling). Empirical validation with production systems remains future work. Promising extensions include semantic cost models estimating injection difficulty, probability-based edges for ambient consumption, and counterfactual analysis for evaluating mitigations. Threat Modeling for In-Vehicle AI Agents 13 7 Conclusion The integration of LLM-based conversational agents into vehicles creates security challenges that existing frameworks inadequately address. By co-mingling assets with attack paths, current taxonomies provide shallow guidance for safetycritical contexts where systematic coverage is essential. This paper contributes AgentHeLLM, a threat modeling framework, grounded in the “separation of concerns” principle from safety-critical systems engineering. Our human-centric asset taxonomy enables systematic enumeration of potential harms, while our formal attack path model, distinguishing poison paths from trigger paths, captures the recursive structure of real-world attacks. The open-source AgentHeLLM Attack Path Generator demonstrates that this formalism can be operationalized into a practical workbench for security practitioners.", "quality": {"lang": "en", "gibberish_score": 0.0029, "char_len": 2783, "word_len": 350}, "dedupe": {"exact_hash": "469bc4da65ee1bef5c0e9d36e2f64c4690bba7d1ee9a5e7ff3353bacb5a57c7f", "simhash64": 11239098252840223766}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1ecff1839dad101e4688222f61b1177a4c7739d1f433f214c4f8f72fff01f173", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 13, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "engineering. Our human-centric asset taxonomy enables systematic enumeration of potential harms, while our formal attack path model, distinguishing poison paths from trigger paths, captures the recursive structure of real-world attacks. The open-source AgentHeLLM Attack Path Generator demonstrates that this formalism can be operationalized into a practical workbench for security practitioners. As agentic AI systems become increasingly prevalent in safety-critical domains, the methodological rigor we propose becomes essential for moving from reactive vulnerability patching to proactive threat anticipation. 7.0.1 Disclosure of Interests. The authors have no competing interests to declare. References 1. Road vehicles – cybersecurity engineering (iso/sae 21434) (2021) 2. Anthropic Team: Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. ArXiv abs/2209.07858 (2022) 3. BMW Group: Generative in-vehicle AI marks milestone in voice interaction (January 2026), https://www.bmwgroup.com/en/news/general/2026/alexa.html 4. Bucaioni, A., Weyssow, M., He, J., Lyu, Y., Lo, D.: A functional software reference architecture for llm-integrated systems. In: 2025 IEEE 22nd International Conference on Software Architecture Companion (ICSA-C). pp. 1–5 (2025) 5. Cerence: Cerence ai and volkswagen advance partnership to deliver next-gen in-car conversations, https://investors.cerence.com/news-events/press-releases/detail/3 48/smarter-faster-and-more-human-cerence-ai-and-volkswagen-advance-partner ship-to-deliver-next-gen-in-car-conversations 6. Cloud Security Alliance: Agentic AI Threat Modeling Framework: MAESTRO. https://cloudsecurityalliance.org/blog/2025/02/06/agentic-ai-threat-modeling-f ramework-maestro (Feb 2025) 7. Commission, E.: Driver distraction – summary. Tech. rep., European Commission, Directorate General for Transport (Feb 2018), https://road-safety.transport.ec.e uropa.eu/system/files/2021-07/ersosynthesis2018-driverdistraction-summary.pdf 8. Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., Fritz, M.: Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In: Proceedings of the 16th ACM workshop on artificial intelligence and security. pp. 79–90 (2023) 9. Karthaus, M., Wascher, E., Getzmann, S.: Effects of visual and acoustic distraction on driving behavior and eeg in young and older car drivers: a driving simulation study. Frontiers in aging neuroscience 10, 420 (2018) 14 Stappen et al. 10. Kirmayr, J., Stappen, L., André, E.: Car-bench: Evaluating the consistency and limit-awareness of llm agents under real-world uncertainty. arXiv preprint arXiv:2601.22027 (2026) 11. Kirmayr, J., Stappen, L., Schneider, P., Matthes, F., Andre, E.: CarMem: Enhancing long-term memory in LLM voice assistants through category-bounding. In: Proceedings of the 31st International Conference on Computational Linguistics: Industry", "quality": {"lang": "en", "gibberish_score": 0.0115, "char_len": 2966, "word_len": 350}, "dedupe": {"exact_hash": "ba21e7523f44556824447529840259f10c832e441c7528180dc3fe54e2d04327", "simhash64": 11240718932896482642}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6a21d2afcdf81ee46b387956cbb589986a6ed8dc68fc67166224e4f7fc6a29c8", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 14, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "André, E.: Car-bench: Evaluating the consistency and limit-awareness of llm agents under real-world uncertainty. arXiv preprint arXiv:2601.22027 (2026) 11. Kirmayr, J., Stappen, L., Schneider, P., Matthes, F., Andre, E.: CarMem: Enhancing long-term memory in LLM voice assistants through category-bounding. In: Proceedings of the 31st International Conference on Computational Linguistics: Industry Track. pp. 343–357. ACL, Abu Dhabi, UAE (Jan 2025) 12. Lee, J.D., See, K.A.: Trust in automation: Designing for appropriate reliance. Human Factors 46(1), 50–80 (2004) 13. LLC, G.: Agent2agent (a2a) protocol. https://google.github.io/A2A/ (2025) 14. Manico, J., Douglen, A., Hofesh, B., Ellis, A., Contributors, O.C.: Agentic ai: Threats and mitigations. https://genai.owasp.org/resource/agentic-ai-threats-a nd-mitigations/ (2024), accessed: 2025-05-14 15. Mercedes-Benz Group: Ai-powered conversational search (2025), https://group. mercedes-benz.com/innovations/product-innovation/technology/ai-powered-con versational-search.html 16. Mercedes-Benz Group AG: Cooperation with ai-powered conversational search. https://group.mercedes-benz.com/innovations/product-innovation/technology/a i-powered-conversational-search.html (Jan 2025), accessed: 2025-11-11 17. National Vulnerability Database, NIST: Cve-2025-32711 details — nvd. https: //nvd.nist.gov/vuln/detail/cve-2025-32711 (2025) 18. National Vulnerability Database, NIST: Cve-2025-54135 details — nvd. https: //nvd.nist.gov/vuln/detail/CVE-2025-54135 (2025) 19. Plaat, A., van Duijn, M.J., van Stein, N., Preuss, M., van der Putten, P., Batenburg, K.J.: Agentic large language models, a survey. ArXiv abs/2503.23037 (2025), https://api.semanticscholar.org/CorpusID:277451794 20. Salvendy, G.: Handbook of Human Factors and Ergonomics. Wiley, 5th edn. (2021) 21. Stappen, L., Dillmann, J., Striegel, S., Vögel, H.J., Flores-Herr, N., Schuller, B.W.: Integrating generative artificial intelligence in intelligent vehicle systems. In: 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC). pp. 5790–5797. IEEE (2023) 22. Strayer, D., Cooper, J., Turrill, J., Coleman, J., Hopman-Droste, R.: Measuring cognitive distraction in the automobile iii: A comparison of ten 2015 in-vehicle information systems. Tech. rep., AAA Foundation for Traffic Safety (oct 2015). https://doi.org/10.13140/RG.2.2.15375.20643 23. The MITRE Corporation: Mitre atlas™: Adversarial threat landscape for artificial intelligence systems. https://atlas.mitre.org/, accessed: 2025-11-16 24. United Nations Economic Commission for Europe (UNECE): Regulation no.155 – uniform provisions concerning the approval of vehicles with regards to cyber-security and cyber-security management system. https://unece.org/tran sport/documents/2021/03/standards/un-regulation-no-155-cyber-security-and-c yber-security (Mar 2021), accessed: 2025-11-11 25. Volkswagen AG: World premiere at ces: Volkswagen integrates chatgpt into its vehicles. https://www.volkswagen-newsroom.com/en/press-releases/world-pre miere-at-ces-volkswagen-integrates-chatgpt-into-its-vehicles-18048 (Jan 2024), accessed: 2025-11-11 26. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.R., Cao, Y.: React:", "quality": {"lang": "en", "gibberish_score": 0.0192, "char_len": 3234, "word_len": 350}, "dedupe": {"exact_hash": "6e15fc12a7f8689f86c3ca3135905c74cd147ba1f75817b825d3723ab7c9909f", "simhash64": 10357609885172335760}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:86098489087f274d7c0b583ad0331846662f2fd14995c1e47aad681543164f3c", "doc_id": "sha256:1066dc751283b7caaa31c2e25e381b4fb047c3ac61ddf7ab39adb9f8d1e2482c", "chunk_index": 15, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Agent2Agent Threats in Safety-Critical LLM Assistants.pdf", "page_start": null, "page_end": null, "text": "with regards to cyber-security and cyber-security management system. https://unece.org/tran sport/documents/2021/03/standards/un-regulation-no-155-cyber-security-and-c yber-security (Mar 2021), accessed: 2025-11-11 25. Volkswagen AG: World premiere at ces: Volkswagen integrates chatgpt into its vehicles. https://www.volkswagen-newsroom.com/en/press-releases/world-pre miere-at-ces-volkswagen-integrates-chatgpt-into-its-vehicles-18048 (Jan 2024), accessed: 2025-11-11 26. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.R., Cao, Y.: React: Synergizing reasoning and acting in language models. In: The eleventh international conference on learning representations (2022)", "quality": {"lang": "en", "gibberish_score": 0.019, "char_len": 685, "word_len": 66}, "dedupe": {"exact_hash": "fa7902e4f8f974388820aa75ecd2c3324b1daf4a43e3c85297d1385746970312", "simhash64": 11481323965046669076}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a89f4164e38396dbcbea874829d27b6faa144c267510641de5e6618e93820724", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 0, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 20 August 2024 More details about this document Latest published version: https://www.w3.org/reports/ai-web-impact/ History: Commit history Editor: Dominique Hazael-Massieux Feedback: GitHub w3c/ai-web-impact (pull requests, new issue, open issues) Translations 简体中文 (Simplified Chinese) Copyright © 2024 World Wide Web Consortium. W3C® liability, trademark and permissive document license rules apply. Abstract This document proposes an analysis of the systemic impact of AI systems, and in particular ones based on Machine Learning models, on the Web, and the role that Web standardization may play in managing that impact. Status of This Document This document is intended to capture the current shared understanding of the W3C Team on the current and expected impact of developments linked to Artificial Intelligence systems on the Web, and identifying explorations the World Wide Web Consortium community has started or ought to be starting, to manage that impact. It does not represent any consensus from the W3C Membership nor is it a standardization document. 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 1/16 1. 2. 2.1 3. 4. 4.1 4.1.1 4.1.2 4.2 4.3 4.4 4.5 4.5.1 5. The document was authored by Dominique Hazaël-Massieux (dom@w3.org), with significant contributions from the rest of the W3C Team. This document aims first and foremost to help structure discussions on what may be needed at the standardization level to make the systemic impact of AI (and specifically, Machine Learning models) less harmful or more manageable. It is bound to be incomplete and sometimes wrong - we are gathering input and feedback in GitHub, preferably before June 30, 2024. If it's not feasible for you to use GitHub, send comments in e-mail to: public-team-reportcomments@w3.org. Please put your comments in the body of the message, not as an attachment. Start your e-mail subject line with: [AI Report]. Depending on the feedback received, possible next steps include more in-depth stakeholder interviews, a dedicated W3C Workshop, or developing a standardization roadmap. Table of Contents Abstract Status of This Document Executive Summary", "quality": {"lang": "en", "gibberish_score": 0.0066, "char_len": 2287, "word_len": 350}, "dedupe": {"exact_hash": "e9a6367a53286602ef28adc592d66eea7d9aa8c83646f638f43e52c90f376a9d", "simhash64": 11226715552787842974}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ea584ccfa5e66a5ea32339e49f088ad8f08f108d19c2f3c4991c86b62d56f5ef", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 1, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "in the body of the message, not as an attachment. Start your e-mail subject line with: [AI Report]. Depending on the feedback received, possible next steps include more in-depth stakeholder interviews, a dedicated W3C Workshop, or developing a standardization roadmap. Table of Contents Abstract Status of This Document Executive Summary Introduction Terminology Intersections between AI systems and the Web Ethics and societal impact Respecting autonomy and transparency Transparency on AI-generated content Transparency on AI-mediated services Right to privacy and data control Safety and security Sustainability Balancing content creators incentives and consumers rights Comparison with search engines Impact of AI systems on interoperability 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 2/16 A. A.1 References Informative references Machine Learning models support a new generation of AI systems. These models are often trained on a large amount of Web content, deployed at scale through web interfaces, and can be used to generate plausible content at unprecedented speed and cost. Given the scope and scale of these intersections, this wave of AI systems is having potential systemic impact on the Web and some of the equilibriums on which its ecosystem had grown. This document reviews these intersections through their ethical, societal and technical impacts and highlights a number of areas where standardization, guidelines and interoperability could help manage these changes: a consent mechanism for the use of Web content in training pipelines, labeling content as computer-generated, surfacing training sources in model cards, exposing model-backed Web APIs, personal data stores to reduce risk of private data exposure, strengthening credentials and identity mechanisms in light of new impersonation risks, an evaluation framework for the environmental impact of Web standards, a framework to manage interoperability based on model inference, including for nondeterministic models. We are seeking input from the community on proposals that could help make progress on these topics, and on other topics that this document has failed to identify. Recent developments in the decades-long computer science field of Artificial Intelligence have made a number of systems emerge that already have started having systemic", "quality": {"lang": "en", "gibberish_score": 0.0017, "char_len": 2389, "word_len": 350}, "dedupe": {"exact_hash": "d80cb5a4242b31cfcb365e94a5f66041b67fbb542bc9ac17923b4ffeb7328240", "simhash64": 11226997027764798358}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:637c6048b61fa8cb42cbf557e4fd76bc53b57dc5214f4377f2903039275be467", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 2, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "seeking input from the community on proposals that could help make progress on these topics, and on other topics that this document has failed to identify. Recent developments in the decades-long computer science field of Artificial Intelligence have made a number of systems emerge that already have started having systemic impacts on the Web and can be § 1. Executive Summary § 2. Introduction 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 3/16 expected to further transform a number of shared expectations on which the health of the Web had relied so far. To help structure a conversation within the W3C community (and possibly with other Web-related standards organization) about these transformations, this document collects the current shared understanding among the W3C Team of the intersections of \"Artificial Intelligence\", more specifically in the field of Machine Learning models (including Large Language Models and other so called generative AI models), with the Web as a system, and ongoing W3C development in this space. It is also a goal to raise questions about additional explorations that may be needed as further developments in this space arise. That current understanding is bound to be incomplete or sometimes plain wrong; we hope that by publishing this document and inviting community reviews on it, we iteratively improve this shared understanding and help build a community roadmap to increase the positive impact and decrease the harms that are emerging in this intersection. The term \"Artificial Intelligence\" covers a very broad spectrum of algorithms, techniques and technologies. [ISO/IEC-22989] defines Artificial Intelligence as \"research and development of mechanisms and applications of AI systems\", with AI systems being \"an engineered system that generates outputs such as content, forecasts, recommendations or decisions for a given set of humandefined objectives\". At the time of the writing of this document in early 2024, the gist of the Web ecosystem conversation on Artificial Intelligence is mostly about systems based on Machine Learning (\"process of optimizing model parameters through computational techniques, such that the model's behavior reflects the data or experience\") and its", "quality": {"lang": "en", "gibberish_score": 0.0031, "char_len": 2291, "word_len": 350}, "dedupe": {"exact_hash": "f0f77d8f86b874cea2ed70b011d8831064609fdb6de6bd4c9d92fa3011dc4028", "simhash64": 11226994760021541270}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:19db1b2792a741c957e213e46e7daf1fdc37347d73f73a1a4040a5f6c4e8bffe", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 3, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "At the time of the writing of this document in early 2024, the gist of the Web ecosystem conversation on Artificial Intelligence is mostly about systems based on Machine Learning (\"process of optimizing model parameters through computational techniques, such that the model's behavior reflects the data or experience\") and its software manifestation, Machine Learning models (\"mathematical construct that generates an inference or prediction based on input data or information\"). While we acknowledge the much broader meaning of Artificial Intelligence and its intersection with a number of other Web- and W3C-related activities (e.g., the Semantic Web and Linked Data), this document willfully focuses only on the current conversation around the impact that these Machine Learning models are bringing to the Web. We further acknowledge that this document has been developed during, and is partially a response to, a cycle of inflated expectations and investments in that space. That situation underlines the need for a framework to structure the conversation. Because of this focus on Machine Learning, this document analyzes AI impact through the two main phases needed to operate Machine Learning models: training (\"process to determine or to improve the § 2.1 Terminology 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 4/16 parameters of a Machine Learning model, based on a Machine Learning algorithm, by using training data\") and inference (the actual usage of these models to produce their expected outcomes), which we also casually refer as running a model. A major role the Web plays is as a platform for content creators to expose at scale their content to content consumers. AI directly relates to these two sides of the platform: In a number of cases, models are trained based on content crawled from the Web; the combination of scale and structure in that content (made possible by the underlying standards) has made it an invaluable source of training data that backs some of the most visible results in recent AI developments, such as large language models or image/video generators; Conversely, a number of these AI models can be", "quality": {"lang": "en", "gibberish_score": 0.0027, "char_len": 2204, "word_len": 350}, "dedupe": {"exact_hash": "343d9e4bbc4853164cc4fbafb7c1a3821c72422ff62340b1d0288976f44a8840", "simhash64": 11208417480278901126}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:898c59dba5d1ac21d281daa5dab0bc8bc682162dd878c7cf6247b4ac45ba1209", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 4, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "scale and structure in that content (made possible by the underlying standards) has made it an invaluable source of training data that backs some of the most visible results in recent AI developments, such as large language models or image/video generators; Conversely, a number of these AI models can be used to generate content at unprecedented scale, which the reach of the Web allows to deploy seamlessly to the billions of users of the platform. When looking more specifically at the browser-mediated part of the Web which remains primarily a client/server architecture, AI models can be run either on the server-side or on the client-side (and somewhat more marginally at this point, in a hybrid-fashion between the two). On the client side, they can either be provided and operated by the browser (either at the user's request, or at the application's request), or entirely by the client-side application itself. It's also worth noting that as AI systems are gaining rapid adoption, their intersection with the Web is bound to evolve and possibly trigger new systemic impact; for instance, emerging AI systems that combine Machine Learning models and content loaded from the Web in real-time may induce revisiting in depth the role and user experience of Web browsers in consuming or searching content. The W3C's Technical Architecture Group Ethical Web Principles [ethical-web-principles] includes ensuring \"the Web should not cause harm to society\". As described above, the Web is already a key enabler in some of the recent developments in Artificial Intelligence, and the usage and impact of Artificial Intelligence is being multiplied in scale through its distribution via the Web. This calls for the W3C community as stewards of the Web to understand potential harms emerging from that combination and to identify potential mitigations to these harms. § 3. Intersections between AI systems and the Web § 4. Ethics and societal impact 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 5/16 The Ethical Principles for Web Machine Learning [webmachinelearning-ethics] started in the Web Machine Learning Working Group combine values and", "quality": {"lang": "en", "gibberish_score": 0.0036, "char_len": 2202, "word_len": 350}, "dedupe": {"exact_hash": "80e53ffbb758e7654fe2ad285f0040f6f2f33f27b08c19c163c3df8074c4f323", "simhash64": 11208417480278524310}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:64954e7b4fcbeeac2ad57813be0ab6c957083606f6d8925df0b581673dd6a565", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 5, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "AI systems and the Web § 4. Ethics and societal impact 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 5/16 The Ethical Principles for Web Machine Learning [webmachinelearning-ethics] started in the Web Machine Learning Working Group combine values and principles from the UNESCO Recommendation on the Ethics of Artificial Intelligence [UNESCO-AI] with Web-specific principles from the Ethical Web Principles to identify 4 values and 11 principles that integration of Machine Learning on the Web should follow, and which have helped structure this document. Recent AI systems are able to assist in the partial or complete creation of content (textual, graphic, audio and video) at a level of (at least superficially) credible quality and in quantities beyond that developed by humans. This provides both opportunities and risks for content creators, but more importantly, it creates a systemic risk for content consumers in no longer being able to distinguish or discover authoritative or curated content in a sea of credible (but either possibly or willfully wrong) generated content. That need is pressing directly for end-users as they individually consume content, but also applies to agents that end-users rely on: typically, search engines would likely benefit from transparency on purely AI generated content. Somewhat ironically, crawlers used to train AI models are likely to need such a signal as well, since training models on the output of models may create unexpected and unwanted results. We do not know of any solution that could guarantee (e.g., through cryptography) that a given piece of content was or was not generated (partially or entirely) by AI systems. That gap unfortunately leaves a systemic risk in terms of misinformation and spam that should be of grave concern for the health of the Web as a content distribution platform and of society as a whole. § 4.1 Respecting autonomy and transparency § 4.1.1 Transparency on AI-generated content 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 6/16 A plausible role of standards in this space would be", "quality": {"lang": "en", "gibberish_score": 0.005, "char_len": 2189, "word_len": 350}, "dedupe": {"exact_hash": "c16af3e82caf26a7f6b8d3de420d2ae868840921eb8b6e3f4bf201745b410d96", "simhash64": 11226451669998749590}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0916878c6b0ecb97edcf92b1820fcdea03ada3a01178fcd4c366397afac0aa78", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 6, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "distribution platform and of society as a whole. § 4.1 Respecting autonomy and transparency § 4.1.1 Transparency on AI-generated content 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 6/16 A plausible role of standards in this space would be to at least facilitate the labeling of content to indicate whether it is the result of a computer-generated process. While such labels are unlikely to be enforceable through technical means, they could gain broad adoption with a combination of being automatically added by AI systems (possibly with enough friction that removing them would be at least somewhat costly at scale), and possibly serve as hooks in the regulatory context. A number of proposals have already emerged in this space, which may benefit from more visibility, discussion and ultimately, scalable deployment: C2PA Guidance for AI and Machine Learning [C2PA-AI] IPTC synthetic media [IPTC-DST] and its matching representation in Schema.org [schemaorg] Proposal: Meta Tag for AI Generated Content in [HTML] (individual submission) An area that could be explored is role Web browsers might play in surfacing labelling or provenance information of content, e.g., embedded content such as images or video. This can currently be done by publishers' websites or independent verifiers, but integrating this capability into the browser could make the information more convenient for users to access, as well as being independent of any particular publisher or website where the same content may be viewed. A well-known issue with relying operationally on Machine Learning models is that they will integrate and possibly strengthen any bias (\"systematic difference in treatment of certain objects, people or groups in comparison to others\" [[[ISO/IEC-22989]) in the data that was used during their training. Bias commonly occurs in other algorithms and human decision processes. Where AI systems make that bias a bigger challenge is because these models are at this point harder to audit and amend since they operate mostly as a closed box. Such bias will disproportionately affect users whose expected input or output is less well represented in training data (as e.g., discussed in", "quality": {"lang": "en", "gibberish_score": 0.0031, "char_len": 2245, "word_len": 350}, "dedupe": {"exact_hash": "bbac3529724f5e6fbb591b66186bc037f4d34274db5a2b859713a61e8626ec4e", "simhash64": 11217426534925960582}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:9a23ed22c4d879ee8786fd7acbd020057d7ad6a707224f275bff15bfbf78aaf0", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 7, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "systems make that bias a bigger challenge is because these models are at this point harder to audit and amend since they operate mostly as a closed box. Such bias will disproportionately affect users whose expected input or output is less well represented in training data (as e.g., discussed in the report from the 2023 AI & Accessibility research symposium [WAI-AI]), which intuitively is likely to correlate strongly with users already disenfranchised by society and technology - e.g., if your language, appearance or behavior doesn't fit the mainstreamexpected norm, you're less likely to feature in mainstream content and thus less visible or misrepresented in training data. § 4.1.2 Transparency on AI-mediated services 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 7/16 Until better tools emerge to facilitate at least the systematic detection of such bias, encouraging and facilitating the systematic publication of information on whether a Machine Learning model is in use, and how such a model was trained and checked for bias may help end-users make more informed choices about the services they use (which, of course, only helps if they have a choice in the first place, which may not apply e.g., to some government-provided services). Model cards for Model Reporting [MODEL-CARDS] are one of the approaches that were discussed in the 2020 W3C Workshop on Web and Machine Learning [W3C-ML-WS]. Assuming this reporting provides meaningful and actionable transparency, a question for (the) technical standardization would be how such cards should be serialized and made discoverable on the Web. W3C should look at a particular category of model deployments: models that are used by browser engines themselves to fulfill API requests. A number of Web browser APIs already expose (more or less explicitly) output of Machine Learning models: Web Speech API [SPEECH-API] Accelerated Shape Detection API [SHAPE-DETECTION-API] Background blur, face detection, gaze correction controls in Media capture As described below, these APIs also raise engineering questions about how to ensure they provide the level of interoperability that have been expected from more traditionally deterministic algorithms. Models trained", "quality": {"lang": "en", "gibberish_score": 0.0026, "char_len": 2281, "word_len": 350}, "dedupe": {"exact_hash": "6bb20beb2e3759732e68ee3d6bcc8209455bf3096da1d1ff9a5ad269081a08fe", "simhash64": 11217708078655717846}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d9a13ee8d7dc0d2db4878d963b8d2629220cd82a656b1f43c7bdf9bf0ff6897d", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 8, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "models: Web Speech API [SPEECH-API] Accelerated Shape Detection API [SHAPE-DETECTION-API] Background blur, face detection, gaze correction controls in Media capture As described below, these APIs also raise engineering questions about how to ensure they provide the level of interoperability that have been expected from more traditionally deterministic algorithms. Models trained on un-triaged or partially triaged content off the Web are bound to include personally identifiable information (PII). The same is true for models trained on data that users have chosen to share (for public consumption or not) with service providers. These models can often be made to retrieve and share that information with any user who knows how to ask, which breaks expectations of privacy for those whose personal information was collected, and is likely to be in breach with privacy regulations in a number of jurisdictions. Worse, they create risks for new types of attacks (see 4.3 Safety and security). § 4.2 Right to privacy and data control 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 8/16 While the exclusion rules discussed in the context of content creation could partially help with the first situation, they would not help with the second one. This problem space is likely to be under heavy regulatory and legal scrutiny. From a technical standardization perspective, beyond labeling content, the emergence of user data being repurposed for model training and some of the pushback it is generating may bring renewed momentum (from user and service provider alike) behind decentralized architectures that leave user data under less centralized control (as illustrated by the recent widening adoption of Activity Streams). A particularly relevant instance of that pattern is emerging with so-called personal data stores: these provide ways for users to exert more fine-grained control of their data, by separating more clearly the roles of data store and data processor (which, in a traditional cloud infrastructure, would otherwise typically be handled by a single entity). That topic has most recently surfaced in W3C through the proposed charter for a SOLID Working Group late 2023 (a charter", "quality": {"lang": "en", "gibberish_score": 0.0022, "char_len": 2252, "word_len": 350}, "dedupe": {"exact_hash": "ebe6947b3a08fe478ce3b54a23ff7244b8b4f02fc8b74f295e8d4a3083599742", "simhash64": 11208417480278393478}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e63114612c206cb44f918c886685a4fc82e291e207bf25886fbc728e6b2d7e92", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 9, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "their data, by separating more clearly the roles of data store and data processor (which, in a traditional cloud infrastructure, would otherwise typically be handled by a single entity). That topic has most recently surfaced in W3C through the proposed charter for a SOLID Working Group late 2023 (a charter that the W3C community has recognized as important, but where there is not yet consensus). Allowing to run a model on personal data without uploading that data to a server is one of the key motivations behind the browser Web Neural Network API [WEBNN] which, completing the computing capabilities already provided by WebAssembly [WASM-CORE-2] and WebGPU [WEBGPU], provides additional Machine Learning specific optimizations to run models efficiently from within the browser (and thus, on the end-user device). A number of Machine Learning models have significantly lowered the cost of generating credible textual, as well as audio and video (real-time or recorded) impersonations of real persons. This creates significant risks of upscaling the capabilities of phishing and other types of frauds, and thus raising much higher the barriers to establish trust in online interactions. If users no longer feel safe in their digitally-mediated interactions, the Web will no longer be able to play its role as a platform for these interactions. § 4.3 Safety and security 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 9/16 This points towards even stronger needs for robust identity and credential management on the Web. The work of the Verifiable Credentials Working Group allows to express credentials in a cryptographically secure, privacy-preserving, and machine-verifiable way [VC-DATA-MODEL]. The proposed work to better integrate Federated Identity systems into browsers [FEDCM] and the emerging one on surfacing digital credentials to Web content [DIGITAL-CREDENTIALS] are likely part of mitigating the risks associated with these new impersonation threats. Training and running Machine Learning models can prove very resource-intensive, in particular in terms of power- and water-consumption. The imperative of reducing humanity's footprint on natural resources should apply particularly strongly to the technologies that standardization helps deploy at scale. There is", "quality": {"lang": "en", "gibberish_score": 0.0021, "char_len": 2339, "word_len": 350}, "dedupe": {"exact_hash": "80f53eb32120f44223b9ec42f3b9ce190f42c5bc5100c14d611aae77d082ac34", "simhash64": 11240507826646663430}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:dec4625b9bf3851fc84c4ad231a32e73e354b86323965f2bef36e5f871818d75", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 10, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "the risks associated with these new impersonation threats. Training and running Machine Learning models can prove very resource-intensive, in particular in terms of power- and water-consumption. The imperative of reducing humanity's footprint on natural resources should apply particularly strongly to the technologies that standardization helps deploy at scale. There is relatively new but promising work in the Sustainable Web Design Community Group (a candidate to become a standardization Working Group) to explain how to use Web technologies in a sustainable way. The Green Software Foundation Software Carbon Intensity Working Group is developing a score to calculate carbon footprint of software applications. W3C still lacks a well-defined framework to evaluate the environmental impact of its standards. Given the documented high environmental impact of AI systems, it will surely become more important that W3C groups that are expected to accelerate the deployment of Machine Learning models take a proactive approach in exploring and documenting how they envision the environmental impact of their work, and possible mitigations they might identify. Some of the largest and most visible Machine Learning models are known or assumed to have been trained with materials crawled from the Web, without the explicit consent of their creators or publishers. § 4.4 Sustainability § 4.5 Balancing content creators incentives and consumers rights 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 10/16 The controversy that has emerged from that situation is being debated (and in some cases, arbitrated) through the lens of copyright law. It is not our place to determine if and how various copyright legislation bears on that particular usage. Beyond legal considerations, the copyright system creates a (relatively) shared understanding between creators and consumers that, by default, content cannot be redistributed, remixed, adapted or built upon without creators' consent. This shared understanding made it possible for a lot of content to be openly distributed on the Web. It also allowed creators to consider a variety of monetization options (subscription, pay per view, advertising) for their content grounded on the assumption that consumers will always reach their pages.", "quality": {"lang": "en", "gibberish_score": 0.0026, "char_len": 2342, "word_len": 350}, "dedupe": {"exact_hash": "0a322e96911c20ac56424ae3ab25c2a2d5d966898c7ef26cc1214a07828cec83", "simhash64": 11227032212136642454}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:19de5c4eb46189e804f15589113a20dadb09da69192127078c3c205386f45682", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 11, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "creators' consent. This shared understanding made it possible for a lot of content to be openly distributed on the Web. It also allowed creators to consider a variety of monetization options (subscription, pay per view, advertising) for their content grounded on the assumption that consumers will always reach their pages. A number of AI systems combine (1) automated large-scale consumption of Web content, and (2) production at scale of content, in ways that do not recognize or otherwise compensate content it was trained from. While some of these tensions are not new (as discussed below), systems based on Machine Learning are poised to upend the existing balance. Unless a new sustainable equilibrium is found, this exposes the Web to the following undesirable outcomes: Significantly less open distributed content (which would likely have a disproportionate impact on the less wealthy part of the population) A less appealing platform to distribute content A less direct risk may emerge from changes in copyright laws meant to help to rebalance the situation but which would reduce the rights from content consumers and then undermine the value of the Web as a platform for which content distribution is a key value proposition. A number of the tensions emerging around the re-use of content crawled at scale from the Web have a long history given the central role that search engines play for the Web. Indeed, search engines provide (and absorb) value from their ability to retrieve and organize information from content on the Web, and they heavily rely on the standardized infrastructure this content is built on to achieve these results. The more or less implicit contract that emerged between search engines and content providers has been that search engines can retrieve, parse and partially display content from the providers, in exchange of bringing more visibility and traffic to them. A further assumption has been encoded in the § 4.5.1 Comparison with search engines 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 11/16 way the Web operates that this contract is the default for", "quality": {"lang": "en", "gibberish_score": 0.0023, "char_len": 2157, "word_len": 350}, "dedupe": {"exact_hash": "84563e2c5fc9e581f869b0e4e7fcb7c698de84dcea2119f1b9404c6b74f1c298", "simhash64": 10636741802579052438}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:aa159ce7671eaffcd61773e6fc576f407c78a00adcba325aac9a40751adc321e", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 12, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "visibility and traffic to them. A further assumption has been encoded in the § 4.5.1 Comparison with search engines 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 11/16 way the Web operates that this contract is the default for anyone making content available publicly on the Web, with an opt-out mechanism encoded via the robots.txt directives [RFC9309]. Over time, in addition to the links to sites matching the user's query, search engines have integrated more ways to surface content directly from the target Web sites: either through rich snippets (typically made possible by the use of schema.org metadata) or through embedded preview (e.g., what the AMP project enabled). These changes were frequently accompanied by sometimes challenging discussions around the balance between bringing additional visibility to crawled content and reducing the incentive from end-users to visit the source website (e.g., because they may have already received sufficient information from the search results page). In a certain number of cases, AI systems are used as an alternative or complement to what users would traditionally have used a search engine for (and indeed, are increasingly integrated into search engine interfaces). So it seems useful to explore to what extent the lessons learned from the evolutionary process balancing the needs from search engines and from content creators can inform the discussion on crawlers used to train Machine Learning models. In making that comparison, it's also important to note significant differences: The implicit contract that content creators expect from search engines crawlers –i.e., that they will bring exposure to their content– does not have a systematic equivalent for content integrated into AI systems; while some such systems are gaining the ability to point back to the source of their training data used in a given inference, this is hardly a widespread feature of these systems, nor is it obvious it could be applied systematically (e.g., would linking back to sources for a generated image even make sense?); even if it could, fewer sources would likely be exposed than in a typical search engine results", "quality": {"lang": "en", "gibberish_score": 0.0036, "char_len": 2220, "word_len": 350}, "dedupe": {"exact_hash": "c3ea8bb861329e3ef31f29f9295863a1c4a440a431d0398570547dba67a01145", "simhash64": 10569187808168502046}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:04325412e0c1c669227263ab839de9e830da605c888a24489b031f1e0841152c", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 13, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "given inference, this is hardly a widespread feature of these systems, nor is it obvious it could be applied systematically (e.g., would linking back to sources for a generated image even make sense?); even if it could, fewer sources would likely be exposed than in a typical search engine results page, and the incentives for the user to follow the links would likely be substantially lower. robots.txt directives allow specific rules to be given to specific crawlers based on their user agent; while this has been practically manageable when dealing with (for better or for worse) few well-known search engine crawlers, expecting content creators to maintain potential allow- and block-lists of the rapidly expanding number of crawlers deployed to retrieve training data seems unlikely to achieve sustainable results. Given the likely different expectations around the quid-pro-quo of crawling in the context of AI systems, it is not obvious that the permission-less pattern inherited from the early days of the Web (robots.txt was designed in 1994) would be a satisfactory match to ensure the long term sustainability of content publication on the Web (itself presumably in the long term interest of AI crawlers themselves). 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 12/16 In general, a possibly helpful line of inquiry for standards in this space would be to identify solutions that help content producers and AI crawlers to find agreeable terms, ideally at a scale that would make it appealing to all parties. Several groups and individuals have been exploring how to make it possible for content publishers to express their willingness to get their content used for training Machine Learning models: The Text and Data Mining Reservation Protocol Community Group has developed the TDM Reservation Protocol (TDMRep) [TDMRep] to \"express the reservation of rights relative to text & data mining applied to lawfully accessible Web content\" Discussions are starting in IETF around updating the robots.txt directives in this context; among others, the update robots.txt Community Group proposes to add an opt-in mechanism to the robots.txt directives. The JournalList", "quality": {"lang": "en", "gibberish_score": 0.0027, "char_len": 2231, "word_len": 350}, "dedupe": {"exact_hash": "5fe7808d37ad33532ae2145c27ed82500b8c95271dd5ee8884be85a36782a501", "simhash64": 11223054179068523406}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c5f1b308f94b628d0c50ca337f3e17c68e6409470661324f0f59ebda542d5d95", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 14, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "to \"express the reservation of rights relative to text & data mining applied to lawfully accessible Web content\" Discussions are starting in IETF around updating the robots.txt directives in this context; among others, the update robots.txt Community Group proposes to add an opt-in mechanism to the robots.txt directives. The JournalList association is looking into revising their trust.txt [TRUST.TXT] to let publishers express whether their content can be used for training models A key part of W3C's vision for the Web [w3c-vision] is to ensure the Web is developed around principles of interoperability: that is, for technologies that W3C codifies as Web standards, to ensure they are implemented and deployed in a way that will work the same across products, allowing for greater choice for users and fostering the long term viability of the content. When the algorithm on which interoperability relies is deterministic, ensuring interoperability is a matter of describing in sufficient detail and clarity the said algorithm, and running sufficient testing on the products to verify they achieve the intended result. The move to more algorithmic specifications [design-principles] and thorough automated testing (e.g., through the Web Platform Tests project) has largely been driven by the goal of providing a robust interoperable platform. § 5. Impact of AI systems on interoperability 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 13/16 As discussed above, Machine Learning models are already finding their way into standardized Web APIs. These creates two challenges to our interoperability goals: Machine Learning models are mostly not built or described as a series of algorithmic steps. If a given standardized behavior is expected to be best fulfilled by Machine Learning models, how should that behavior be specified? How can it be tested to a level that sufficiently verifies an interoperable outcome across products that would use different models? What impact would it have on the fingerprinting surface of the browsers? A number of important Machine Learning models are not deterministic; if or when some of these non-deterministic models get exposed in standardized APIs, this consistency question is", "quality": {"lang": "en", "gibberish_score": 0.0039, "char_len": 2279, "word_len": 350}, "dedupe": {"exact_hash": "624e0f92422fa86055d7b452aebf809393c93106ba54e12af94a6987f39134e7", "simhash64": 11225587453858792334}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:56ecbbba258c6db155bf87fd760be5810e2b4250d719ec5eabc62d2eca363497", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 15, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "verifies an interoperable outcome across products that would use different models? What impact would it have on the fingerprinting surface of the browsers? A number of important Machine Learning models are not deterministic; if or when some of these non-deterministic models get exposed in standardized APIs, this consistency question is no longer limited to two products using two different models, since a given input would no longer produce a predetermined output. It is not clear to us at the moment how to prepare for interoperable behaviors based on non-deterministic models, which probably raises the question of whether and how such models should be acceptable as part of interoperable implementations. A possible consequence of these challenges is a reduction of the scope of what can be meaningfully made interoperable and standardized as a possibly growing number of features get mediated by Machine Learning models (similar to the postulated impact of growing capabilities of web applications on the need to standardize protocols). In that context, discussions e.g., around AI-based codecs point towards possible significant changes in the interoperability landscape. [C2PA-AI] Guidance for Artificial Intelligence and Machine Learning. C2PA. URL: https://c2pa.org/specifications/specifications/1.3/ai-ml/ai_ml.html [design-principles] Web Platform Design Principles. Lea Verou. W3C. 18 July 2024. W3C Working Group Note. URL: https://www.w3.org/TR/design-principles/ [DIGITAL-CREDENTIALS] Digital Credentials. WICG. March 2024. URL: https://wicg.github.io/digital-identities/ § A. References § A.1 Informative references 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 14/16 [ethical-web-principles] Ethical Web Principles. Daniel Appelquist; Hadley Beeman; Amy Guy. W3C. 13 August 2024. W3C Working Group Note. URL: https://www.w3.org/TR/ethical-web-principles/ [FEDCM] Federated Credential Management API. W3C. Draft Community Group Report. URL: https://fedidcg.github.io/FedCM/ [HTML] HTML Standard. Anne van Kesteren; Domenic Denicola; Ian Hickson; Philip Jägenstedt; Simon Pieters. WHATWG. Living Standard. URL: https://html.spec.whatwg.org/multipage/ [IPTC-DST] Digital Source Type vocabulary. IPTC. URL: https://cv.iptc.org/newscodes/digitalsourcetype/ [ISO/IEC-22989] Artificial intelligence concepts and terminology. ISO/IEC. July 2022. Published. URL: https://www.iso.org/standard/74296.html [MODEL-CARDS] Model Cards for Model Reporting. M. Mitchell; S. Wu; A. Zaldivar; P. Barnes; L. Vasserman; B. Hutchinson; E. Spitzer; I. Deborah Raji; T. Gebru. 14 Juan 2019. URL: https://arxiv.org/pdf/1810.03993.pdf [RFC9309] Robots Exclusion Protocol. M. Koster; G. Illyes; H.", "quality": {"lang": "en", "gibberish_score": 0.0193, "char_len": 2749, "word_len": 350}, "dedupe": {"exact_hash": "ebbc9bf9197ac10dd7f5ccae99dd321f2d8b874c92d6879404a9b3dc80f582d9", "simhash64": 11231210356859176838}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0783de46146097a637d330697899be50b1a8fd966e4983a845457b999a6e915a", "doc_id": "sha256:7cdb75ced12dcba34183d550db27db631c079dfb7a88a10201cd5b44c240959c", "chunk_index": 16, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\AI & the Web_ Understanding and managing the impact of Machine Learning models on the Web.pdf", "page_start": null, "page_end": null, "text": "Artificial intelligence concepts and terminology. ISO/IEC. July 2022. Published. URL: https://www.iso.org/standard/74296.html [MODEL-CARDS] Model Cards for Model Reporting. M. Mitchell; S. Wu; A. Zaldivar; P. Barnes; L. Vasserman; B. Hutchinson; E. Spitzer; I. Deborah Raji; T. Gebru. 14 Juan 2019. URL: https://arxiv.org/pdf/1810.03993.pdf [RFC9309] Robots Exclusion Protocol. M. Koster; G. Illyes; H. Zeller; L. Sassman. IETF. September 2022. Proposed Standard. URL: https://www.rfc-editor.org/rfc/rfc9309 [schema-org] Schema.org. W3C Schema.org Community Group. W3C. 6.0. URL: https://schema.org/ [SHAPE-DETECTION-API] Accelerated Shape Detection in Images. WICG. cg-draft. URL: https://wicg.github.io/shapedetection-api/ [SPEECH-API] Web Speech API. WICG. cg-draft. URL: https://wicg.github.io/speech-api/ [TDMRep] Final Community Group Report. Text and Data Mining Reservation Protocol Community Group. February 2024. URL: https://www.w3.org/community/reports/tdmrep/CG-FINAL-tdmrep20240202/ [TRUST.TXT] Specification for trust.txt file and underlying system. JournalList.net. May 2020. URL: https://journallist.net/reference-document-for-trust-txt-specifications 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 15/16 [UNESCO-AI] Recommendation on the Ethics of Artificial Intelligence. UNESCO. 2021. URL: https://unesdoc.unesco.org/ark:/48223/pf0000380455 [VC-DATA-MODEL] Verifiable Credentials Data Model v1.1. Manu Sporny; Grant Noble; Dave Longley; Daniel Burnett; Brent Zundel; Kyle Den Hartog. W3C. 3 March 2022. W3C Recommendation. URL: https://www.w3.org/TR/vc-data-model/ [W3C-ML-WS] W3C Workshop Report on Web and Machine Learning. W3C. October 2020. URL: https://www.w3.org/2020/06/machine-learning-workshop/report.html [w3c-vision] Vision for W3C. Chris Wilson. W3C. 3 April 2024. W3C Working Group Note. URL: https://www.w3.org/TR/w3c-vision/ [WAI-AI] Artificial Intelligence (AI) and Accessibility Research Symposium 2023. W3C Web Accessibility Initiative. Jan 2023. URL: https://www.w3.org/WAI/research/ai2023/ [WASM-CORE-2] WebAssembly Core Specification. Andreas Rossberg. W3C. 16 August 2024. W3C Working Draft. URL: https://www.w3.org/TR/wasm-core-2/ [WEBGPU] WebGPU. Kai Ninomiya; Brandon Jones; Jim Blandy. W3C. 13 August 2024. W3C Working Draft. URL: https://www.w3.org/TR/webgpu/ [webmachinelearning-ethics] Ethical Principles for Web Machine Learning. Anssi Kostiainen. W3C. 8 January 2024. W3C Working Group Note. URL: https://www.w3.org/TR/webmachinelearning-ethics/ [WEBNN] Web Neural Network API. Ningxin Hu; Dwayne Robinson. W3C. 5 August 2024. W3C Candidate Recommendation. URL: https://www.w3.org/TR/webnn/ ↑ 2/6/26, 7:29 PM AI & the Web: Understanding and managing the impact of Machine Learning models on the Web 16/16", "quality": {"lang": "en", "gibberish_score": 0.032, "char_len": 2814, "word_len": 314}, "dedupe": {"exact_hash": "517d1fbceef6c613b22ef97d9c30f6de3de256822aafdc318646f28d76b3734b", "simhash64": 2232745626809206610}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d6f9105990f2ce8058fb12518ab16c6cdda9030ef4d1ac346e361eadc9b5686b", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 0, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "Identity & the Web 25 February 2025 More details about this document Copyright © 2025 World Wide Web Consortium. W3C® liability, trademark and permissive document license rules apply. This document proposes an overview of Digital Identities on the Web and an analysis through different use cases of the systemic impact on both the market side and the human side, as well as the role that Web standardization may play in managing that impact. This document is intended to capture the current shared understanding of the W3C Team on the current and expected impact of developments linked to identity on the Web and identifying explorations the World Wide Web Consortium and its community have started or ought to be starting to manage that impact. It does not represent any consensus from the W3C Membership, nor is it a standardization document. The document was authored by Simone Onofri (simone@w3.org), with significant contributions from the W3C Team and people listed in the Acknowledgements section. This document helps structure discussions on what may be needed at the standardization level to make Identity’s systemic impact less harmful or more manageable. It is bound to be incomplete and sometimes wrong — we are gathering input and feedback in GitHub, preferably before October 6, 2024. If it’s not feasible for you to use GitHub, send comments in e-mail to: public-team-reportcomments@w3.org. Please put your comments in the body of the message, not as an attachment. Start your e-mail subject line with: [identity-web-impact]. Depending on the feedback received, possible next steps include more in-depth stakeholder interviews, a dedicated W3C Workshop, or developing a standardization roadmap. Abstract Status of this document 2/6/26, 7:30 PM Identity & the Web 1/43 Table of Contents 1 Executive summary 2 Introduction 2.1 Terminology 2.2 Why identity is important 2.2.1 Human rights 2.2.2 Sustainable development goal 2.2.3 Identity for Development (ID4D) 2.2.4 Opportunities and threats 3 Digital identity management models 3.1 Centralized identity model 3.2 Federated identity model 3.3 Decentralized identity model 3.3.1 Architecture 3.3.1.1 Layer 5: Trust Frameworks and Ecosystems 3.3.1.2 Layer 4: Applications, Wallets, Products 3.3.1.3 Layer 3: Credential Layer 3.3.1.4 Layer 2:", "quality": {"lang": "en", "gibberish_score": 0.0052, "char_len": 2303, "word_len": 350}, "dedupe": {"exact_hash": "1159188a17b1a1329dc2d63dca8ce3d7fad9ce3ad3f47a84079f7c736ba412c2", "simhash64": 1647574539070981398}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:27453ff28df996bf255f49239bcd3f2d06db50fe990b00415b11d591c22d3085", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 1, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "goal 2.2.3 Identity for Development (ID4D) 2.2.4 Opportunities and threats 3 Digital identity management models 3.1 Centralized identity model 3.2 Federated identity model 3.3 Decentralized identity model 3.3.1 Architecture 3.3.1.1 Layer 5: Trust Frameworks and Ecosystems 3.3.1.2 Layer 4: Applications, Wallets, Products 3.3.1.3 Layer 3: Credential Layer 3.3.1.4 Layer 2: Agent Frameworks and Infrastructure 3.3.1.5 Layer 1: Identifiers and Namespaces 3.3.2 Data Flow 3.3.3 Security and Privacy 3.3.4 Standards 4 Uses cases 4.1 Organizations 4.1.1 Organizational Identity 4.1.2 Identity and Access Management (IAM) 4.1.3 Global Workforce 4.2 Things 4.2.1 Supply Chain 4.2.2 Energy Devices (IoT) 4.2.3 Automotive (IoT) 4.3 Human identities and governments 4.3.1 Physical Identity 4.3.2 Textual Credentials 2/6/26, 7:30 PM Identity & the Web 2/43 Digital Identities have been in development for decades. As governments increasingly consider becoming providers and consumers of these technologies, they more than ever have the potential to change the Web and the concept of identity as we know it. Given the scope and scale of this innovation, digital identities are significantly impacting the web and, in particular, privacy, altering the assumptions and the balance that have shaped its ecosystem. This document further develops the concepts described in \"Identity on the Web\" at W3C’s Member Meeting of April 2024 [identity-on-the-web]. It reviews the intersections of Digital Identities through their societal, ethical, and technical impacts and highlights several areas where standardization, guidelines, and interoperability could help manage these changes: Through exploratory thinking, the following understanding emerged: 4.3.3 Photographic Credentials 4.3.4 Machine Readable Credentials 4.3.5 Physical Credentials as Digital Credentials 4.3.6 Pure Digital Credentials 5 Acknowledgment 6 Revision History References Informative References 1. Executive summary § Enabling passwordless credentials for authentication and payments Enabling federated identity in the web platform without third-party cookies Modeling security, privacy, and human rights threats of decentralized credentials Mitigating surveillance, censorship, intrusion, and discrimination and ensuring interoperability by standardizing digital credentials in the web platform Mitigating the threats at technological and governance levels Standards can help, as they have in the past, to drive innovation while mitigating threats and to enable technical progress while having a positive impact on the world.", "quality": {"lang": "en", "gibberish_score": 0.0023, "char_len": 2578, "word_len": 350}, "dedupe": {"exact_hash": "f42939b4846c51d2d20d2b29e305dcf1fd2350e55b3df4bb2cbd7ef515a557f3", "simhash64": 4953214484011107540}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d6d701ef6831f342ea42b6c76e4d1d315fe1eaca1d399a50cb092f6403c321c3", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 2, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "intrusion, and discrimination and ensuring interoperability by standardizing digital credentials in the web platform Mitigating the threats at technological and governance levels Standards can help, as they have in the past, to drive innovation while mitigating threats and to enable technical progress while having a positive impact on the world. 2/6/26, 7:30 PM Identity & the Web 3/43 We seek input from the community on proposals that could help progress on these topics and other topics that this document may contribute to identifying. Digital Identities have been in development for decades, and at this moment in history, they are about to be implemented government-wide. They can change the Web and the concept of Identity as we know it. There are many opportunities but also threats to society and the Web. The concept of identity is very broad and covers psychology, social sciences, mathematics, and logic. There is no agreed-upon definition of all the terminology. Let us start with a set of definitions to have a common ground in this paper. When we think about identity, we often think about our identity as individuals. It is inherent, although we tend to give a different meaning to our identity according to our culture, from the Western \"Cogito ergo sum\" (I think therefore I am) [discourse-on-the-method] to the African \"Ubuntu\" (I am because you are) [what-does-ubuntu-really-mean] or the Eastern \"tat tvam asl\" (that thou art), which express two notions, the man’s real self (ātman), and the Cosmic Self (brahman) [a-dictionary-of-hinduism]. Analyzing the etymology, the term identity comes from the Latin root “idem”, which means “the same” [oxford-etymology-identity], so while it is an intimate concept, we also use it to distinguish ourselves from others. This is well explained in the Cambridge Dictionary in which the identity is “the fact of being, or feeling that you are, a particular type of person, organization, etc.; the qualities that make a person, organization, etc. different from others” [cambridge-dictionary-identity]. Thus, \"from a sociocultural perspective, an individual’s identity is socially constructed, forming from early childhood from their interactions and relationships with others\" [constructing-an-identity]. The technology stack is composite and", "quality": {"lang": "en", "gibberish_score": 0.0052, "char_len": 2301, "word_len": 350}, "dedupe": {"exact_hash": "eeb18ef5aa99c79dcb1273e70e8ff1b2b986384d68ac8e4a9c0ff563ffa5a8ab", "simhash64": 11231216953391945110}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:df501be9d49140c58c4089b6a31adfde154b5fac0768b21893b6a79fa7355f4d", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 3, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "you are, a particular type of person, organization, etc.; the qualities that make a person, organization, etc. different from others” [cambridge-dictionary-identity]. Thus, \"from a sociocultural perspective, an individual’s identity is socially constructed, forming from early childhood from their interactions and relationships with others\" [constructing-an-identity]. The technology stack is composite and broad and needs to be coordinated across standards and Standards Development Organizations (SDOs). People, SDOs, and governments are the key actors who need to collaborate to ensure that digital credentials/identities solve more problems than they create because identity is not only technology but also governance. It is crucial to pay close attention to the impact on security, privacy, and human rights in general. The proposed method of analysis is threat modeling. 2. Introduction § 2.1. Terminology § 2/6/26, 7:30 PM Identity & the Web 4/43 Therefore, our identity is tied to society and the third parties we interact with. These parties often give us an identity and the elements to refer to and prove who we are. Looking more closely at the Information Technology (IT) domain, the ISO/IEC 24760-1:2019 [ISO-IEC-24760-1] defines Identity as “a set of attributes related to an entity”. Where the entity is something \"that has recognizably distinct existence\", and that can be \"logical or physical\" such as \"a person, an organization, a device, a group of such items, a human subscriber to a telecom service, a SIM card, a passport, a network interface card, a software application, a service or a website\". These attributes are “characteristics or properties” such as “an entity type, address information, telephone number, a privilege, a MAC address, a domain name”. To complete the definition of entity and identifiers, it is important to note that they always refer to a domain of applicability, the specific context where they can be used (e.g., an organization, a country, a university). Thus, a particularly important point is clear: there are not only identities of people, individuals, or human beings. We can also have identities for organizations, pets, and Non-Human Identities (NHI). NHI are all those accounts widely used by “devices, services, and servers” in networking, cloud,", "quality": {"lang": "en", "gibberish_score": 0.0078, "char_len": 2306, "word_len": 350}, "dedupe": {"exact_hash": "075145bbf7d419eed0a2aebbfdc3738207f628800b70684a9f4cb803fd6e423d", "simhash64": 845918411707747718}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0a70e1c1109aae5ded0724dee31369a8cbcffa8676cf62d86c2f880cb8b9d182", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 4, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "organization, a country, a university). Thus, a particularly important point is clear: there are not only identities of people, individuals, or human beings. We can also have identities for organizations, pets, and Non-Human Identities (NHI). NHI are all those accounts widely used by “devices, services, and servers” in networking, cloud, and workloads [the-evolving-landscape-of-non-human-identity]. Now, an important logical step. We present credentials to claim our identities, whether in the physical or digital world. Just as we do not have a one-size-fits-all definition of identity, we also do not have a one-size-fits-all definition of credential, as it changes according to context. Starting with the definition from the Cambridge Dictionary, a (digital) credential is “a piece of information that is sent from one computer to another to check that a user is who they claim to be or to allow someone to see information” [cambridge-dictionary-identity]. While high-level, this definition considers two important aspects: on the one hand, the credential is used to prove our claims, such as who we are, and on the other hand, it can be used to gain access to information: The ISO/IEC 24760-1 definition is very close to the last aspect from the dictionary, where a credential is a “representation of an identity for use in authentication” [ISO-IEC-24760-1]. The Identification for Development (ID4D) definition is close to the first aspect: “any document, object, or data structure that vouches for a person’s identity through some method of trust and authentication” [types-of-credentials-and-authenticators]. The NIST SP 800-63-3 definition echoes the first aspect, “an object or data structure that authoritatively binds an identity—via an identifier or identifiers—and (optionally) additional attributes to at least one authenticator” [NIST-SP-800-63-3]. It adds the important concept of binding an identity to its attributes—recalling ISO’s definition of identity—and using identifiers. The W3C Verifiable Credentials Data Model (VCDM) definition states, “a set of one or more claims made by an issuer” [vc-data-model-2.0]. On the one hand, this definition seems similar to NIST’s. However, its framing is in the decentralized versus federated model (which we will 2/6/26, 7:30 PM Identity & the Web 5/43 An additional definition introduced in the NIST", "quality": {"lang": "en", "gibberish_score": 0.0101, "char_len": 2365, "word_len": 350}, "dedupe": {"exact_hash": "b3846ef420a6adc9adf254e9287a45fbd715edf4182ebf5ad610673121909037", "simhash64": 11234596508538799494}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7bddcb408229f265f4a4bef3aad431e9ee87f0ba9273b90d07ea6fd35f2e8892", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 5, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "“a set of one or more claims made by an issuer” [vc-data-model-2.0]. On the one hand, this definition seems similar to NIST’s. However, its framing is in the decentralized versus federated model (which we will 2/6/26, 7:30 PM Identity & the Web 5/43 An additional definition introduced in the NIST blog is Verifiable Digital Credentials as a broader term encompassing the ecosystem [nist-verifiable-digital-credential]. This definition was established to resolve a terminological issue. The term “Verifiable Credentials” emerged within the W3C Verifiable Credentials Working Group, and subsequently the vc media type. Other SDOs also used the vc media type but applied it to a different type of credential, particularly within the IETF OAuth group. The OAuth group later transitioned to using dc for “Digital Credentials”. Hence the need for an all-encompassing term [digital-credentials-that-can-be-verified]. These definitions introduced important concepts that need clarification, such as identifiers, authentication, and trust. Identifiers are pieces of information that uniquely refer to an entity within a specific context. According to the W3C Decentralized Identifiers, there are various types of identifiers: “communication addresses (telephone numbers, email addresses, usernames on social media), ID numbers (for passports, driver’s licenses, tax IDs, health insurance), and product identifiers (serial numbers, barcodes, RFIDs). URIs (Uniform Resource Identifiers) are used for resources on the Web, and each web page you view in a browser has a globally unique URL (Uniform Resource Locator)” [did-core]. Let us then try to understand the authentication process and how it differs from identification, verification, and authorization: analyze shortly), and thus, to ISO’s definition of identity mapping the ISO’s attributes to VCDM claims. NOTE: Therefore, we will refer to the specific definition of credential in the various sections of the document according to the context. NOTE: Although entity, identity, and identifier are related, they are distinct: Identity refers to the essence of who or what an entity is, while an identifier is a specific piece of information used to recognize and refer to that entity uniquely. Identification is recognizing an entity through the information it provides. For example, we enter our first name, surname, and email address in a social", "quality": {"lang": "en", "gibberish_score": 0.0067, "char_len": 2392, "word_len": 350}, "dedupe": {"exact_hash": "40b59ed9c58566f970fcccad74c20cc6241ed0b1db31ce2f5799d1fafc2c420a", "simhash64": 11213204753872160134}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1064a9fe14a9bada7f91c76100644497628c9d1e18cccf20c5c478700cd99adf", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 6, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "essence of who or what an entity is, while an identifier is a specific piece of information used to recognize and refer to that entity uniquely. Identification is recognizing an entity through the information it provides. For example, we enter our first name, surname, and email address in a social network (and there are different levels of proofing of our real identity). Verification allows us to confirm that the presented information is valid through further testing. Verification is a generic process that can take different forms and have different effects. For example, we often receive an email with a confirmation link to verify an email address. We confirm that the email address is under our control by clicking on it. This type of verification 2/6/26, 7:30 PM Identity & the Web 6/43 Let us see how these concepts can be applied to physical credentials. When we present our passport to cross the border, here is an example of the processes that might be carried out: When we use digital credentials on the Internet instead, the issue is more challenging, as illustrated by Peter Steiner’s celebrated cartoon published in the New Yorker in 1993: \"On the Internet, nobody knows you are a dog\" [nobody-knows-you-re-a-dog]. Historically, digital credentials have taken various forms, such as: demonstrates control over the identifier. Verifying identity information online, such as a specific name and surname, is more complex. When verifying identity information, we use the term identity verification. Authentication is a specific, formal verification type that aims to grant access to a resource, service, or information. This process usually involves verifying control of our identifier with something we know (e.g., a password), something we have (e.g., a hardware token), or something we are (e.g., a biometric characteristic). For instance, similar to the email example, we demonstrate control over a username (the identifier of our identity) by entering the corresponding password. Authorization is another key process that follows authentication. It verifies whether our authenticated identity has the necessary permissions to access a particular resource. This step ensures that we are only granted access to resources we can use", "quality": {"lang": "en", "gibberish_score": 0.0022, "char_len": 2248, "word_len": 350}, "dedupe": {"exact_hash": "a6c492ecdd6b02837bec44070150a44be67dfde7a7becb5b7027b8109dca18dd", "simhash64": 11216582453627819142}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0dfa726028fa2527e2f934453e37f3dabe1df482a907f3e4b1e45ffa7dea96ba", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 7, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "over a username (the identifier of our identity) by entering the corresponding password. Authorization is another key process that follows authentication. It verifies whether our authenticated identity has the necessary permissions to access a particular resource. This step ensures that we are only granted access to resources we can use even after confirming our identity. Identification: We present our passport to the border control officer, claiming our identity through our credentials and its identifier (the passport ID). Verification: The border control officer verifies that the passport is genuine, not tampered with, not expired, and issued by a recognized government. Authentication: In this context, the authentication involves verifying that the person presenting the passport is the rightful holder. This might include checking biometric data stored in the passport against the person’s actual biometrics (e.g., fingerprints or facial recognition). Authorization: Finally, authorization is the process where border control determines whether the authenticated individual has permission to enter the country. This decision is based on various factors, including visa validity, passport not expiring in six months or less, and confirmation that the individual is not on any watchlists, unwanted lists, or other checks. The usernames and passwords we use to log in to our favorite social network and communicate with friends. The same usernames and passwords from our favorite social network, but used to authenticate on an e-commerce website and make a purchase. 2/6/26, 7:30 PM Identity & the Web 7/43 Therefore, it’s important to remember that we can have digital credentials that are not identity documents, such as diplomas, which, in this case, are issued by universities. Several projects exist, such as the Digital Credential Consortium (DCC) and Blockcerts, which are committed to building an infrastructure for academic digital credentials. We introduce the last topic with the example of credentials that universities can issue. In addition to degree certificates, universities usually have student ID cards containing information such as first name, last name, and photo. Why can a driver’s license and a student ID card, having similar attributes and being cryptographically verifiable, only allow the driver’s license to open a bank account?", "quality": {"lang": "en", "gibberish_score": 0.0038, "char_len": 2368, "word_len": 350}, "dedupe": {"exact_hash": "e5e5336ea611223602787be8556913d6c90e3749d1c320ef4687dffc4989f6c9", "simhash64": 11239661202693264406}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:044617853474861a15caa55db56c803bb7d7b72ea44679882f9803e80f9f5743", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 8, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "can issue. In addition to degree certificates, universities usually have student ID cards containing information such as first name, last name, and photo. Why can a driver’s license and a student ID card, having similar attributes and being cryptographically verifiable, only allow the driver’s license to open a bank account? There are several aspects, first of all, the context and domain in which the credential lives. The key difference lies in the trust we place in the issuers of these credentials. Trust can be defined as \"the belief that someone is good and honest and will not harm you, or that something is safe and reliable\" [cambridge-dictionary-trust]. Essentially, trust is a choice we make; we choose to trust or not trust someone or something [OSSTMM-3], and often, it is not a binary question. Cryptographic trust, such as verifying the signature of a credential, differs from human trust [self-sovereign-identity]. Cryptographic methods ensure that the credentials haven’t been tampered with and that they have been issued by a trusted issuer. Human trust involves trusting the entity that issued the credential or, in the case of an issuer chain, trusting the root, and that the issuer provided the credential to the legitimate user. This is why we also need governance frameworks or trust frameworks. These frameworks include business, legal, and technical rules that help establish and maintain trust in credential issuers. This includes establishing the Levels of (identity) Assurance (LOA). Follow as an example the Identity Assurance Level (IAL) from NIST-SP-800-63-3 [NIST-SP-800-63-3]: A digital driver’s license in our digital wallet. NOTE: The last form of credential, as defined by W3C, has a wider range of use cases than just authentication. One important clarification: it may make sense to use a driver’s license to authenticate only on the issuer’s systems (e.g., it is good to authenticate ourselves on government websites but not on our personal email provider). Furthermore, additional information (claims) on the driver’s license, such as date of birth and, in some cases, home address verified by a trusted entity such as a government, enables interesting use cases. IAL1: No requirement to prove", "quality": {"lang": "en", "gibberish_score": 0.0036, "char_len": 2235, "word_len": 350}, "dedupe": {"exact_hash": "eaefed2d5b06ab97622c32763943a6f55db1bb7935eaac5f4124d250b9b6f787", "simhash64": 11162536990355592342}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8d634f55bfa761f5bdb55eaafc9113143c66797a36ebb595935e6cc9f9619c99", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 9, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "to authenticate ourselves on government websites but not on our personal email provider). Furthermore, additional information (claims) on the driver’s license, such as date of birth and, in some cases, home address verified by a trusted entity such as a government, enables interesting use cases. IAL1: No requirement to prove a specific real life identity, e.g., identity can be self-asserted. IAL2: Remote or in-person identity proofing with supporting evidence is required. 2/6/26, 7:30 PM Identity & the Web 8/43 Having concluded this roundup of terminology, before we delve into the various digital identity management models that have come and gone over time and that we have used in the previous examples, let us try to understand why identities are so important. Human identities are a very special case, particularly those issued by governments. We know that they are not the only type and that the others are also important and have interesting business implications, but human ones have distinctive characteristics. Let us see why. Identity is a fundamental human right that underpins personal dignity and autonomy. Article 6 of the Universal Declaration of Human Rights states, \"Everyone has the right to recognition everywhere as a person before the law\" [UDHR]. This principle is reinforced by Article 16 of the International Covenant on Civil and Political Rights, which similarly reads: \"Everyone shall have the right to recognition everywhere as a person before the law\" [ICCPR]. Although the term \"identity\" is not explicitly used here, its concept is inherent in recognizing the identity as a person. Despite being a right, much work still needs to be done to provide identities for all the population. However, target 16.9 of the 2030 United Nations Sustainable Development Goals (SDGs) aims to achieve \"legal identity for all, including birth registration\" [SDGS-16]. ​ Achieving legal identity for all is a challenging goal on several fronts. In response, the World Bank has launched the ID4D initiative, aiming to \"secure a unique legal identity and enable digital IDbased services for all by 2030\" [ID4D-initiative]. IAL3: Physical presence is required for identity proofing, with proper verification of evidence. 2.2. Why", "quality": {"lang": "en", "gibberish_score": 0.0027, "char_len": 2246, "word_len": 350}, "dedupe": {"exact_hash": "74b1b00e634175218caf51ceb0dcbb92f2ddfc207da1da968001e7999d631b04", "simhash64": 11168184082075848854}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:990641cadad16d91f881656decb7a09e50e27a313df073a067d71078f0bc8d0b", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 10, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "all is a challenging goal on several fronts. In response, the World Bank has launched the ID4D initiative, aiming to \"secure a unique legal identity and enable digital IDbased services for all by 2030\" [ID4D-initiative]. IAL3: Physical presence is required for identity proofing, with proper verification of evidence. 2.2. Why identity is important § 2.2.1. Human rights § 2.2.2. Sustainable development goal § 2.2.3. Identity for Development (ID4D) § 2/6/26, 7:30 PM Identity & the Web 9/43 Digital identities and credentials are powerful business enablers and offer significant opportunities for individuals, governments, and organizations. They can guarantee other rights, such as the right to accessibility promoted by the Marrakesh Treaty [marrakesh-treaty]. They can also \"empower refugees, stateless individuals, and forcibly displaced persons\" [UNHCR-digital-identity]. These technologies can also be used on a humanitarian level. Referring to the NHIs, the International Committee of Red Cross (ICRC) investigated Digital Emblems [ADEM] to identify ICT assets protected under international law [digitalizing-report]. Therefore, it is necessary to analyze the various threats to mitigate them at their root in designing and implementing these technologies and related standards. As an example, below is an initial analysis of threats to human rights (Harms) concerning government-issued digital identities using Microsoft’s Harms Modeling [harms-modeling]: 2.2.4. Opportunities and threats § NOTE: However, like all innovations, these technologies can have downsides. To paraphrase Paul Watzlawick, the innovation of these technologies must not become “ultra-solutions” where “operation successful, patient dead” [ultra-solutions]. So, the challenge is to enable this technological innovation by being aware of the threats to privacy, security, and human rights. Opportunity Loss (Discrimination): This complex issue spans multiple areas. Digital divide: if digital identities are required for access to public services and no alternatives are present, and if they depend on certain hardware, software, or stable connectivity, it can lead to discrimination for people who do not have availability of these resources. In addition to discrimination within the same country, there is further discrimination if there is no “cross-border” interoperability between the technologies and implementations used by different governments. Economic loss (Discrimination): The availability of digital identities and related credentials, which", "quality": {"lang": "en", "gibberish_score": 0.0063, "char_len": 2536, "word_len": 350}, "dedupe": {"exact_hash": "b8f46afe7a70e29de8d593c0b238ba3dfb352b49cc1afe8045e97025e52ac9a6", "simhash64": 11240241746980488390}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:9279555296714c25e4e6fdce12008828a08612cd0641278e6cf3bb9f1f0edc6d", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 11, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "discrimination for people who do not have availability of these resources. In addition to discrimination within the same country, there is further discrimination if there is no “cross-border” interoperability between the technologies and implementations used by different governments. Economic loss (Discrimination): The availability of digital identities and related credentials, which can contain a lot of information regarding wealth status, can be used to discriminate against access to credit. This can also be generalized - as was identified during a W3C breakout session - and concerns the Javons paradox. The more information available, the more likely it is that collection, particularly in greedy data-driven contexts, is abused. Dignity loss (Dehumanization): For example, if the vocabulary does not correctly describe people’s characteristics, this can reduce or obscure people’s humanity and characteristics. Privacy Loss (Surveillance): if this technology is not designed and implemented properly, it can lead to surveillance by state and non-state actors such as government and private technology 2/6/26, 7:30 PM Identity & the Web 10/43 With these assumptions, before proceeding, it is important to understand how digital identities are managed and how they have evolved over the years. Let us start with the example of a person’s identity given earlier and break it down. We had: These examples represent the evolutionary stages of Internet Identity described by Christopher Allen at the Internet Identity Workshop (IIW). From these developmental stages, the community agrees that there are currently three models of identity relationships [three-models-of-digital-identity-relationships]. Let us analyze them. In the centralized identity model, also known as siloed or traditional, a single provider offers both the identity (and its credentials, typically a username and password) and the service. This older model was used in the early days of the Internet and the Web and is still used today. The centralized identity model is the typical scenario when the user logs in to a social network to use it, and the credentials here are used to authenticate. Figure 1 Centralized Identity Management providers. For example, centralized or federated models are more prone to these threats, while decentralized models are less so, but it depends", "quality": {"lang": "en", "gibberish_score": 0.0038, "char_len": 2357, "word_len": 350}, "dedupe": {"exact_hash": "457f7873c27fdd66a0ee4adf48039ffcf0ce741f78080f1b1b00fff075153489", "simhash64": 11231234545546461334}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:3e66a6269806ae3cdf17120d76277e645a237312d7185abe2b0781fd5cc7e71f", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 12, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "the typical scenario when the user logs in to a social network to use it, and the credentials here are used to authenticate. Figure 1 Centralized Identity Management providers. For example, centralized or federated models are more prone to these threats, while decentralized models are less so, but it depends on how they are implemented. Therefore, it is necessary to provide privacy-preserving technologies and implement them properly. 3. Digital identity management models § Credentials of a social network that are used on the same site. Credentials of a social network that are used on a different site. Driver’s license within a digital wallet application. 3.1. Centralized identity model § 2/6/26, 7:30 PM Identity & the Web 11/43 Here is the simplified Data Flow: Perspectives: Enabling passwordless credentials for authentication and payments To mitigate security threats, in particular the use of multiple passwords and phishing, FIDO Alliance created Passkeys, \"a replacement for passwords that provide faster, easier, and more secure sign-ins to websites and apps across a user’s devices\" [passkeys-101]. The W3C Web Authentication Working Group brought strong authentication to the Web Platform through Web Authentication Level 2 [webauthn-2] and is developing Level 3 [webauthn-3]. Synchronized Web Authentication credentials —passkeys— are well-suited for login authentication but less well-suited for some regulated high-assurance use cases, notably payments. To fulfill additional requirements of payments ecosystems, the W3C Web Payments Working Group is developing Secure Payment Confirmation [secure-payment-confirmation] to support multi-factor authentication and requirements for cryptographic evidence of user consent to the terms of a transaction. Authentication: The user authenticates themselves with the centralized system using their credentials. Access Granting: This system grants access to the resource. Security: there are different issues. For the user: password re-use in case of compromised password, so the user should use different passwords for different providers; there are also Phishing and Man-in-The-Middle attacks. From the provider’s point of view, as the passwords are stored on their systems, they need to implement proper security measures to protect them at rest and during transport. Privacy: the centralized system can completely track the user. Standards: Standards intervene at different levels. In", "quality": {"lang": "en", "gibberish_score": 0.0045, "char_len": 2451, "word_len": 350}, "dedupe": {"exact_hash": "5da7d270da70c305e71655cbe0158455a3845ceea747a40b45df13ebc5fc681b", "simhash64": 11239938279623480726}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ccfcbb58bca6db343b80391af1f0cf3dea94a8b0ae017512efe69aed3d8c6332", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 13, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "also Phishing and Man-in-The-Middle attacks. From the provider’s point of view, as the passwords are stored on their systems, they need to implement proper security measures to protect them at rest and during transport. Privacy: the centralized system can completely track the user. Standards: Standards intervene at different levels. In how credentials are exchanged and sent: historically, Basic Authentication [RFC1945], Digest Authentication [RFC2069] (and related updates), and via HTML forms with input type=password and Cookies for maintaining the session information on the Client. Other standards for increasing authentication factors include HOTP [RFC4226] and TOTP [RFC6238]. Also, with SSL/TLS [RFC2246] (and related updates) for credential transport and general traffic protection. . Other standards protecting the credentials at rest such as the (now obsolete) MD5 [RFC1321] and other hashing algorithms by NIST. 2/6/26, 7:30 PM Identity & the Web 12/43 In the federated identity model, the function of making available identity information, also known as a third-party Identity Provider (IdP), is separated from the one which provides a service to the user - the Service Provider (SP) or Relying Party (RP) [ISO-IEC-24760-1]. The federated identity model is the typical scenario when a user logs into a third-party site using a social network’s \"Sign in with...\" feature or through Single Sign-On (SSO) in enterprise environments. This model allows users to utilize a single Identity Provider (IdP) to authenticate and access multiple Service Providers (SPs) or Relying Parties (RPs) without needing to create separate accounts for each one. Figure 2 Federated Identity Management Here is the simplified Data Flow: Perspectives: 3.2. Federated identity model § Authentication: The user sends their credentials to the IdP to authenticate. Obtaining Identity Assertions: The IdP then creates an identity assertion, a verifiable confirmation of the user’s identity. Sending Identity Assertions: The user sends their identity assertion to the SP or RP. Trust and Access: The SP or the RP, trusting the IdP, accepts the user’s Identity Assertion and grants access. 2/6/26, 7:30 PM Identity & the Web 13/43 Enabling Federated identity in the Web platform without third-party cookies The Federated Identity Community Group was created to resolve these", "quality": {"lang": "en", "gibberish_score": 0.0064, "char_len": 2359, "word_len": 350}, "dedupe": {"exact_hash": "aa444ab82ddb67f479cbeeac1d5bd91d36b23b6f133a5af085964089a22d5b6b", "simhash64": 11221086053253615878}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:82a34d7b109aff523fc192d613b36d62478bbe73e4d24393f54ff03fe0e56383", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 15, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "which underpins OAuth for authorization and different token formats. 3.3. Decentralized identity model § NOTE: We will examine the decentralized identity model more closely, as it is the source of new challenges. 3.3.1. Architecture § 2/6/26, 7:30 PM Identity & the Web 14/43 Figure 3 Decentralized Identity Model, adapted from Verifiable Credentials Data Model (VCDM) [vc-data-model-2.0] To best understand the decentralized model and its end-to-end operation, we can refer to the model by the Decentralized Identity Foundation (DIF) [did-faq] build on the Trust Over IP architecture [introduction-toip] [evolution-toip]. This model comprises both technology and governance aspects, sliced into different layers. We will then provide an overview of the various layers, focusing on specific elements related to credentials and identifiers, considered the two pillars of decentralized identities. 2/6/26, 7:30 PM Identity & the Web 15/43 Figure 4 Decentralized Identity Architecture, inspired and adapted by ToIP [introduction-toip] Let us start by looking at the layers. At this level, we find compliance and regulation, governance, and trust frameworks, which tell us who we can trust, such as a particular issuer that follows a specific Level of Assurance. These trust elements depend on the context — or domain — of reference. We can have different contexts, such as state, government, and university contexts. Each context is governed by one or more governance or trust frameworks. For example, for the domain of credentials issued by governments in Europe, we have the EU Digital Wallet, and we can have others related to other states. As well as other domains such as individual organizations, enterprises, or universities. So, a critical issue is interoperability across states. 3.3.1.1. Layer 5: Trust Frameworks and Ecosystems § 2/6/26, 7:30 PM Identity & the Web 16/43 At this level, we find the various applications available to end users: At this level, the various actors exchange credentials. Let us see what happens using the specific definitions of the W3C Verifiable Credentials Data Model (VCDM) [vc-data-model-2.0]. The actors are: NOTE: There are differences between the levels of DIF and the ToIP Technology Stack: Governance is divided into parallel halves, with specific frameworks for each", "quality": {"lang": "en", "gibberish_score": 0.0073, "char_len": 2313, "word_len": 350}, "dedupe": {"exact_hash": "12a4daa5682208ae9edf62cdbdcbbbcfcc9676f42812cdcb8d196751b0c90eeb", "simhash64": 11135535252520142086}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a1c4328c7e6a9d8e238e343f5340eb451e7c66b5bdf2ba6df0c48df313e8b165", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 16, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "various actors exchange credentials. Let us see what happens using the specific definitions of the W3C Verifiable Credentials Data Model (VCDM) [vc-data-model-2.0]. The actors are: NOTE: There are differences between the levels of DIF and the ToIP Technology Stack: Governance is divided into parallel halves, with specific frameworks for each level. 3.3.1.2. Layer 4: Applications, Wallets, Products § Applications: These can be Native or Web-based, and it is possible to interact with them differently. Digital Wallets: These are where the Holder stores their credentials. Just as a physical wallet holds more than just IDs, the digital wallet can store various credentials and information. We can classify the wallets depending on several factors: Simplifying, the Wallet structure comprises secure storage and an agent managing interactions. Technology: Wallets can be native applications (e.g., Mobile) or web applications (e.g., Cloud). Cryptographic Key location: Custodial (e.g., the key is stored in the Cloud) or NonCustodial (e.g., the key is stored in a device in possession of the end-user) NOTE: There are differences between the levels of DIF and with ToIP Technology Stack: Wallets are at Layer 2. Layer 4 is for practical applications, which we cover in Use cases. 3.3.1.3. Layer 3: Credential Layer § The Issuer creates and issues credentials to the Holder and writes the necessary information within the Verifiable Data Registry. This can be a trusted third-party entity like governments or universities. In some cases, credentials can be self-issued by the user, e.g., to represent informal skills or competencies. This flexibility allows for a broader range of credentials and applications. The Holder (the user), at the heart of this architecture, receives the credentials from the Issuer, stores them in a Digital Wallet, and presents them to the Verifier. 2/6/26, 7:30 PM Identity & the Web 17/43 The actors exchange: The Verifier receives the presented credentials by the Holder and verifies them. This actor is akin to an SP or RP in federated models. This process does not necessarily involve informing the Issuer. This decoupling is a key aspect of the decentralized identity model, enhancing privacy and control for the user. NOTE:", "quality": {"lang": "en", "gibberish_score": 0.0027, "char_len": 2260, "word_len": 350}, "dedupe": {"exact_hash": "2766f825b6c466170bc72df6a4607d3a772e91e564a2d00a6bbb8a37aa468178", "simhash64": 11140021259927912838}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7f3f33177b6deee09b4fe5b214da908a6ee8c379c24d4516d153021764a8dfe4", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 17, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "receives the presented credentials by the Holder and verifies them. This actor is akin to an SP or RP in federated models. This process does not necessarily involve informing the Issuer. This decoupling is a key aspect of the decentralized identity model, enhancing privacy and control for the user. NOTE: In this model, the definition of a credential shifts to a set of claims (attributes) linked to identifiers controlled by the user. While credentials represent identities, not all claims within a credential are used for identification. They can describe various characteristics, extending the application of credentials beyond mere identification. Verifiable Credential (VC): When the Issuer sends them to the Holder, who then stores it in their Wallet. The word Verifiable refers to the characteristic of a credential (or presentation) as being able to be verified (though cryptographic mechanisms) by a Verifier. The addition of technologies, such as digital signatures, makes verifiable credentials more tamper-evident and more trustworthy than their physical counterparts. Metadata: of the Credentials, to describe properties such as the Issuer, the expiry date and time, a representative image, the Issuer public key to use for verification purposes, the revocation mechanism, and so on. Claim(s): one or more assertions where a characteristic of a subject is described (e.g., the subject is a citizen of a certain state, was born in a certain place on a certain day, month, and year, and can drive cars of this type). Proof(s): cryptographic proof of the integrity and the authenticity of the credential, typically via a digital signature. The proof is generated by the Issuer. Verifiable Presentation (VP): When the Holder sends a credential to the Verifier, which then verifies it. VC are used to present claims to a Verifier by proving control over credentials that certify them. The basic case is to present the credential as is. However, in many scenarios, the holder may wish to present only a subset of the credential claims to the verifier - this mechanism is called Selective Disclosure (SD) - or a combination of information from different credentials. It may contain: Metadata: of the Presentation, including", "quality": {"lang": "en", "gibberish_score": 0.0, "char_len": 2230, "word_len": 350}, "dedupe": {"exact_hash": "da16c2c26e7bbdb97e0d1b072c607eff6aebbd80e744b29e7f6d567a4ea317a1", "simhash64": 11230374727451508742}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:da9908ff6099dc1b2056cb6f16a74b9db49f345b795d5aa85be4fd4407181448", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 18, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "present the credential as is. However, in many scenarios, the holder may wish to present only a subset of the credential claims to the verifier - this mechanism is called Selective Disclosure (SD) - or a combination of information from different credentials. It may contain: Metadata: of the Presentation, including the Issuer public key to use for verification purposes. Credential(s): information derived or combined from one or more credentials. If Selective Disclosure is adopted, no credentials are shown, but only a subset of the credential claims. 2/6/26, 7:30 PM Identity & the Web 18/43 Obviously, the Issuer, the Holder, and the Verifier need a credential exchange protocol for VCs and VPs. This is not described in VCDM but is defined by other standards, such as VC API and OpenID4VC. At this level, the software responsible for the interaction between the various actors communicates by processing the identifiers in the credentials. Agents retrieve content, resolve it, render it, and facilitate interactions between the various actors and the lower levels. At this level, we find identifiers and the Verifiable Data Registry (VDR), where referenced information is written, updated, read, and deleted. As identifiers, we mainly refer to the Decentralized Identifiers (DIDs), a new Uniform Resource Identifier (URI) [RFC3986] type that allows entities and resources to be identified. DIDs, along with Verifiable Credentials, are considered one of the two pillars of decentralized models — DIDs provide for different methods of referencing resources [did-core], called methods. These methods can rely on various technologies, including blockchains such as Bitcoin or Ethereum, the web, InterPlanetary File System (IPFS), and Domain Name System (DNS) [did-spec-registries], and it is possible to write your method. Some distinctive properties of DIDs are [did-intro]: Proof(s): cryptographic proof of the integrity and authenticity of the presentation. The proof is generated by the Holder. It consists in a proof of knowledge of a credential certifying the (dislosed) credential claims. If Selective Disclosure is adopted, the proof may be obtained through the use of a cryptographic zero-knowledge proof. NOTE: Refer to Ivan Herman’s W3C Verifiable Credentials Overview for a comprehensive overview of Verifiable Credentials.", "quality": {"lang": "en", "gibberish_score": 0.0026, "char_len": 2337, "word_len": 350}, "dedupe": {"exact_hash": "3f1ff3aed1107804876a06563181ad3c3d3a749969f8a8088fa0c26f24ae40b0", "simhash64": 11231500627391907222}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:b38324b21475c1be5ec88c8ddefdf04eb808b07be810d75613c94a7fd6dc2111", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 19, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "Holder. It consists in a proof of knowledge of a credential certifying the (dislosed) credential claims. If Selective Disclosure is adopted, the proof may be obtained through the use of a cryptographic zero-knowledge proof. NOTE: Refer to Ivan Herman’s W3C Verifiable Credentials Overview for a comprehensive overview of Verifiable Credentials. 3.3.1.4. Layer 2: Agent Frameworks and Infrastructure § 3.3.1.5. Layer 1: Identifiers and Namespaces § Decentralized: do not depend on centralized registries, identity providers, authorities, etc. Persistent: once created, it is permanently assigned to the subject. Resolvable: it is possible to find out a basic set of information on the subject. 2/6/26, 7:30 PM Identity & the Web 19/43 The Verifiable Data Registry (VDR) holds the data needed to verify credentials and their status. This can be government databases, distributed ledgers, or other services. By maintaining this information, the VDR, depending on its form, enables verification without direct communication between the Issuer and the Verifier. Having concluded this overview of the whole ecosystem of decentralized identities, we can return to focus on the specific level of credentials. Here is the data flow of credentials from when they are created to when they are revoked. Cryptographically verifiable: there is a mechanism to prove identity and ownership cryptographically. NOTE: The ToIP Technology Stack also includes Identifiers at other layers. 3.3.2. Data Flow § Credential Issuing (CI): 1. The Issuer requests a certain authentication mechanism from the Holder. 2. After authentication, the Holder asks the Issuer for the credential or the Issuer submits it. 3. If both parties agree, the Issuer sends the credential to the Holder in a specific format. 4. The Holder enters their credential into the Wallet. Credential-Presentation (CP) 1. The Holder requests access to a specific resource or service from the Verifier. 2. The Verifier then presents a request for proof to the Holder. This can either be done actively (e.g., the Verifier presents a QR code that the Holder has to scan) or passively (e.g., they accessed a web page and were asked to access a credential). 3. Through the Wallet, the holder’s user agent determines if there", "quality": {"lang": "en", "gibberish_score": 0.004, "char_len": 2262, "word_len": 350}, "dedupe": {"exact_hash": "96bc9711b358a0105de4d3b5ddc22652ce692e8c496be1385f3bb60c15a4517e", "simhash64": 11213202555117864198}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6ee81c2b0643eaf0da54a906c7b59c6307e96d1053123c53f9fcf4e51c009d3a", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 20, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "for proof to the Holder. This can either be done actively (e.g., the Verifier presents a QR code that the Holder has to scan) or passively (e.g., they accessed a web page and were asked to access a credential). 3. Through the Wallet, the holder’s user agent determines if there are credentials to generate the required Proof. 4. The Holder may use the proof explicitly if they possess it. 5. The user agent of the Holder then prepares the Presentation - which can contain the full credential or part of it- and sends it to the Verifier. Credential-Verification (CV) 2/6/26, 7:30 PM Identity & the Web 20/43 It is interesting to reflect on how this model differs from a security and privacy perspective both from previously described models and from the use of physical identity documents, as credentials can enable this use case: 1. The user agent of the Verifier verifies the Presentation (e.g., if the Presentation and the contained Credentials are signed correctly, issued by an Issuer they trust, compliant with their policy, the Holder is entitled to hold it, and that it has not been revoked or expired). The revocation check can be done using the methods defined by the specific credential. 2. If the verification is successful, the Verifier gives the Holder the access. Credential-Revocation (CR) 1. The Issuer can revoke a credential in various ways. 3.3.3. Security and Privacy § Decentralized vs. Federated Model: Let us analyze one of the privacy issues of the federated model: whoever provides the identity can track the user. This is one of the threats this model wants to mitigate since the identity is in the user’s Wallet, and they use it as they wish. Does this guarantee its untraceability? One can presume that the answer is \"it depends\". It depends on how the architecture is defined and implemented and the technologies used. For example, when we present our credentials to log in, the verifier contacts the issuer directly, asking if the credentials are still valid, and we continue to be traceable. Decentralized vs Physical Document: If we instead think about", "quality": {"lang": "en", "gibberish_score": 0.0038, "char_len": 2084, "word_len": 350}, "dedupe": {"exact_hash": "bdc4333bcb595b83673844a7057899a431aa46a45d6d09e655fe9d8a19a431ea", "simhash64": 11212076654944163078}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7300b4eadea006090dbbc69c598c3abeaa9824ba376f7752a5c1236dcecd99e3", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 22, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "and security features [verifiable-credentials-flavors-explained]. We can expand the discussion not only to formats but also to all other components of the architecture, which must be aligned to ensure security and privacy. Verifiable: Identities, credentials, and various claims must be verifiable, which is possible through appropriate cryptographic proofs. This is part of the security aspects, which include cryptography. Minimal: This is a privacy aspect. When we send information to the verifier, the information should be minimized as much as possible. For example, if we have to show that we are of age, it is okay to submit all the credentials or even the specific claim of the date of birth, but simply that I am of age. Unlinkable: This is another privacy aspect of the minimal issue anyway. Suppose any party involved in the interaction, such as the Issuer, Verifier, or even a third party, can link and correlate the information we sent. In that case, this can be done through various techniques, and privacy is compromised. 2/6/26, 7:30 PM Identity & the Web 22/43 Modeling security, privacy, and human rights threats of decentralized credentials Given the levels of complexity, a comprehensive analysis of threats to privacy, security, and human rights is necessary [human-rights-and-technical-standards]. This is especially important for high-assurance credentials, such as those issued by governments, as highlighted by organizations such as the Electronic Frontier Foundation (EFF) [eff-digital-identification] and Access Now [access-now-whyid]. W3C recognized the need for rights-respecting digital credentials and started a joint Threat Model for Decentralized Identities: Threat Modeling is \"a family of structured, repeatable processes that allows to make rational decisions to secure applications, software, and systems\" [threat-modeling-designing-for-security]. This Threat Model is using different frameworks and toolkits to cover the different threat types, such as: Other approaches include the Self-Review Questionnaire: Security and Privacy and the OSSTMM [OSSTMM-3]. The Threat Model also includes a list of various mitigation techniques, particularly those based on cryptography techniques such as Zero Knowledge Proof (ZKP) and additional methods for Technical Architecture Group (TAG) Privacy Interest Group (PING) Federated Identity Working Group (FedID WG) Verifiable Credentials Working Group (VCWG) Credentials Community Group", "quality": {"lang": "en", "gibberish_score": 0.0016, "char_len": 2462, "word_len": 350}, "dedupe": {"exact_hash": "4c62a97adaa482ca9e9c81b602acd2302f3a13c16f291f471cd0eb652fc492df", "simhash64": 11238814509986844870}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:72563e419d4e9ffe17902a0c1645b44b99f21486992bdc578181df6f096f1d8e", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 24, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "the DID methods, IndieAuth, WebID, X.509, URLs, or URIs. Encoding Schemas: JSON, JSON-LD, CBOR, CBOR-LD. Securing Mechanisms: Each mechanism may or may not support different privacy features or be quantum-resistant: Enveloped Formats (Credential Formats): The proof wraps around the serialization of the credential. JSONs are enveloped using JSON Object Signing and Encryption (JOSE), and we can find JWT, JWS, and JWK here. JOSE is cryptographically agile (as it can fit different cryptographic primitives) and can also have Selective Disclosure (SD) with SDJWT (which uses HMAC). New securing mechanisms are coming up, like SD-BLS (which uses BLS) and ongoing efforts to fit BBS. CBORs are enveloped using CBOR Object Signing and Encryption (COSE). Other formats include ISO mDocs, and SPICE. The mechanism to use VCDM with JOSE/COSE is described in Securing Verifiable Credentials using JOSE and COSE. 2/6/26, 7:30 PM Identity & the Web 24/43 Embedded Formats (Signature Algorithms): The proof is included in the serialization alongside the credentials (e.g., BBS, ECDSA, EdDSA). The mechanism is described in Verifiable Credential Data Integrity 1.0. New securing mechanisms are coming up, like [SDBLS](https://arxiv.org/abs/2406.19035) (which uses BLS), and ongoing efforts to use BBS. Status Information (Revocation Algorithms): Issuers can implement several ways to keep the credential’s status up to date, such as a Revocation List, a Status List (e.g., W3C Bitstring Status List v1.0, Status Assertions, Token Status List), and Cryptographic Accumulators, etc.. Communication Protocols: for the different phases of Issuance and Presentation such as OID4VCI, OID4VP, SIOPv2, ISO REST’s API, and Verifiable Credentials API. NOTE: This list is representative. For more detailed information, please refer to the Comparison Matrix. 2/6/26, 7:30 PM Identity & the Web 25/43 Mitigating surveillance, censorship, intrusion, and discrimination and ensuring interoperability by standardizing digital credentials in the web platform It’s not easy to manage digital identities. In particular, if they are related to humans, it’s important to give people control over the identifying information about themselves they are presenting in different contexts on the web, and be transparent about it [design-principles]. Given that a user agent should help its user present the identity they want in each context", "quality": {"lang": "en", "gibberish_score": 0.0071, "char_len": 2403, "word_len": 350}, "dedupe": {"exact_hash": "6fe79b2381927b2ef67a61177d081b668e5e70cdc2c63e49b1c3275dabbb9722", "simhash64": 15275451617794426070}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:81cbc4aa45d3297f7a27bca7349c3946a7295ecbda122b6afce692fdab2ecc3d", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 25, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "if they are related to humans, it’s important to give people control over the identifying information about themselves they are presenting in different contexts on the web, and be transparent about it [design-principles]. Given that a user agent should help its user present the identity they want in each context they are in and should prevent or support recognition as appropriate [privacy-principles], the Web Platform Incubator Community Group (WICG), incubated the Digital Credentials API [DIGITAL-IDENTITIES], a browser API to mediate the use of Digital Credentials between websites and wallets, to mitigate different threats to the users and promoting interoperability, and it is already working on the Profile for OpenID4VP. The interoperability is important: if a service supports only a specific format, it excludes and discriminates against users who use a different format due to their government’s decisions rather than their own. Consequently, this limitation affects users and restricts the service’s business potential. This API will follow the users first, developers second, and browser engines third principle [design-principles], and meeting the needs of regulations (e.g. eIDAS or Children’s Online Privacy Protection Rule (COPPA)) [digital-identity-explainer]. For these reasons, the Federated Identity Working Group—which has an API to integrate federated identities and thus has a close implementation link to decentralized ones—is in a rechartering process to adopt the Digital Credentials API, guided by the Threat Model to make the API secure, privacy-preserving, and rights-respecting. Moreover, the group’s new scope proposal has a broader perspective. It can also use the other data flows of the decentralized identity model to enable all possible use cases through the web platform. The world of Digital Identities is quite broad and has different uses in different industries, where it can enhance the user experience and act as a business enabler. To imagine these use cases, we can play a game: see what is inside our physical or digital wallets. 4. Uses cases § 2/6/26, 7:30 PM Identity & the Web 26/43 For example, the driver’s license (and the international one), the passport (and the passport also has visas for entry to other countries, or if you have minor", "quality": {"lang": "en", "gibberish_score": 0.0057, "char_len": 2297, "word_len": 350}, "dedupe": {"exact_hash": "5268a3245087d2fa9fdf25958381d6a366e872d3001512fba09097bf06c8b18a", "simhash64": 11231214754335150214}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:08e26d37ea4fbeb23aeabd738390e69bdb4245dd27a24a42b90fb89908e530ea", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 26, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "a game: see what is inside our physical or digital wallets. 4. Uses cases § 2/6/26, 7:30 PM Identity & the Web 26/43 For example, the driver’s license (and the international one), the passport (and the passport also has visas for entry to other countries, or if you have minor children, they can be in your passport), payment cards, cash, association cards, tickets (e.g., events, concerts, boarding passes), loyalty cards (from hotels, airlines, the grocery store), the university card, the badge to get into the office, medical insurance card and health card, emergency contacts, some receipts, public transportation card, my business card and the business cards of some people I met, the card to get into the gym and the library. If we extend this concept to include those documents that are often too large to be put inside a physical wallet if not unfolded but which we use during the day, we also have employment contracts, house contracts, utility bills, ​ ​ the papers of our pet (which, if it travels, has a chip and a passport), marriage certificate (for those who are married), a power of attorney to sign the documents of a company, the tax return, bank statements, amateur radio license or other licenses, medical prescriptions, exam results (both medical and college), degree, professional qualifications (e.g., medical doctor, lawyer, psychologist), warranty certificates of the items I bought and much more. Let us go ahead and look over the use cases for those organization-related identities. We can look at organizations from different aspects. On the one hand, they can benefit from their government-issued digital identity; on the other hand, they can issue identities themselves to better manage their identification and access systems, both for people and for identities of specific services, software, or processes. To top it off, they can leverage people’s identities for greater assurance, particularly when distributed worldwide. Although this is only a partial list, it already allows us to make several observations: The first is that not every credential is suited to every use case. I cannot generally use a college diploma to cross the", "quality": {"lang": "en", "gibberish_score": 0.0042, "char_len": 2167, "word_len": 350}, "dedupe": {"exact_hash": "474f99cc4a3fee99acd1f6b849bed2511a507868137a758e70c8f35cb4037d5b", "simhash64": 11239667799730001302}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7751d5d1babf3841993a2ec39a5bbcf1cb7612a44773bff517b00e1b1298af05", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 27, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "they can leverage people’s identities for greater assurance, particularly when distributed worldwide. Although this is only a partial list, it already allows us to make several observations: The first is that not every credential is suited to every use case. I cannot generally use a college diploma to cross the border to another country. The second is that the value of a credential increases with its interoperability. The fact that passports are recognized across many countries makes them extremely powerful, including for purposes other than establishing national identity (e.g., for proving age). The third is that credential reuse is often tied to the strength of its subproperties. For example, a utility bill may be used to prove our physical address in a KYC context (because it is tied to the mail system) but may not be sufficient in a KYC context to prove our name or surname. We are not the subjects of some credentials, as in the case of pet travel documents. 4.1. Organizations § 2/6/26, 7:30 PM Identity & the Web 27/43 Organizations can also have a digital identity and related identifiers such as the registration number with the government where it was opened and possibly the VAT number, if not the legal entity identifier. Although the organization has an identity of its own, it operates through individuals who, in the bylaws, have various authorizations, delegations, and signing powers. Therefore, when you do any transaction, such as opening a bank account or a business transaction, you need the organization’s and the personal documentation of the various individuals involved. The use of digital identity in a wallet, with delegation managed through Verifiable Credentials, certainly streamlines the various transactions both with governments and suppliers and with customers, particularly for those aspects of global transactions where the trust relationship goes through a digital transaction and the Association of Certified Fraud Examiners (ACFE) estimates that organizations lose 5% of revenue to fraud each year [acfe-occupational-fraud-2024]. The IAM market is thriving, with an estimated growth of 43 billion USD in 2029 [statista-identity-and-access-management]. Such systems enable an employee’s identification, authentication, and authorization on the", "quality": {"lang": "en", "gibberish_score": 0.0039, "char_len": 2293, "word_len": 350}, "dedupe": {"exact_hash": "64cde3c33e7762574ba4de34d32eff5fb0421119e5ccd249d310a75f2a968e2c", "simhash64": 11238816777763127430}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c7919858031b5a28798bc22b50a06ec2e8065be48643862660f99ec4e3830bca", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 28, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "a digital transaction and the Association of Certified Fraud Examiners (ACFE) estimates that organizations lose 5% of revenue to fraud each year [acfe-occupational-fraud-2024]. The IAM market is thriving, with an estimated growth of 43 billion USD in 2029 [statista-identity-and-access-management]. Such systems enable an employee’s identification, authentication, and authorization on the organization’s platforms according to assigned roles and responsibilities. Decentralized identities enable an additional approach, such as Bring Your Own Identity (BYOI), where users can use their identity to interact with corporate assets and not just for human resource management practices. Digital Transformation has been a trend for several years and has played a crucial role, particularly in the Workforce, during the COVID-19 pandemic. The pandemic accelerated a variety of phenomena including the trend to remote work. Because remote work implies fewer geography-based constraints, there will be demands for other forms of identification, and for interoperable credentials, in order to meet the demands of a more mobile workforce. The fact is that, net of a further trend in the last year of \"back to the office\", remote workers are estimated to be 67 percent in the technology industry, and this approach is preferred by 91 percent of workers [statista-work-from-home]. In a global context, digital identities can help register employees and contractors by verifying their identities and qualifications, which is particularly challenging for a global workforce. 4.1.1. Organizational Identity § 4.1.2. Identity and Access Management (IAM) § 4.1.3. Global Workforce § 2/6/26, 7:30 PM Identity & the Web 28/43 Although applications with identities linked to individuals are the most studied cases and are delicate to handle, identities also find fertile ground in the supply chain and IoT world, which is decentralized and distributed by nature. A particularly common and interesting scenario is the use of identities and the identification of physical assets and other organizations in the supply chain as well as in end-user services: Identifying physical goods presents unique challenges, such as associating the physical good with the credential. Some solutions include using barcodes, DNA fingerprinting of agricultural products, and radio frequency identification (RFID). In \"Self-Sovereign Identity\" [self-sovereign-identity], we find an", "quality": {"lang": "en", "gibberish_score": 0.0041, "char_len": 2442, "word_len": 350}, "dedupe": {"exact_hash": "f2edfbe86af6028553b3b144f9f6c60625d19b1d3a8a4f3592674d103ea2ab62", "simhash64": 11222211953169368196}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:29bfed3db42da749e78a2b2521669218185a35fbf3df15b40a57042bd0f0d9c5", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 29, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "and other organizations in the supply chain as well as in end-user services: Identifying physical goods presents unique challenges, such as associating the physical good with the credential. Some solutions include using barcodes, DNA fingerprinting of agricultural products, and radio frequency identification (RFID). In \"Self-Sovereign Identity\" [self-sovereign-identity], we find an interesting pilot project in the Energy Sector initiated by the Austrian Power Grid (APG) and Energy Web Foundation (EWF) to enable small and medium-sized devices called Distributed Energetic Resources (DER), to participate in frequency regulation of the national power grid NOTE: By using together the identities issued by governments to both people and organizations, it is possible to make hiring processes smoother with benefits for organizations and people often subject to scams. To enable this scenario, it’s important to have interoperability at both the technical and governance levels. 4.2. Things § 4.2.1. Supply Chain § Import-export markets: the \"cost of trade\" tends to double the cost of a good when it is exported, creating significant barriers to entry, even for small and medium-sized enterprises (SMEs) [edata-verifiable-credentials-for-cross-border-trade]. Digital Identities for other organizations and goods can support the traceability of the supply chain, especially when there are certifications related to sustainable production. Counterfeit-prone markets: such as luxury goods. Proving that the physical good has a proper digital identity and demonstrating the ownership of its Digital Twin in the form of a credential issued by the producer can benefit the end-user and mitigate fraud. 4.2.2. Energy Devices (IoT) § 2/6/26, 7:30 PM Identity & the Web 29/43 [distributed-energy-resources-for-frequency-regulation]. This response to the UN’s Sustainable Development Goal 7 \"Ensure access to affordable, reliable, sustainable and modern energy for all\". The challenge is that the transmission grid must maintain a consistent frequency to function properly. Power plants typically coordinate to adjust the input frequency in response to changes in energy consumption. However, this becomes particularly complex when integrating small and distributed devices. It is necessary to identify small devices correctly to avoid issues throughout the network. Verifiable Credentials are present within the devices' operating systems to ensure the IAM aspect and DIDs to identify them", "quality": {"lang": "en", "gibberish_score": 0.0036, "char_len": 2483, "word_len": 350}, "dedupe": {"exact_hash": "554e627fd9e32ed0ab58ba2a216c31a2df2a552bc47c8731767093685250f1ca", "simhash64": 11149024061135636358}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:79b60adcb6e00c2a74cd7bedd446d1e8be303009ff92721ecb9946b542a750ef", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 30, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "response to changes in energy consumption. However, this becomes particularly complex when integrating small and distributed devices. It is necessary to identify small devices correctly to avoid issues throughout the network. Verifiable Credentials are present within the devices' operating systems to ensure the IAM aspect and DIDs to identify them correctly [energy-web-credentials-overview]. An interesting use case for automotive can be found in \"Self-Sovereign Identity - Foundations, Applications, and Potentials of Portable Digital Identities\" [ssi-foundation-applications-and-potentials]. A car, identified by the Vehicle Identification Number (VIN), interacts with various entities throughout its lifecycle, including: It could also be opened and closed directly through the owner’s Wallet, making the car a Verifier during unlocking and a Subject in the owner’s wallet. This illustrates the utility of IoT identities and credentials and their integration with governmental and human identities [self-sovereign-identity]. For example, when buying and selling a used car, several elements must be verified, such as: 4.2.3. Automotive (IoT) § Manufacturer, vendors and workshops: Tracking maintenance and service history. Governmental Entities: Registration and tax payment. Owners and Users: Ownership verification and usage rights. Road Infrastructure: Toll payments and other interactions during use. Insurance Companies: Policy management and claims processing. The vehicle’s characteristics and history. Ownership verification of the seller. The buyer’s creditworthiness. Completion of ownership transfer and insurance paperwork. 2/6/26, 7:30 PM Identity & the Web 30/43 Let us return to the initial example and analyze human identities, focusing on those issued by the government. These have the most assurance and thus expose the user to the most security, privacy, and human rights threats. A government issues a citizen a specific set of credentials for the purpose of identification and to outline their attributes: We will conduct a thorough historical analysis. Previously, individuals were known and acknowledged based on their physical attributes and voices, particularly in small, close-knit communities where mutual familiarity prevailed. Within such contexts, establishing trust among acquaintances served as an effective means of identification. NOTE: The automotive case is particularly interesting. Even though it is a Non-Human Identity, being often used by humans could have serious privacy implications, as is currently the", "quality": {"lang": "en", "gibberish_score": 0.0035, "char_len": 2553, "word_len": 350}, "dedupe": {"exact_hash": "1eea4eb86d409c0703c1331a76886ab91a973625a0d64f11713641426952f2f1", "simhash64": 11220800180205225030}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:dfd1dc4ce89a72240ee3f18c285100ee39d95f8d3c073ce36fd3b4d0a9a6d2ca", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 31, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "in small, close-knit communities where mutual familiarity prevailed. Within such contexts, establishing trust among acquaintances served as an effective means of identification. NOTE: The automotive case is particularly interesting. Even though it is a Non-Human Identity, being often used by humans could have serious privacy implications, as is currently the case with insurance black boxes. 4.3. Human identities and governments § Travel documents (e.g., Passports and Entry Visas) Personal licenses (e.g., Driver’s Licenses, Amateur radio licenses, Professional licenses, Marriage Licenses) Permits (e.g., Residence Permit, Work Permit) Registration of vehicles, ships, and other property Welfare programs Proof of residency Proof of age 4.3.1. Physical Identity § NOTE: Notably, the assurance of our identity in the social realm often relies on a third party, such as society as a collective entity or directly through government authorities. 2/6/26, 7:30 PM Identity & the Web 31/43 Up until the 1700s-1800s, when there was a lack of direct knowledge between the parties (and thus trust), such as when traveling, to identify oneself, it began to be necessary to present credentials issued by a trusted third party, such as a government, in the form of a paper with written information proofed by the authority. These credentials are issued by a trusted entity (e.g., a government), carried or presented by the person in question (e.g., the user with a passport), and then verified by those in charge to authenticate (e.g., the border police) and provide something (e.g., permission to cross the border). Even then, there were security problems: on the one hand, counterfeiting—which was mitigated by using stamps, seals, or special paper—and the use of documents by persons other than the one for whom the document was issued, which was mitigated by including a written description of the owner’s facial features to bind them to the document as photography had not yet been invented. The first documented use of photography for identification was in 1876, thanks to the photographer William Notman, who had used photographs to identify workers and guests at the Centennial Exposition in Philadelphia [the-world-of-william-notman]. However, government-wide use was introduced only in 1915 after the", "quality": {"lang": "en", "gibberish_score": 0.0043, "char_len": 2303, "word_len": 350}, "dedupe": {"exact_hash": "59e8b0327592ca585a26c8474ffc05f3eada5e943e2419051aa0c239f3cdaabc", "simhash64": 10639840226882983190}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:dda830f185a7bc9d8b3203f3c2d633bb6b07836aaac6d413ef48218822344be8", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 32, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "as photography had not yet been invented. The first documented use of photography for identification was in 1876, thanks to the photographer William Notman, who had used photographs to identify workers and guests at the Centennial Exposition in Philadelphia [the-world-of-william-notman]. However, government-wide use was introduced only in 1915 after the U.S. government discovered that a German spy was using a U.S. passport because he had physical characteristics similar to those described in written words in the passport and could talk in English [how-have-passport-photos-changed-in-100-years]. 4.3.2. Textual Credentials § NOTE: A particularly well-known example of textual credentials is the first driver’s license, issued in 1888 to Karl Benz so he could use his experimental car [how-might-driver-licensing]. It was a paper signed by the local authority (a trusted party), which was required after neighbors complained about noise generated by his driving, so not for identifying himself. NOTE: the process used by text credentials has the same structure as that used for digital credentials described above. 4.3.3. Photographic Credentials § NOTE: The primary purpose of photography is to associate the passport with the individual to whom it was issued. It is essential to ensure that only the legitimate holder of the credential can utilize it. 2/6/26, 7:30 PM Identity & the Web 32/43 As the technology evolved, the idea was to use machines to help read the documents. This would speed up the verification process. But it was necessary to make the documents easy for machines to read. To address this, particularly for travel documents, ICAO began working on machine-readable travel documents in 1968, and in 1980, it published Document 9303, which contained the specification of a machine-readable code to be printed on documents [doc-9303]. It is the code with many \"<\"s in our passports and on some ID cards. As an evolution, in 1998, Doc 9303 also included biometric information transmitted via RFID technology. Nowadays, other machine-readable techniques include barcodes and QR codes. While these practices have certainly sped up reading and verification in physical contexts - when the verifier has access to the original physical document, they are inefficient if used in", "quality": {"lang": "en", "gibberish_score": 0.0035, "char_len": 2294, "word_len": 350}, "dedupe": {"exact_hash": "2c1329b1867db825111cb3a19980928188897d55b357456ec6a01ff03901c420", "simhash64": 11225624839401357718}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f55debe3380000f59e5f002c627101e1e4a84be6b2c6d32d905c2afb93c378e5", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 33, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "1998, Doc 9303 also included biometric information transmitted via RFID technology. Nowadays, other machine-readable techniques include barcodes and QR codes. While these practices have certainly sped up reading and verification in physical contexts - when the verifier has access to the original physical document, they are inefficient if used in a digital context, in particular when the verifier has no access to the original document as the physical credential is scanned or photographed and its file is used. A classic use of government-issued documents on the Internet and the Web is enrollment in financial services. The user must indeed provide these documents. At the same time, the financial service provider must verify that they comply with Know Your Customer (KYC) and Anti-Money Laundering (AML) practices to Counter the Financing of Terrorism (CFT). Then, the user photographs or scans the documents (rendering ineffective the anti-counterfeiting measures inherent in the physical document) and themself (to bind with the document) and sends these files to the financial provider. Often, the financial provider delegates the process to specialized companies that use Machine Learning and manual control to verify the information. Thus, we have at least two problems: the entire document is sent to different places, making a data breach more likely, and Machine Learning systems often analyze it. The problem is well described in 4.3.4. Machine Readable Credentials § NOTE: ISO endorsed this document through ISO/IEC 7501-1, making the role of Standard Development Organizations (SDOs) particularly important for interoperability in this field. 4.3.5. Physical Credentials as Digital Credentials § 2/6/26, 7:30 PM Identity & the Web 33/43 \"AI & the Web\". Moreover, an additional privacy concern is inherent in this use case - which applies even when the document is used physically. Even if the user uses the document for a specific reason (e.g., proof of address or proof of age), they must send the whole document, thus showing more information than is needed for the specific verification, violating the privacy principle of data minimization. Governments and regulatory bodies have also stepped up to issue digital credentials for citizens. Each government has made its", "quality": {"lang": "en", "gibberish_score": 0.0035, "char_len": 2288, "word_len": 350}, "dedupe": {"exact_hash": "5fd2b0e72fe95c2ae7481ad002c8481ffbb5db78f316b7db00e5052194e28f10", "simhash64": 10636741733826675078}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2b80c744945cc364c508b024d0512b95befb91c87f3f2b47b5152d64c11c970c", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 34, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "proof of address or proof of age), they must send the whole document, thus showing more information than is needed for the specific verification, violating the privacy principle of data minimization. Governments and regulatory bodies have also stepped up to issue digital credentials for citizens. Each government has made its own architectural choices and can offer different services, from centralized or federated authentication to decentralized identities that give citizens a wallet to hold their digital credentials. Below is a short list with some implementation examples: Some governments are doing pilot projects with Decentralized Identities, providing their citizens with Digital Wallets and IDs. Let us delve into an extensively debated use case requiring a solution: age verification. 4.3.6. Pure Digital Credentials § Canada: Pan-Canadian Trust Framework™ China: IIFAA Decentralized Trusted Authentication Technical Specification - Part 1: General Requirements. Estonia: Estonian e-Identity. Europe: European Digital Identity Architecture and Reference Framework (EUDI-ARF). India: Aadhaar. Italy: Italy’s Public Digital Identity System (SPID). Nigeria: Nigeria’s eID. Singapore: Singpass. Spain: Cl@ve. Switzerland: Swiss E-ID & Trust Infrastructure United Arab Emirates: UAE Pass. United States of America: U.S. DHS on Digital Identities and Mobile Driving Licence (e.g., Maryland, Arizona, Utah, California). 2/6/26, 7:30 PM Identity & the Web 34/43 The holder has a digital passport in the form of government-issued credentials; these credentials, in their claims, also contain age information. The presentation can be done in different ways, providing different levels of privacy. The problem is that, even in the last two cases, we can present potentially linkable information to us or our issuer, which the verifier can use to make correlations. For example, it is necessary to decouple the signature from the signer and not use the same identifiers in different sessions. Conversely, the verifier will have to prove that they performed the age verification, further complicating the matter. Therefore, even in a scenario that may seem trivial, it requires extensive study. Full Credential: It is possible to send the full credential since it also contains the date of birth, from which the verifier can derive the age. However, this doesn’t meet the principle of Data", "quality": {"lang": "en", "gibberish_score": 0.0046, "char_len": 2389, "word_len": 350}, "dedupe": {"exact_hash": "6255f5a6fbc505af68f825bb5ecce843f61dc2e1017e142c7617fbf0d2664ffe", "simhash64": 11230093252474734214}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7182c201ba51073d52860e5073ffe588465b6c9883bd73fb6f719cfca9902222", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 35, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "complicating the matter. Therefore, even in a scenario that may seem trivial, it requires extensive study. Full Credential: It is possible to send the full credential since it also contains the date of birth, from which the verifier can derive the age. However, this doesn’t meet the principle of Data Minimization, as I’m sending a lot of other information that can be misused and make us traceable. Selective Disclosure [selective-disclosure]: Suppose the credential provided supports this privacy feature, which allows us to send individual attributes/claims and hide the others. In that case, we can send only the date of birth, by which the verifier can derive the age. It certainly improves the situation concerning Data Minimization, but it does not solve it totally. To overcome this problem, some credentials have specific attributes with boolean values to present that our age exceeds a certain value (e.g., 16, 18, 21). Range Proof [range-proofs]: Zero-knowledge range proofs allow a prover to convince a verifier that a secret value lies in a given interval (without showing the credential attribute). If the verifier ask for a specific attribute is within a given range, a range proof-presentation can be sent to the verifier (e.g., the verifier asks us if we are older than 21 years old, we send the result of the computation on the date of birth that proves that our age falls in that range without revealing it). 2/6/26, 7:30 PM Identity & the Web 35/43 Mitigating the threats at technological and governance levels In the context of high-assurance credentials, particularly those issued by governments, even the solution to a seemingly simple problem requires a thorough analysis of the impacts these solutions may have on the population. As we have analyzed, an end-to-end solution requires the conjunction of technological aspects related to the standardization of technologies, their implementation, and their adoption, which is defined by elements of governance that permeate the technological aspects. In this specific case, we have different stakeholders, such as SDOs, implementers, and governments, who, through regulatory bodies, define the needs, requirements, architectures, and, last but not least, the users impacted", "quality": {"lang": "en", "gibberish_score": 0.0031, "char_len": 2246, "word_len": 350}, "dedupe": {"exact_hash": "62401555b95c86da06db83452a27e7d92f20b1804991b61189c2136bc051b25e", "simhash64": 11208980361478770950}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4472addf717e513d23d6a85bbb3e3967386743230f3649a3b5a058a98009db1a", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 36, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "of technologies, their implementation, and their adoption, which is defined by elements of governance that permeate the technological aspects. In this specific case, we have different stakeholders, such as SDOs, implementers, and governments, who, through regulatory bodies, define the needs, requirements, architectures, and, last but not least, the users impacted by these solutions. Therefore, it is important for all these stakeholders to work together for joint value creation [stakeholder-relationships-and-responsibilities] and ensure the proper handling of threats to security, privacy, and human rights. Some threats exist at the technology level and can be managed by SDOs and implementers, but governments must manage others at the governance level: Active cooperation between governments, SDOs, implementers, and users is essential. SDOs can be a neutral forum to discuss these issues and create value. Several individuals contributed to the document. The editor especially thanks Pierre-Antoine Champin, Andrea D’Intino, Giuseppe De Marco, Heather Flanagan, Ivan Herman, Tommaso Innocenti, Ian Jacobs, Philippe Le Hegaret, Coralie Mercier, and Denis Roio. A centralized system is prone to surveillance. In contrast, a decentralized system with certain technological features and cryptographic methods can mitigate surveillance and respect human rights. When a decentralized system is used, issues related to digital wallets arise. On the one hand, it is necessary to balance security and hardware and software requirements that could discriminate. On the other hand, it is important to avoid vendor lock-in and prevent what happened with the Digital Market Act and default browser choice. If threats cannot be effectively managed at the technology level, they should be addressed at the governance level. This can involve prohibiting certain uses or removing features that cannot be technically mitigated to reduce the threat. 5. Acknowledgment § 2/6/26, 7:30 PM Identity & the Web 36/43 Margaret Stutley; James Stutley. A Dictionary of Hinduism. 2019. #WhyID. URL: https://www.accessnow.org/campaign/whyid/ Occupational Fraud 2024: A Report To The Nations®. URL: https://legacy.acfe.com/report-tothe-nations/2024/ Felix Linker; David Basin. ADEM: An Authentic Digital EMblem. URL: https://dl.acm.org/doi/10.1145/3576915.3616578 CREDENTIAL | English meaning. URL: https://dictionary.cambridge.org/dictionary/english/credential trusT | English meaning. URL: https://dictionary.cambridge.org/dictionary/english/trust Constructing an Identity. In: Fan, S., Fielding-Wells, J. (eds) What is Next in Educational Research?. URL:", "quality": {"lang": "en", "gibberish_score": 0.0137, "char_len": 2635, "word_len": 350}, "dedupe": {"exact_hash": "45d23fd45ef9bf797b14dfe7696c6e2c39fad6cd4f418a4c1a8b99279d0e6a8e", "simhash64": 15852472779573531782}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:32a7cb03d56c155def9e0470c16ec32eff4eec79ecaed932ac6a1ee8c7e048eb", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 37, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "https://www.accessnow.org/campaign/whyid/ Occupational Fraud 2024: A Report To The Nations®. URL: https://legacy.acfe.com/report-tothe-nations/2024/ Felix Linker; David Basin. ADEM: An Authentic Digital EMblem. URL: https://dl.acm.org/doi/10.1145/3576915.3616578 CREDENTIAL | English meaning. URL: https://dictionary.cambridge.org/dictionary/english/credential trusT | English meaning. URL: https://dictionary.cambridge.org/dictionary/english/trust Constructing an Identity. In: Fan, S., Fielding-Wells, J. (eds) What is Next in Educational Research?. URL: https://doi.org/10.1007/978-94-6300-524-1_8 Lea Verou; Martin Thomson; Jeffrey Yasskin. Web Platform Design Principles. URL: https://w3ctag.github.io/design-principles/ Manu Sporny; et al. Decentralized Identifiers (DIDs) v1.0. URL: https://w3c.github.io/did-core/ 6. Revision History § 25 February 2025: Added BBS, Status List (Thanks to Kristina Yasuda) 4 November 2024: Clarifying Turst Concept (Thanks to Veronica Cristiano) 13 Semptember 2024: Improved definitions of VC and VP (Thanks to Veronica Cristiano), IndieAuth (Thanks to Tantek Çelik) 30 August 2024: Added Government Issued use-cases 13 August 2024: First publication References § Informative References § [A-DICTIONARY-OF-HINDUISM] [ACCESS-NOW-WHYID] [ACFE-OCCUPATIONAL-FRAUD-2024] [ADEM] [CAMBRIDGE-DICTIONARY-IDENTITY] [CAMBRIDGE-DICTIONARY-TRUST] [CONSTRUCTING-AN-IDENTITY] [DESIGN-PRINCIPLES] [DID-CORE] 2/6/26, 7:30 PM Identity & the Web 37/43 DID FAQ. URL: https://identity.foundation/faq/ Introduction to Decentralized Identifiers (DID). URL: https://www.youtube.com/watch? v=t8lMCmjPKq4 Manu Sporny; Markus Sabadello. Decentralized Identifier Extensions. URL: https://w3c.github.io/did-extensions/ Digital Credentials That Can Be Verified: A Lesson in Terminology. URL: https://sphericalcowconsulting.com/2025/01/20/digital-credentials-that-can-be-verified-a-lessonin-terminology/ Digital Credentials. Draft Community Group Report. URL: https://wicg.github.io/digitalcredentials/ Digital Credentials API Explainer. URL: https://github.com/WICG/digitalcredentials/blob/main/explainer.md Digitalizing the Red Cross, Red Crescent and Red Crystal Emblems: Benefits, Risks, and Possible Solutions. URL: https://dl.acm.org/doi/10.1145/3576915.3616578 Rene Descartes. Discourse On The Method. 1637. Austrian Power Grid and Energy Web Foundation Launch Proof of Concept to Use Distributed Energy Resources for Frequency Regulation. URL: https://medium.com/energy-webinsights/austrian-power-grid-and-energy-web-foundation-launch-proof-of-concept-to-usedistributed-energy-d9a378f5f5ee Machine Readable Travel Documents. URL: https://www.icao.int/publications/Documents/9303_p1_cons_en.pdf eDATA Verifiable Credentials for Cross Border Trade. URL: https://unece.org/sites/default/files/2023-08/WhitePaper_VerifiableCredentialsCrossBorderTrade_September2022.pdf Digital Identification Must Be Designed for Privacy and Equity. URL: https://www.eff.org/deeplinks/2020/08/digital-identification-must-be-designed-privacy-and- [DID-FAQ] [DID-INTRO] [DID-SPEC-REGISTRIES] [DIGITAL-CREDENTIALS-THAT-CAN-BE-VERIFIED] [DIGITAL-IDENTITIES] [DIGITAL-IDENTITY-EXPLAINER] [DIGITALIZING-REPORT] [DISCOURSE-ON-THE-METHOD] [DISTRIBUTED-ENERGY-RESOURCES-FOR-FREQUENCY-REGULATION] [DOC-9303] [EDATA-VERIFIABLE-CREDENTIALS-FOR-CROSS-BORDER-TRADE] [EFF-DIGITAL-IDENTIFICATION] 2/6/26, 7:30 PM Identity & the Web 38/43 equity-10 Credentials-Overview. URL: https://energy-web-foundation.gitbook.io/energy-web/ew-dostechnology-components-2023/identity-and-access-management-iam/patterns/credentiallifecycle#credentials-overview Evolution of the ToIP Stack. URL: https://trustoverip.org/wp-content/uploads/Evolution-of-theToIP-Stack-V1.0-2022-11-14.pdf Nicolas Pena Moreno. Federated Credential Management API. URL: https://w3cfedid.github.io/FedCM/ Harms modeling. URL: https://learn.microsoft.com/en-us/azure/architecture/guide/responsibleinnovation/harms-modeling/ Justin Parkinson. How have passport photos changed in 100 years?. URL: https://www.bbc.com/news/magazine-30988833 Scott McLachlan. How might Driver Licensing and Vehicle Registration evolve if we adopt Autonomous Cars and Digital Identification? [preprint]. URL: https://www.researchgate.net/publication/358738434_How_might_Driver_Licensing_and_Vehicl e_Registration_evolve_if_we_adopt_Autonomous_Cars_and_Digital_Identification Human rights and technical", "quality": {"lang": "en", "gibberish_score": 0.038, "char_len": 4419, "word_len": 350}, "dedupe": {"exact_hash": "a941c170eb005705707deb0446a6fe5aeb1f6c9873c1a4e9c6f557db22bbbccf", "simhash64": 11231641364880364498}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:be3cdcaab13d53879d4c7d99d18dee578d6ee2191c4dd36a457c8e7e66037992", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 38, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "Pena Moreno. Federated Credential Management API. URL: https://w3cfedid.github.io/FedCM/ Harms modeling. URL: https://learn.microsoft.com/en-us/azure/architecture/guide/responsibleinnovation/harms-modeling/ Justin Parkinson. How have passport photos changed in 100 years?. URL: https://www.bbc.com/news/magazine-30988833 Scott McLachlan. How might Driver Licensing and Vehicle Registration evolve if we adopt Autonomous Cars and Digital Identification? [preprint]. URL: https://www.researchgate.net/publication/358738434_How_might_Driver_Licensing_and_Vehicl e_Registration_evolve_if_we_adopt_Autonomous_Cars_and_Digital_Identification Human rights and technical standard-setting processes for new and emerging digital technologies : report of the Office of the United Nations High Commissioner for Human Rights. URL: https://digitallibrary.un.org/record/4031373?v=pdf International Covenant on Civil and Political Rights. URL: https://www.ohchr.org/en/instruments-mechanisms/instruments/international-covenant-civil-andpolitical-rights Sustainable Development Goals. URL: https://www.worldbank.org/content/dam/Worldbank/Governance/GGP%20ID4D%20flyer.pdf Heather Flanagan. Identity on the Web. 2024. URL: https://www.w3.org/2024/04/AC/talk/identity [ENERGY-WEB-CREDENTIALS-OVERVIEW] [EVOLUTION-TOIP] [FEDCM] [HARMS-MODELING] [HOW-HAVE-PASSPORT-PHOTOS-CHANGED-IN-100-YEARS] [HOW-MIGHT-DRIVER-LICENSING] [HUMAN-RIGHTS-AND-TECHNICAL-STANDARDS] [ICCPR] [ID4D-initiative] [IDENTITY-ON-THE-WEB] 2/6/26, 7:30 PM Identity & the Web 39/43 Introduction to Trust Over IP. URL: https://trustoverip.org/wp-content/uploads/Introduction-toToIP-V2.0-2021-11-17.pdf IT Security and Privacy — A framework for identity management — Part 1: Terminology and concepts. 2019. URL: https://www.iso.org/obp/ui/#iso:std:iso-iec:24760:-1:ed-2:v1:en LINDDUN: PRIVACY THREAT MODELING. URL: https://linddun.org Sustainable Development Goals. URL: https://www.wipo.int/marrakesh_treaty Types of credentials and authenticators. URL: https://doi.org/10.6028/NIST.SP.800-63-3 Digital Identities: Getting to Know the Verifiable Digital Credential Ecosystem. URL: https://www.nist.gov/blogs/cybersecurity-insights/digital-identities-getting-know-verifiabledigital-credential-ecosystem Peter Steiner. On the Internet, Nobody Knows You're a Dog. Open Source Security Testing Methodology Manual v3. URL: https://www.isecom.org/OSSTMM.3.pdf T. F. Hoad. Identity - The Concise Oxford Dictionary of English Etymology. 2003. URL: https://www.oxfordreference.com/display/10.1093/acref/9780192830982.001.0001/acref9780192830982-e-7482 Passkeys 101. URL: https://fidoalliance.org/passkeys/ Robin Berjon; Jeffrey Yasskin. Privacy Principles. URL: https://w3ctag.github.io/privacyprinciples/ Miranda Christ; et al. SoK: Zero-Knowledge Range Proofs. URL: https://eprint.iacr.org/2024/430 R. Rivest. The MD5 Message-Digest Algorithm. April 1992. Informational. URL: https://www.rfc-editor.org/rfc/rfc1321 [INTRODUCTION-TOIP] [ISO-IEC-24760-1] [LINDDUN] [MARRAKESH-TREATY] [NIST-SP-800-63-3] [NIST-VERIFIABLE-DIGITAL-CREDENTIAL] [NOBODY-KNOWS-YOU-RE-A-DOG] [OSSTMM-3] [OXFORD-ETYMOLOGY-IDENTITY] [PASSKEYS-101] [PRIVACY-PRINCIPLES] [RANGE-PROOFS] [RFC1321] 2/6/26, 7:30 PM Identity & the Web 40/43 T. Berners-Lee; R. Fielding; H. Frystyk. Hypertext Transfer Protocol -- HTTP/1.0. May 1996. Informational. URL: https://www.rfc-editor.org/rfc/rfc1945 J. Franks; et al. An Extension to HTTP : Digest Access Authentication. January 1997. Proposed Standard. URL: https://www.rfc-editor.org/rfc/rfc2069 T. Dierks; C. Allen. The TLS Protocol Version 1.0. January 1999. Historic. URL: https://www.rfceditor.org/rfc/rfc2246 E. Rescorla; B. Korver. Guidelines for Writing RFC Text on Security Considerations. July 2003. Best Current Practice. URL: https://www.rfc-editor.org/rfc/rfc3552 T. Berners-Lee; R. Fielding; L. Masinter. Uniform Resource Identifier (URI): Generic Syntax. January 2005. Internet Standard. URL: https://www.rfc-editor.org/rfc/rfc3986 D. M'Raihi; et al. HOTP: An", "quality": {"lang": "en", "gibberish_score": 0.0373, "char_len": 4023, "word_len": 350}, "dedupe": {"exact_hash": "85fdf938e3feea9ffcd5418a66a8f5c800d4da881edf2455afa71b222177c24c", "simhash64": 1944937458265560724}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f58c3aad695c20bfeab21f457c172093406415ba0e65be13e96f7556dfecfe10", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 39, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "Version 1.0. January 1999. Historic. URL: https://www.rfceditor.org/rfc/rfc2246 E. Rescorla; B. Korver. Guidelines for Writing RFC Text on Security Considerations. July 2003. Best Current Practice. URL: https://www.rfc-editor.org/rfc/rfc3552 T. Berners-Lee; R. Fielding; L. Masinter. Uniform Resource Identifier (URI): Generic Syntax. January 2005. Internet Standard. URL: https://www.rfc-editor.org/rfc/rfc3986 D. M'Raihi; et al. HOTP: An HMAC-Based One-Time Password Algorithm. December 2005. Informational. URL: https://www.rfc-editor.org/rfc/rfc4226 D. M'Raihi; et al. TOTP: Time-Based One-Time Password Algorithm. May 2011. Informational. URL: https://www.rfc-editor.org/rfc/rfc6238 A. Cooper; et al. Privacy Considerations for Internet Protocols. July 2013. Informational. URL: https://www.rfc-editor.org/rfc/rfc6973 Sustainable Development Goals. URL: https://sdgs.un.org/goals/goal16#targets_and_indicators Rouslan Solomakhin; Stephen McGruer. Secure Payment Confirmation. URL: https://w3c.github.io/secure-payment-confirmation/ Ben Laurie. Selective Disclosure (v0.2). URL: https://www.links.org/files/selective-disclosure.pdf Alex Preukschat; Drummond Reed. Self-Sovereign Identity. Strüker, Jens; et al. Self-Sovereign Identity - Foundations, Applications, and Potentials of Portable Digital Identities. URL: https://www.researchgate.net/publication/354653404_SelfSovereign_Identity_-_Foundations_Applications_and_Potentials_of_Portable_Digital_Identities [RFC1945] [RFC2069] [RFC2246] [RFC3552] [RFC3986] [RFC4226] [RFC6238] [RFC6973] [SDGS-16] [SECURE-PAYMENT-CONFIRMATION] [SELECTIVE-DISCLOSURE] [SELF-SOVEREIGN-IDENTITY] [SSI-FOUNDATION-APPLICATIONS-AND-POTENTIALS] 2/6/26, 7:30 PM Identity & the Web 41/43 Chiara Civera; R. Edward Freeman. Stakeholder Relationships and Responsibilities: A New Perspective. URL: https://doi.org/10.4468/2019.1.04civera.freeman Alexandra Borgeaud. Identity and Access Management - statistics & facts. URL: https://www.statista.com/topics/10552/identity-and-access-management/ Ahmed Sherif. Work from home: remote & hybrid work - Statistics & Facts. URL: https://www.statista.com/topics/6565/work-from-home-and-remote-work/ STRIDE model. URL: https://shostack.org/files/microsoft/The-Threats-To-Our-Products.docx Heather Flanagan. The Evolving Landscape of Non-Human Identity. 2024. URL: https://sphericalcowconsulting.com/2024/04/05/the-evolving-landscape-of-non-human-identity/ Hall, Roger; Gordon Dodds; Stanley Triggs. The World of William Notman. Amy Guy; Daniel Appelquist; Hadley Beeman. Third Party Cookies Must Be Removed. URL: https://www.w3.org/2001/tag/doc/web-without-3p-cookies/ Adam Shostack. Threat Modeling: Designing for Security. Timothy Ruff. Three Models of Digital Identity. URL: https://medium.com/evernym/the-threemodels-of-digital-identity-relationships-ca0727cb5186 Types of credentials and authenticators. URL: https://id4d.worldbank.org/guide/typescredentials-and-authenticators Universal Declaration of Human Rights. URL: https://www.un.org/en/universal-declarationhuman-rights/ Paul Watzlawick. Ultra-Solutions: How to Fail Most Successfully. UNHCR Strategy on Digital Identity and Inclusion. URL: https://www.unhcr.org/blogs/wpcontent/uploads/sites/48/2018/03/2018-02-Digital-Identity_02.pdf [STAKEHOLDER-RELATIONSHIPS-AND-RESPONSIBILITIES] [STATISTA-IDENTITY-AND-ACCESS-MANAGEMENT] [STATISTA-WORK-FROM-HOME] [STRIDE] [THE-EVOLVING-LANDSCAPE-OF-NON-HUMAN-IDENTITY] [THE-WORLD-OF-WILLIAM-NOTMAN] [THIRD-PARTY-COOKIES-MUST-BE-REMOVED] [THREAT-MODELING-DESIGNING-FOR-SECURITY] [THREE-MODELS-OF-DIGITAL-IDENTITY-RELATIONSHIPS] [TYPES-OF-CREDENTIALS-AND-AUTHENTICATORS] [UDHR] [ULTRA-SOLUTIONS] [UNHCR-digital-identity] 2/6/26, 7:30 PM Identity & the Web 42/43 Manu Sporny; et al. Verifiable Credentials Data Model v2.0. URL: https://w3c.github.io/vc-datamodel/ Kaliya Young. Verifiable Credentials Flavors Explained. URL: https://www.lfph.io/wpcontent/uploads/2021/02/Verifiable-Credentials-Flavors-Explained.pdf Jeff Hodges; et al. Web Authentication: An API for accessing Public Key Credentials - Level 2. URL: https://w3c.github.io/webauthn/ Tim Cappalli; et al. Web Authentication: An API for accessing Public Key", "quality": {"lang": "en", "gibberish_score": 0.0332, "char_len": 4191, "word_len": 350}, "dedupe": {"exact_hash": "7bb9693565cfdedf7c25a38f8c711a9608af81bb3a2952f3bfdf52b3ef26b39f", "simhash64": 11239962473443169878}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c35bae96bd680641498677716f1bfcf4746fc7e0f05e71913fb762144254eeb8", "doc_id": "sha256:59b42c7ea24cfc1d9fdd7569c9c651a3b32c4099a3cfa7d6f45752a7f69c7b67", "chunk_index": 40, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Identity & the Web.pdf", "page_start": null, "page_end": null, "text": "42/43 Manu Sporny; et al. Verifiable Credentials Data Model v2.0. URL: https://w3c.github.io/vc-datamodel/ Kaliya Young. Verifiable Credentials Flavors Explained. URL: https://www.lfph.io/wpcontent/uploads/2021/02/Verifiable-Credentials-Flavors-Explained.pdf Jeff Hodges; et al. Web Authentication: An API for accessing Public Key Credentials - Level 2. URL: https://w3c.github.io/webauthn/ Tim Cappalli; et al. Web Authentication: An API for accessing Public Key Credentials - Level 3. URL: https://w3c.github.io/webauthn/ Nkem Ifejika. What does ubuntu really mean?. 2006. URL: https://www.theguardian.com/theguardian/2006/sep/29/features11.g2 [VC-DATA-MODEL-2.0] [VERIFIABLE-CREDENTIALS-FLAVORS-EXPLAINED] [WEBAUTHN-2] [WEBAUTHN-3] [WHAT-DOES-UBUNTU-REALLY-MEAN] 2/6/26, 7:30 PM Identity & the Web 43/43", "quality": {"lang": "en", "gibberish_score": 0.0397, "char_len": 806, "word_len": 79}, "dedupe": {"exact_hash": "acb1c36ed7d779995c5b5c456df4e799984549ec04283e3e473d9bc7a016bb04", "simhash64": 11221948003773285330}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4d5b66aac973bbc4b944f7152cfcc82069ce7041543c0a6f1ab8d1a86efd218a", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 0, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation Vadim Sokolov George Mason University Farnaz Behnia George Mason University Dominik Karbowski Argonne National Laboratory Abstract—Synthetic vehicle speed trajectory generation is essential for evaluating vehicle control algorithms and connected vehicle technologies. Traditional Markov chain approaches suffer from discretization artifacts and limited expressiveness. This paper proposes a physics-informed diffusion framework for conditional micro-trip synthesis, combining a dual-channel speed-acceleration representation with soft physics constraints that resolve optimization conflicts inherent to hard-constraint formulations. We compare a 1D UNet architecture against a transformer-based Conditional Score-based Diffusion Imputation (CSDI) model using 6,367 GPS-derived micro-trips. CSDI achieves superior distribution matching (Wasserstein distance 0.30 for speed, 0.026 for acceleration), strong indistinguishability from real data (discriminative score 0.49), and validated utility for downstream energy assessment tasks. The methodology enables scalable generation of realistic driving profiles for intelligent transportation systems (ITS) applications without costly field data collection. I. Introduction Accurate modeling of vehicle speed trajectories is fundamental to numerous applications in intelligent transportation systems (ITS), including energy consumption assessment (Karbowski, Sokolov, and Rousseau 2015; Karbowski et al. 2016; Karbowski, Sokolov, and Jongryeol 2016), route optimization (Moawad et al. 2021), traﬀic simulation (Auld et al. 2013; Auld, Hope, et al. 2016), and predictive control of autonomous vehicles (Dauner et al. 2023). The ability to generate synthetic speed profiles that faithfully reproduce the statistical and physical characteristics of real-world driving behavior enables scalable evaluation of transportation policies, vehicle technologies, and traﬀic management strategies without the prohibitive cost of extensive field data collection. Traditional approaches to synthetic trajectory generation have relied on Markov chain models (Karbowski et al., n.d., 2016), which discretize the speed state space and model transitions between consecutive states. While computationally eﬀicient and interpretable, these methods face fundamental limitations: rigid discretization loses finegrained dynamics, the Markov assumption ignores longrange temporal dependencies, and incorporating physicsbased constraints or conditional controls (e.g., powertrain type, road conditions) requires ad-hoc engineering. More recent efforts have explored deep generative models, including generative adversarial networks (GANs) (Behnia, Karbowski, and Sokolov 2023; Yoon, Jarrett, and van der Schaar 2019) and normalizing flows (Papamakarios et al. 2019), with mixed success", "quality": {"lang": "en", "gibberish_score": 0.0003, "char_len": 2878, "word_len": 350}, "dedupe": {"exact_hash": "4b8fb2363f5fc7a61e12da64b88f210098f8c75b42d9c30b2ac6530e0972c096", "simhash64": 9794232152982914838}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:098a2f7f85c64de92a09a5f2f6af3d2570125472f7f0936e0ca3560e11bce1f0", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 1, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "and incorporating physicsbased constraints or conditional controls (e.g., powertrain type, road conditions) requires ad-hoc engineering. More recent efforts have explored deep generative models, including generative adversarial networks (GANs) (Behnia, Karbowski, and Sokolov 2023; Yoon, Jarrett, and van der Schaar 2019) and normalizing flows (Papamakarios et al. 2019), with mixed success due to training instability, mode collapse, and diﬀiculty enforcing hard boundary constraints. This paper introduces an application of denoising diffusion probabilistic models (DDPMs) (Ho, Jain, and Abbeel 2020) to the problem of conditional vehicle speed trajectory synthesis. Diffusion models have emerged as an effective class of generative models, demonstrating strong performance in image generation, audio synthesis, and recently, time series forecasting (Ansari et al. 2025). Their key advantages include stable training dynamics, the ability to naturally model complex multi-modal distributions, and relevant for this application, a principled mechanism for enforcing constraints through inpainting during the reverse diffusion process. Unlike prior work on diffusionbased trajectory generation for autonomous driving (Feng et al. 2023; Suo et al. 2021), which focuses on spatial path planning in multi-agent scenarios, we target univariate speed profiles for energy assessment applications, requiring strict enforcement of micro-trip boundaries (zero initial and final speeds) and precise control over aggregate statistics (average speed, duration). We compare two diffusion-based architectures: a standard 1D U-Net diffusion model and a Conditional Score-based Diffusion Imputation (CSDI) model (Tashiro et al. 2021) adapted from time series imputation. Both models are conditioned on trip characteristics (target average speed, duration) and trained on 6,367 micro-trip observations from the 2007 Chicago Metropolitan Agency for Planning (CMAP) Regional Household Travel Survey. Our investigation systematically explores the evolution of these models through multiple design iterations, documenting the failures of hard-constraint physics penalties in standard diffusion and the successful integration of soft physics constraints in CSDI. We benchmark these approaches against traditional Markov chain baselines and analyze the failure modes of alternative deep generative methods (DoppelGANger, SDV) to provide guidance for practitioners. arXiv:2602.05028v1 [stat.AP] 4 Feb 2026 The practical utility of high-fidelity synthetic trajectory generation spans several key areas of intelligent transportation systems. For energy assessment, synthetic trajectories enable evaluation of", "quality": {"lang": "en", "gibberish_score": 0.0, "char_len": 2682, "word_len": 350}, "dedupe": {"exact_hash": "eadee195e99b12cfed94112f7fcfb39faf41c7cb58d580c0060efc902074185b", "simhash64": 11239878837284194070}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:86bc8d02b632c2eac89ba0167e2492d3bf32426d78330b50f46a1905617ba919", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 2, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "traditional Markov chain baselines and analyze the failure modes of alternative deep generative methods (DoppelGANger, SDV) to provide guidance for practitioners. arXiv:2602.05028v1 [stat.AP] 4 Feb 2026 The practical utility of high-fidelity synthetic trajectory generation spans several key areas of intelligent transportation systems. For energy assessment, synthetic trajectories enable evaluation of electric vehicle range, charging infrastructure requirements, and fleet electrification strategies across diverse driving scenarios without costly field trials (Chen et al. 2016; X. Huang et al. 2020). Traﬀic microsimulation platforms like POLARIS (Auld, Hope, et al. 2016) require realistic speed profiles to accurately model emissions, fuel consumption, and networklevel energy impacts. Connected and autonomous vehicle testing demands diverse, physically plausible trajectories for validating perception, planning, and control systems (Mozaffari et al. 2020), as well as generating representative safety-critical scenarios (Wu et al. 2025). By demonstrating both distributional fidelity (via Wasserstein metrics) and downstream utility (via TSTR), this work provides a validated methodology for generating synthetic driving data that directly supports ITS energy management, simulation-based policy analysis, and intelligent vehicle development. The principal contributions of this work are: 1. The first application of diffusion models specifically to vehicle speed micro-trip generation for transportation energy assessment, demonstrating superior distribution matching compared to Markov chains and GANs. 2. The first successful integration of kinematic physics constraints into transformer-based diffusion for transportation, combining direct speed and acceleration channels with physics-based training objectives to effectively bridge the gap between deep generative modeling and kinematic consistency. 3. Documentation of the complete model development lifecycle, including failures (hard-constraint diffusion, DoppelGANger, SDV), providing insights into why certain approaches fail and how to avoid common pitfalls in generative modeling for transportation applications. 4. A rigorous evaluation framework encompassing distributional fidelity (Wasserstein distance, MMD), kinematic validity (smoothness metrics, boundary violations), and utility (discriminative score, TSTR), establishing best practices for assessing synthetic trajectory quality. 5. Open-source implementation and reproducibility package at https://github.com/VadimSokolo v/diffusion- trajectory- generation, including complete training code, pretrained model weights, preprocessed data, and figure generation scripts enabling full reproduction of all results. The remainder of this paper is organized as follows: Section II reviews related work in vehicle trajectory", "quality": {"lang": "en", "gibberish_score": 0.0014, "char_len": 2849, "word_len": 350}, "dedupe": {"exact_hash": "995f71ca96791bf44fddf46d96010beefef8cefe5bbce68aba636c6f28920624", "simhash64": 11239949204914799894}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7adf84656dc0d32fb7a583a8a71970986f5c85a3f40737b89bbdd0e000429ab4", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 3, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "synthetic trajectory quality. 5. Open-source implementation and reproducibility package at https://github.com/VadimSokolo v/diffusion- trajectory- generation, including complete training code, pretrained model weights, preprocessed data, and figure generation scripts enabling full reproduction of all results. The remainder of this paper is organized as follows: Section II reviews related work in vehicle trajectory generation, deep generative models for time series, and diffusion models. Section III describes the CMAP dataset, preprocessing, and clustering analysis. Section IV presents the methodology, including problem formulation, the Markov chain baseline, and detailed descriptions of the diffusion and CSDI architectures along with their evolution through multiple design iterations. Section V reports quantitative and qualitative results, comparing all approaches and documenting failure modes. Section VI discusses implications for practice, limitations, and future research directions. II. Related Work The problem of generating realistic vehicle trajectories has been approached from multiple perspectives in the transportation literature. Early work relied on parametric models and Markov processes to capture driving patterns (Karbowski et al., n.d., 2016), where speed transitions are modeled as discrete-state stochastic processes. Karbowski et al. developed Markov chain models for trip prediction in energy-eﬀicient vehicle routing, demonstrating that second-order Markov models can capture speedacceleration dependencies (Karbowski, Sokolov, and Jongryeol 2016). While interpretable and computationally eﬀicient, these approaches are limited by their memoryless nature and inability to model long-range temporal dependencies inherent in driving behavior. Agent-based traﬀic simulation platforms such as POLARIS (Sokolov, Auld, and Hope 2012; Auld et al. 2013; Auld, Hope, et al. 2016) have incorporated microsimulation of individual vehicle movements to assess regional transportation policies. These systems combine activitybased demand models with network assignment, requiring realistic speed profiles as inputs (Auld, Karbowski, et al. 2016). Moawad et al. proposed neural recommender systems for real-time route assignment considering energy consumption, highlighting the need for fast generation of representative speed trajectories conditioned on route characteristics (Moawad et al. 2021). More recently, Behnia et al. explored deep generative models for vehicle speed trajectories, comparing variational autoencoders (VAEs), GANs, and normalizing flows (Behnia, Karbowski, and Sokolov 2023). Their analysis demonstrated that GANs achieved the best distribution matching but suffered from training instability and", "quality": {"lang": "en", "gibberish_score": 0.0015, "char_len": 2744, "word_len": 350}, "dedupe": {"exact_hash": "90f6ffb969561cad74b70c1e224612e98955a1c627755bc649090cbbdfed10d5", "simhash64": 11239878837275952918}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4228915fa5a4dcadda4a752e0aa3ce5f4da0a12c07589b895952f846369c8445", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 4, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "conditioned on route characteristics (Moawad et al. 2021). More recently, Behnia et al. explored deep generative models for vehicle speed trajectories, comparing variational autoencoders (VAEs), GANs, and normalizing flows (Behnia, Karbowski, and Sokolov 2023). Their analysis demonstrated that GANs achieved the best distribution matching but suffered from training instability and mode collapse, particularly for diverse driving regimes. This work established important evaluation metrics for trajectory generation but did not explore diffusion models, which have since emerged as a more stable alternative to GANs. Adjacent to single-vehicle speed generation is the rapidly growing literature on multi-agent trajectory prediction and scenario generation for autonomous vehicle testing. TraﬀicGen (Feng et al. 2023) and ScenarioNet (Li et al. 2023) introduced learning-based frameworks for generating diverse, realistic traﬀic scenarios from large-scale driving datasets. TraﬀicSim (Suo et al. 2021) proposed a multi-agent behavioral model learning to simulate realistic interactions among vehicles. These works differ fundamentally from our focus: they generate spatial trajectories (x, y coordinates over time) for multiple interacting agents in 2D road networks, whereas we target univariate speed profiles for isolated micro-trips. The former requires modeling complex spatial interactions and collision avoidance; the latter emphasizes matching kinematic distributions and enforcing strict boundary conditions for energy assessment. Nevertheless, recent applications of diffusion models to autonomous driving trajectory planning (Liao et al. 2024) and controllable motion generation (Lan et al. 2025) demonstrate the potential of this generative modeling paradigm for transportation applications. The application of deep generative models to sequential data has evolved rapidly. TimeGAN (Yoon, Jarrett, and van der Schaar 2019) extended GANs to time series by combining unsupervised adversarial learning with supervised stepwise prediction, improving temporal coherence. DoppelGANger addressed the challenge of generating mixed-type time series with both temporal features and static attributes through a dual-stage generation process. However, both methods remain susceptible to mode collapse and require careful architectural tuning to enforce constraints. Autoregressive models, including probabilistic variants implemented in the Synthetic Data Vault (SDV), represent another popular approach. The PARSynthesizer uses probabilistic autoregression to sequentially generate time steps conditioned on previous values. While simple to implement, these models suffer from exposure bias—errors", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2698, "word_len": 350}, "dedupe": {"exact_hash": "79e88764e8c5cba2297a4a95547df4565328e2551412c1b596c045d05915610f", "simhash64": 11239947075716498198}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e8f8c843bf7dbe9faf8314a22a4e98868ef8610e5824d57ffd7b878b1491e1cc", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 5, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "mode collapse and require careful architectural tuning to enforce constraints. Autoregressive models, including probabilistic variants implemented in the Synthetic Data Vault (SDV), represent another popular approach. The PARSynthesizer uses probabilistic autoregression to sequentially generate time steps conditioned on previous values. While simple to implement, these models suffer from exposure bias—errors accumulate over long sequences—and struggle with capturing diverse multi-modal distributions without explicit diversity mechanisms. Foundation models for time series have recently emerged as zero-shot alternatives. Chronos (Ansari et al. 2025) adapts transformer language model architectures to probabilistic forecasting by tokenizing time series and training on diverse datasets. While showing promise for forecasting tasks, these models lack fine-grained control over generation and have not been extensively evaluated for constrained trajectory synthesis. Denoising diffusion probabilistic models (DDPMs) (Ho, Jain, and Abbeel 2020) define a forward process that gradually corrupts data with Gaussian noise and learn a reverse process to denoise random samples into data. Unlike GANs, diffusion models optimize a tractable variational lower bound, resulting in stable training and high sample quality. Score-based generative models (Y. Song et al. 2020) provide an equivalent formulation by learning the gradient of the data log-density (score function) and using Langevin dynamics for sampling. The application of diffusion models to time series has largely focused on forecasting and imputation. CSDI (Conditional Score-based Diffusion for Imputation) (Tashiro et al. 2021) introduced a transformer-based diffusion architecture for missing value imputation in multivariate time series, demonstrating superior performance to autoregressive and GAN-based methods. The key innovation is self-attention’s ability to capture long-range dependencies while the diffusion framework handles uncertainty. We adapt this architecture for conditional generation by treating the entire trajectory as “missing” conditioned on aggregate trip statistics. Recent extensions in the ITS domain have combined diffusion with GANs for trajectory reconstruction (Qian et al. 2025) and proposed interpretable causal diffusion networks for speed prediction (Rong et al. 2025). TimeGrad (Rasul et al. 2021) introduced autoregressive denoising diffusion for probabilistic time series forecasting, demonstrating strong performance on multivariate prediction tasks. SSSD (Alcaraz and Strodthoff 2022) combined structured state-space models with diffusion for imputation and forecasting. TimeWeaver and Diffusion-TS have", "quality": {"lang": "en", "gibberish_score": 0.0018, "char_len": 2706, "word_len": 350}, "dedupe": {"exact_hash": "f70a7d2b6ce489e45c400050b2ca54e8a1a0eadaadab306be0d77c04a85dda29", "simhash64": 11240518753009878806}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:135d4ac9325a51c52db476471792a163a66aeee0a36290742290c6c389758ac8", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 6, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "causal diffusion networks for speed prediction (Rong et al. 2025). TimeGrad (Rasul et al. 2021) introduced autoregressive denoising diffusion for probabilistic time series forecasting, demonstrating strong performance on multivariate prediction tasks. SSSD (Alcaraz and Strodthoff 2022) combined structured state-space models with diffusion for imputation and forecasting. TimeWeaver and Diffusion-TS have further extended diffusion models to multivariate time series synthesis. However, these methods do not address the specific challenges of vehicle trajectory generation: strict boundary constraints (zero start/end speeds), physicsbased plausibility (acceleration limits, smoothness), and fine-grained conditional control (vehicle type, road conditions). While deep learning has been extensively applied to trajectory prediction in ITS (Schultz and Sokolov 2018; Mozaffari et al. 2020; Zhao et al. 2019; Altché and de La Fortelle 2017; Y. Huang et al. 2022), generation of complete speed profiles for energy assessment remains underexplored. Multi-agent diffusion models have also been explored for traﬀic flow prediction (Adam et al. 2025). To our knowledge, this work is the first to apply diffusion models specifically to vehicle speed micro-trip generation for energy assessment, systematically comparing U-Net and transformer-based architectures and documenting the integration of physics-informed constraints. III. Data The data for this study originate from the 2007 Chicago Metropolitan Agency for Planning (CMAP) Regional Household Travel Survey, conducted by NuStats for CMAP, an 11-county region, encompassing dense urban (Chicago), suburban, and rural environments. This geographic diversity ensures representation of varied driving conditions, road types, and traﬀic patterns. The survey employed a dual-frame sampling strategy combining Random Digit Dialing (RDD) and address-based sampling to reach over 3.2 million households. Data collection utilized Computer-Assisted Telephone Interview (CATI), mail questionnaires, and follow-up telephone interviews, achieving an overall response rate of 10 percent (19 percent recruitment rate, 55 percent retrieval rate) (Chicago Metropolitan Agency for Planning 2008). A subset of participating households contributed GPS-based trip-level data, which forms the basis of our analysis. The dataset captures passenger vehicle movements (cars, SUVs, vans, pickup trucks) used for personal travel. Heavyduty commercial vehicles and fleet operations are excluded. Road type coverage includes Interstate highways and expressways, major arterials connecting economic centers, and local urban and suburban roads.", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2664, "word_len": 350}, "dedupe": {"exact_hash": "783c167f3a2a94b06e973ac72a39da01c770b76613c051386d591580b46cd0f6", "simhash64": 11239890931878790918}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d3ae356326ee0dee8ecc7fd0f84e5e9adee48ac1f10da6d326f1622a444a47d2", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 7, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "data, which forms the basis of our analysis. The dataset captures passenger vehicle movements (cars, SUVs, vans, pickup trucks) used for personal travel. Heavyduty commercial vehicles and fleet operations are excluded. Road type coverage includes Interstate highways and expressways, major arterials connecting economic centers, and local urban and suburban roads. No filtering by road type was applied during collection; GPS traces reflect the natural distribution of household travel patterns. A. Exploratory Data Analysis Following preprocessing to extract individual micro-trips (continuous speed profiles beginning and ending at zero velocity), our dataset comprises 6,367 observations sampled at 1 Hz. Table I summarizes key distributional statistics. Table I: Dataset Summary Statistics Attribute Min Max Mean Median Duration (s) 34 12,841 304 187 Duration (min) 0.57 214 5.06 3.12 Distance (m) 257 393,070 5,884 3,076 Distance (km) 0.26 393 5.88 3.08 Avg Speed (m/s) 5.18 31.57 16.92 16.45 Avg Speed (km/h) 18.63 113.66 60.92 59.21 The dataset exhibits substantial variation in trip characteristics, with durations ranging from half a minute to over three hours and distances spanning two orders of magnitude. Average speeds vary from slow urban crawl (5 m/s ≈18 km/h) to highway cruising (32 m/s ≈115 km/h), reflecting the diversity of driving contexts in the metropolitan area. 0 100 200 300 400 500 Duration (seconds) 0 50 100 150 200 250 300 Frequency Trip Duration Mean Median 0 2 4 6 Distance (km) 0 50 100 150 200 250 300 Frequency Trip Distance Mean Median 25 50 75 100 Average Speed (km/h) 0 100 200 300 400 500 Frequency Average Speed Mean Median Figure 1: Data distributions for trip duration, distance, and average speed. Histograms show the empirical distributions from 6,367 micro-trips with mean (red dashed) and median (orange dashed) markers. The distributions exhibit substantial heterogeneity, reflecting diverse driving contexts from urban congestion to highway cruising. To characterize the heterogeneity of driving patterns in the dataset, we performed K-means clustering (K=4) on extracted features from each trajectory. Clustering serves two purposes: (1) understanding the data structure to inform model design, and (2) enabling stratified evaluation to ensure generated trajectories cover all driving", "quality": {"lang": "en", "gibberish_score": 0.0043, "char_len": 2334, "word_len": 350}, "dedupe": {"exact_hash": "f9a4b01b81d5adc5bd847e51345e7f5e52b44b1416583a5edf65a07b507bfa87", "simhash64": 11221877701651273998}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ad17fc23072c7ca97cdd5e4b851b1779867eca6555d42759d54d6c98555b07c3", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 8, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "congestion to highway cruising. To characterize the heterogeneity of driving patterns in the dataset, we performed K-means clustering (K=4) on extracted features from each trajectory. Clustering serves two purposes: (1) understanding the data structure to inform model design, and (2) enabling stratified evaluation to ensure generated trajectories cover all driving regimes. Feature Extraction: For each micro-trip, we computed: • Average speed ( ̄ 𝑣) and maximum speed (𝑣𝑚𝑎𝑥) • Speed standard deviation • Idle time ratio (fraction of time at 𝑣< 0.5 m/s) • Stops per kilometer (number of near-zero speed events per unit distance) • Acceleration noise (standard deviation of acceleration) Clustering Results: The algorithm identified four distinct driving regimes, described in Table II: Table II: Cluster Characteristics Cluster Label Count Avg Speed (m/s) Max Speed (m/s) Stops/km Idle Ratio (%) 0 Arterial/ Suburban 2,224 15.6 22.6 0.59 2.0 1 Highway/ Interstate 1,020 22.2 30.8 0.12 0.5 2 Congested/ City 636 13.9 21.8 1.29 4.4 3 Free-flow Arterial 2,487 16.7 22.7 0.28 1.0 Cluster 1 (Highway/Interstate) exhibits the highest average and maximum speeds with minimal stops and idle time, characteristic of uninterrupted highway travel. Cluster 2 (Congested/City) shows the opposite profile: low speeds, frequent stops (1.29 per km), and high idle time (4.4 percent), typical of stop-and-go urban driving. Clusters 0 and 3 represent intermediate arterial road conditions, with Cluster 3 having somewhat higher speeds and fewer stops, suggesting less congested arterial travel. (a) PCA (b) t-SNE Figure 2: PCA and t-SNE projections of micro-trips in feature space. Principal component analysis (PCA) and t-SNE projections (Figure 2) visualize the cluster separation in feature space, with the first two PCA components explaining approximately 65% of the variance. t-SNE reveals a tighter cluster structure that highlights the distinct kinematic signatures of the four driving regimes. While clusters form distinguishable groups, the continuous transitions between them reflect the spectrum of real-world driving conditions rather than discrete categories. Relatedly, Figure 3 display example speed profiles from each cluster, selected from the cluster-to-trip mapping to illustrate characteristic patterns. 0 20 40 60 80 Time (s) 0 10 20 30 Speed (m/s) Trip ID: 10", "quality": {"lang": "en", "gibberish_score": 0.0085, "char_len": 2365, "word_len": 350}, "dedupe": {"exact_hash": "c6ae90a6db2df2bd4d1f2936ef58a0c7b8b5fb5ddfd63d8f5b61f1eb5cb8afca", "simhash64": 53232101286070942}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2b1afa3d6f80474c46a3b8632bbd4d03e3876fac8b18470151a7b02714489988", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 9, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "continuous transitions between them reflect the spectrum of real-world driving conditions rather than discrete categories. Relatedly, Figure 3 display example speed profiles from each cluster, selected from the cluster-to-trip mapping to illustrate characteristic patterns. 0 20 40 60 80 Time (s) 0 10 20 30 Speed (m/s) Trip ID: 10 (a) Arterial, Cluster 0. 0 50 100 150 200 250 Time (s) 0 10 20 30 Speed (m/s) Trip ID: 102 (b) Highway, Cluster 1. 0 50 100 150 200 250 300 Time (s) 0 10 20 30 Speed (m/s) Trip ID: 11624 (c) City congested, Cluster 2. 0 25 50 75 100 125 150 175 Time (s) 0 10 20 30 Speed (m/s) Trip ID: 10288 (d) City free-flow, Cluster 3. Figure 3: Examples of observed trajectories from each cluster. These clusters inform our evaluation strategy: we assess whether generated trajectories match the proportions and characteristics of each driving regime, ensuring the model captures both highway eﬀiciency and urban congestion patterns. IV. Methodology Let v = (𝑣0, 𝑣1, … , 𝑣𝑇) ∈ℝ𝑇+1 represent a vehicle speed trajectory sampled at 1 Hz, where 𝑡∈{0, 1, … , 𝑇} indexes time in seconds. Our unit of analysis is the micro-trip: a stop-to-stop segment that begins and ends at zero velocity. Each complete trip in the dataset is partitioned into a sequence of such micro-trips, segmenting when the vehicle comes to rest. This decomposition allows us to model the fundamental building blocks of driving behavior; a complete origin-to-destination journey can be reconstructed by concatenating generated micro-trips. Each micro-trip satisfies the following constraints: Boundary conditions: 𝑣0 = 𝑣𝑇= 0 (trip begins and ends at rest) Non-negativity: 𝑣𝑡≥0 ∀𝑡∈{0, … , 𝑇} Conditioning variables: We denote the conditioning information as c ∈ℝ𝑑, typically comprising: • ̄ 𝑣: target average speed (m/s) • 𝑇: trip duration (seconds) • 𝑣𝑚𝑎𝑥: maximum speed (m/s) [for CSDI only] • 𝑑𝑣𝑒ℎ: vehicle dynamics parameter ∈[0, 1] [for CSDI only] The objective is to learn a conditional generative model 𝑝𝜃(v|c) that: 1. Generates trajectories matching the empirical distribution 𝑝𝑑𝑎𝑡𝑎(v) 2. Satisfies physical plausibility constraints (smooth acceleration, bounded jerk) 3. Precisely", "quality": {"lang": "en", "gibberish_score": 0.0138, "char_len": 2175, "word_len": 350}, "dedupe": {"exact_hash": "4177e7e3a76ea64c8176a9c4ebde3741fc46b224ed0b229709b27c4d1071390b", "simhash64": 269404883399835954}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2ac33e3af0ef84c5359147f6e691b6d2be3872c7b4bbfb2da7355c9883f9fcd0", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 10, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "duration (seconds) • 𝑣𝑚𝑎𝑥: maximum speed (m/s) [for CSDI only] • 𝑑𝑣𝑒ℎ: vehicle dynamics parameter ∈[0, 1] [for CSDI only] The objective is to learn a conditional generative model 𝑝𝜃(v|c) that: 1. Generates trajectories matching the empirical distribution 𝑝𝑑𝑎𝑡𝑎(v) 2. Satisfies physical plausibility constraints (smooth acceleration, bounded jerk) 3. Precisely controls aggregate trip statistics via conditioning c Formally, we seek to minimize the distributional divergence: min 𝜃 𝐷𝐾𝐿(𝑝𝑑𝑎𝑡𝑎(v|c)‖𝑝𝜃(v|c)) while enforcing boundary and kinematic constraints. A. Baseline: Markov Chain Approach Traditional trajectory generation relies on discrete-state Markov models (Karbowski et al., n.d.). We implement a second-order Markov chain baseline following Karbowski et al.’s methodology as a reference point for evaluating deep generative models. State Space Discretization: Speed is discretized into bins of width Δ𝑣= 0.5 m/s, creating bins {𝐵0, 𝐵1, … , 𝐵𝐾} where 𝐵𝑖= [𝑖Δ𝑣, (𝑖+1)Δ𝑣). Let 𝑠𝑡∈{0, 1, … , 𝐾} denote the bin index at time 𝑡. Second-Order Markov Model: The model assumes speed at time 𝑡depends on the previous two time steps: 𝑃(𝑠𝑡|𝑠𝑡−1, 𝑠𝑡−2) = 𝐶(𝑠𝑡−2, 𝑠𝑡−1, 𝑠𝑡) ∑𝑠′ 𝐶(𝑠𝑡−2, 𝑠𝑡−1, 𝑠′) where 𝐶(⋅) denotes empirical counts from the training data. This can be reformulated as a first-order Markov chain on pair-states 𝑋𝑡= (𝑠𝑡−1, 𝑠𝑡) with state space size 𝐾2. Boundary-Constrained Sampling (Markov Bridge): To enforce 𝑣0 = 𝑣𝑇= 0, we employ forward-backward sampling (Durham and Gallant 2002). Backward messages 𝛽𝑡(𝑥) represent the probability of reaching the terminal state (bin 0) from state 𝑥at time 𝑡: 𝛽𝑇(𝑥) = 𝟙[𝑥end = 0], 𝛽𝑡(𝑥) = ∑ 𝑥′ 𝑃(𝑥′|𝑥)𝛽𝑡+1(𝑥′) Forward sampling at each step uses the modified transition probabilities: 𝑃bridge(𝑥𝑡+1|𝑥𝑡) ∝𝑃(𝑥𝑡+1|𝑥𝑡) ⋅𝛽𝑡+1(𝑥𝑡+1) Post-Processing: To reduce discretization artifacts, we apply Gaussian smoothing to the acceleration signal 𝑎𝑡= 𝑣𝑡+1 −𝑣𝑡with a 5-point moving average, followed by integration to recover smooth speed profiles. Limitations: The Markov baseline suffers from three fundamental weaknesses: (1) rigid bin discretization loses finegrained dynamics, (2) limited temporal memory cannot capture long-range trip structure (e.g., highway segments followed by urban navigation), and (3) conditional control requires separate models per condition or rejection sampling, both of which are ineﬀicient. All models were trained on an NVIDIA A100 GPU (40GB) using", "quality": {"lang": "en", "gibberish_score": 0.0232, "char_len": 2417, "word_len": 350}, "dedupe": {"exact_hash": "7c5804020a940a65fd5e5ea248207f464a57ee82adfd6643e53f7e2418c843f3", "simhash64": 5097329654890350342}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:be81e98679dd435dd8cd33baf7231cecb7f5aabf41787b0eabbea2d023b420a7", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 11, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "rigid bin discretization loses finegrained dynamics, (2) limited temporal memory cannot capture long-range trip structure (e.g., highway segments followed by urban navigation), and (3) conditional control requires separate models per condition or rejection sampling, both of which are ineﬀicient. All models were trained on an NVIDIA A100 GPU (40GB) using PyTorch 2.0. Training utilized mixed-precision (FP16) computation to accelerate convergence and reduce memory footprint. The Markov chain baseline requires no GPU training, completing parameter estimation (transition matrix construction) in under 5 minutes on a single CPU core. B. Conditional Diffusion Model with 1D U-Net Our diffusion model employs a 1D U-Net encoder-decoder architecture with Feature-wise Linear Modulation (FiLM) for conditioning. The input is a two-channel tensor x𝑡∈ ℝ2×512 representing the joint speed-acceleration state: x0 = [𝑣0, 𝑣1, … , 𝑣511 𝑎0, 𝑎1, … , 𝑎511 ] where 𝑎𝑡= 𝑣𝑡+1 −𝑣𝑡is the discrete acceleration. Trajectories shorter than 512 seconds are zero-padded; longer trajectories are truncated (less than 1% of data). The encoder consists of four downsampling blocks with channel dimensions [64, 128, 256, 512], each containing: • 1D convolution (kernel size 3, stride 2) • Two ResNet blocks with Group Normalization and SiLU activation • FiLM conditioning layers injecting time embedding and trip conditions Self-attention layers are inserted at resolutions 128 and 64 to capture long-range dependencies without computational explosion at the full 512 resolution. The decoder mirrors the encoder with skip connections, upsampling to reconstruct the predicted noise 𝜖𝜃(x𝑡, 𝑡, c) ∈ ℝ2×512. Time Embedding: The diffusion timestep 𝑡∈{1, … , 𝑇𝑑𝑖𝑓𝑓} is embedded using sinusoidal positional encoding: PE𝑖(𝑡) = {sin(𝑡/10000𝑖/𝑑) 𝑖even cos(𝑡/10000𝑖/𝑑) 𝑖odd projected to 256 dimensions. Conditioning via FiLM: At each ResNet block, the conditioning vector c = [ ̄ 𝑣/30, 𝑇/1000] (normalized) is transformed into scale and shift parameters using Feature-wise Linear Modulation (FiLM) (Perez et al. 2018): FiLM(h, c) = 𝛾(c) ⊙h + 𝛽(c) where 𝛾, 𝛽∶ℝ2 →ℝ𝑑ℎare learned MLP projections. This multiplicative and additive modulation allows the model to adapt its feature representations based on trip conditions. Figure 4: Simplified U-Net Diffusion Architecture. Noisy trajectory 𝑥𝑡is encoded to a latent representation and decoded to predict", "quality": {"lang": "en", "gibberish_score": 0.0137, "char_len": 2410, "word_len": 350}, "dedupe": {"exact_hash": "2e22efb2e442a9c787eba219ead0fcda4fc642a38e84d692f1cc4961bc811344", "simhash64": 4881066712302978822}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:36f1c57c03a4fe02a82c6aeac101e1f34b1e58317aee9fd3c50c7e1475dc4319", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 12, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "2018): FiLM(h, c) = 𝛾(c) ⊙h + 𝛽(c) where 𝛾, 𝛽∶ℝ2 →ℝ𝑑ℎare learned MLP projections. This multiplicative and additive modulation allows the model to adapt its feature representations based on trip conditions. Figure 4: Simplified U-Net Diffusion Architecture. Noisy trajectory 𝑥𝑡is encoded to a latent representation and decoded to predict noise, modulated by time 𝑡and conditions 𝑐. 1) Forward Diffusion Process: We define a forward Markov process that gradually corrupts the data with Gaussian noise over 𝑇𝑑𝑖𝑓𝑓= 1000 steps: 𝑞(x𝑡|x0) = 𝒩(x𝑡; √̄ 𝛼𝑡x0, (1 − ̄ 𝛼𝑡)I) where ̄ 𝛼𝑡= ∏ 𝑡 𝑠=1(1 −𝛽𝑠) and 𝛽𝑡follows a linear schedule from 𝛽1 = 0.0001 to 𝛽𝑇𝑑𝑖𝑓𝑓= 0.02. This choice balances noise corruption (ensuring x𝑇𝑑𝑖𝑓𝑓≈𝒩(0, I)) with smooth transitions. 2) Training Objective: The model is trained to predict the noise 𝜖∼𝒩(0, I) added at each diffusion step. The simplified variational lower bound objective is: ℒsimple =𝔼𝑡∼𝑈(1,𝑇𝑑𝑖𝑓𝑓),x0∼𝑝𝑑𝑎𝑡𝑎,𝜖∼𝒩(0,I) [∥𝜖−𝜖𝜃(√̄ 𝛼𝑡x0 + √1 − ̄ 𝛼𝑡𝜖, 𝑡, c)∥ 2] This objective is optimized using Adam (𝛽1 = 0.9, 𝛽2 = 0.999) with learning rate 10−4 and batch size 32. The model is trained with ℒsimple for 1000 epochs, with boundary enforcement via post-processing (tail ramping and velocity scaling). Early experiments with hard physics constraints (distance matching, jerk penalties, boundary enforcement) failed catastrophically—producing 100% boundary violations and degraded distribution matching (WD Speed = 4.40 vs. 0.56 for the baseline). This fundamental conflict between diffusion denoising and stiff physics optimization motivated our transition to CSDI’s threshold-activated soft constraints. 3) Reverse Sampling (Generation): Starting from pure noise x𝑇𝑑𝑖𝑓𝑓∼𝒩(0, I), we iteratively denoise using the learned reverse process: x𝑡−1 = 1 √𝛼𝑡 (x𝑡− 1 −𝛼𝑡 √1 − ̄ 𝛼𝑡 𝜖𝜃(x𝑡, 𝑡, c)) + 𝜎𝑡z where z ∼𝒩(0, I) and 𝜎2 𝑡 = (1−̄ 𝛼𝑡−1) (1−̄ 𝛼𝑡) 𝛽𝑡controls the stochasticity of the reverse step. Inpainting for Boundary Enforcement: At each reverse step, we enforce 𝑣0 = 𝑣𝑇= 0 through constrained inpainting. Define a mask 𝑀∈{0, 1}512 with 𝑀0 = 𝑀𝑇= 1 and 𝑀𝑡= 0 otherwise. After computing x𝑡−1, we replace masked positions: x𝑡−1 ∶= x𝑡−1 ⊙(1 −𝑀) + xknown 𝑡−1 ⊙𝑀 where xknown 𝑡−1 contains zeros at the boundary", "quality": {"lang": "en", "gibberish_score": 0.0347, "char_len": 2220, "word_len": 350}, "dedupe": {"exact_hash": "faf64ad2d5f155e831f355a2a4b003f91c92a799be983230f62476ce2fe952fb", "simhash64": 4953214466764130214}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:bf0d77fb201bc4c6b899deea4f6ce6c032b30fa4c9c40cd9671e250eab5402d4", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 13, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "each reverse step, we enforce 𝑣0 = 𝑣𝑇= 0 through constrained inpainting. Define a mask 𝑀∈{0, 1}512 with 𝑀0 = 𝑀𝑇= 1 and 𝑀𝑡= 0 otherwise. After computing x𝑡−1, we replace masked positions: x𝑡−1 ∶= x𝑡−1 ⊙(1 −𝑀) + xknown 𝑡−1 ⊙𝑀 where xknown 𝑡−1 contains zeros at the boundary indices. This direct enforcement ensures strict satisfaction of constraints without degrading the learned diffusion dynamics. Classifier-Free Guidance (optional): To amplify conditioning strength, we employ classifier-free guidance (Ho and Salimans 2022) with scale 𝑤: ̃ 𝜖𝜃= (1 + 𝑤)𝜖𝜃(x𝑡, 𝑡, c) −𝑤𝜖𝜃(x𝑡, 𝑡, ∅) where ∅denotes unconditional generation (achieved by randomly dropping conditions during training with probability 0.1). We increased 𝑤from 1.0 in the baseline to 3.0 in the final model to improve average speed matching. Generation of 1000 trajectories requires approximately 5 minutes on A100. C. Conditional Score-based Diffusion Imputation (CSDI) CSDI adapts the transformer-based imputation model of Tashiro et al. (Tashiro et al. 2021) for conditional trajectory generation. Unlike the 1D U-Net which processes speed-acceleration jointly, CSDI operates on univariate speed sequences v ∈ℝ512, relying on the transformer’s self-attention to implicitly model temporal derivatives. The architecture consists of: Input Projection: The noisy trajectory v𝑡is linearly projected to 𝑑model = 256 dimensions and combined with learnable positional encodings: h0 = Linear(v𝑡) + PE Time Embedding: Diffusion timestep 𝑡is embedded via sinusoidal encoding (as in the U-Net model) and broadcastadded to all sequence positions: h0 ∶= h0 + TimeEmbed(𝑡) Condition Injection: The conditioning vector c = [ ̄ 𝑣/30, 𝑇/1000, 𝑣𝑚𝑎𝑥/40, 𝑑𝑣𝑒ℎ] ∈ℝ4 (normalized) is embedded to 256 dimensions and injected via cross-attention in each transformer layer. Transformer Encoder: Six layers of multi-head selfattention (8 heads) with feed-forward networks (𝑑𝑓𝑓= 1024): q = Heads(SelfAttn(hℓ)) hℓ+1 = LayerNorm(hℓ+ q) hℓ+1 = LayerNorm(hℓ+1 + FFN(hℓ+1)) Output Projection: The final hidden states are projected to predict the noise 𝜖𝜃∈ℝ512. The model has approximately 5.5M parameters, smaller than the U-Net (~8M) but with greater receptive field due to global attention. Figure 5: CSDI Transformer architecture (5.5M parameters). Noisy trajectory 𝑥𝑡, diffusion time 𝑡, and conditions 𝑐are embedded and processed through 6 transformer encoder layers with", "quality": {"lang": "en", "gibberish_score": 0.0212, "char_len": 2405, "word_len": 350}, "dedupe": {"exact_hash": "d6e5e3d3cc8b622ea06a78058b699413522411dbf00806aa6e3fb85774b8d81b", "simhash64": 1350334764918064710}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c4d5b30ad362feaa21657c02036edf18b043408c531256d2cf539f82c57e301f", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 14, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "to predict the noise 𝜖𝜃∈ℝ512. The model has approximately 5.5M parameters, smaller than the U-Net (~8M) but with greater receptive field due to global attention. Figure 5: CSDI Transformer architecture (5.5M parameters). Noisy trajectory 𝑥𝑡, diffusion time 𝑡, and conditions 𝑐are embedded and processed through 6 transformer encoder layers with positional encoding, time injection, and condition injection. Output is predicted noise ̂ 𝜖.” Reverse Diffusion (Sampling): Generation proceeds from pure noise to a clean trajectory through iterative denoising: x𝑇∼𝒩(0, I) →x𝑇−1 →⋯→x1 →x0 (clean trajectory) Similar to the U-Net model, we apply hard boundary constraints at each reverse step via inpainting, ensuring 𝑣0 = 𝑣𝑇= 0 throughout the denoising trajectory. 1) Physics-Informed Training: Unlike the failed PID diffusion approach, CSDI successfully integrates physics constraints through carefully weighted auxiliary losses. The total training objective is: ℒCSDI = ℒMSE + 0.1ℒsmooth + 0.03ℒaccel + 0.02ℒjerk + 0.05ℒaccel_dist where: Denoising Loss: ℒMSE = ‖𝜖−𝜖𝜃(v𝑡, 𝑡, c)‖ 2 Temporal Smoothness Loss (second derivative penalty): ℒsmooth = ∑ 𝑡 (𝑣𝑡+2 −2𝑣𝑡+1 + 𝑣𝑡)2 Acceleration Penalty (soft constraint on extremes): ℒaccel = ∑ 𝑡 [ReLU(𝑎𝑡−4.0)2 + ReLU(−𝑎𝑡−5.0)2] where 𝑎𝑡= 𝑣𝑡+1 −𝑣𝑡. Positive acceleration is capped at 4 m/s², braking at 5 m/s². Jerk Penalty (comfort and realism): ℒjerk = ∑ 𝑡 ReLU(|𝑗𝑡| −2.0)2, 𝑗𝑡= 𝑎𝑡+1 −𝑎𝑡 Acceleration Distribution Matching: ℒaccel_dist = (𝜎pred 𝑎 −𝜎target 𝑎 )2 where 𝜎target 𝑎 ≈0.5 m/s² is the empirical standard deviation. CSDI succeeds where hard-constraint diffusion fails for three reasons: (1) threshold-activated penalties (ReLU gating) allow natural variation within physical bounds rather than enforcing stiff equality constraints; (2) the transformer’s global self-attention better accommodates long-range kinematic consistency than local convolutions; and (3) careful weight tuning ensures physics losses remain 20-200× smaller than the primary MSE objective, preventing gradient dominance. Training for 200 epochs required approximately 12 hours on A100. Vehicle-Type Compatibility: The physics constraints (4/5 m/s²) were chosen to cover the acceleration capabilities of all common vehicle types: Table III: Vehicle-Type Compatibility and Acceleration Limits Vehicle Max Accel Max Decel Sports car 4-6 m/s2 10-12 m/s2 Passenger car 2.5-4 m/s2 8-10 m/s2 SUV 2-3.5 m/s2 7-9 m/s2 Bus 1-2 m/s2 4-6", "quality": {"lang": "en", "gibberish_score": 0.0262, "char_len": 2440, "word_len": 350}, "dedupe": {"exact_hash": "c2ac77687d4409e697217f56272f232ba5f16822704c6ce01eadf334b74c9a79", "simhash64": 4953148496066463042}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:b0a7c2e9c7674bbf4412cf9f81134970b3447c27a91533f2653b2ec21d4de3ac", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 15, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "The physics constraints (4/5 m/s²) were chosen to cover the acceleration capabilities of all common vehicle types: Table III: Vehicle-Type Compatibility and Acceleration Limits Vehicle Max Accel Max Decel Sports car 4-6 m/s2 10-12 m/s2 Passenger car 2.5-4 m/s2 8-10 m/s2 SUV 2-3.5 m/s2 7-9 m/s2 Bus 1-2 m/s2 4-6 m/s2 Heavy truck 0.5-1.5 m/s2 3-5 m/s2 These values are consistent with established vehicle dynamics characteristics (Gillespie 1992), ensuring the model generates trajectories that are physically realizable across the full spectrum of road vehicles encountered in the CMAP dataset. Weighted Condition Sampling (Boost): To address data imbalance where certain speed/duration ranges are underrepresented in training data, we use importance sampling during generation. The probability of sampling a condition 𝑐is weighted: 𝑃(𝑐) ∝(AvgSpeed)𝛽𝑠⋅(Duration)𝛽𝑑, where boost parameters 𝛽𝑠, 𝛽𝑑∈[0, 2] control oversampling of high-speed or long-duration trips. With 𝛽𝑠= 1.75 for the U-Net Diffusion model, we effectively fill the sparse “blue gap” (25-30 m/s) in the speed histogram. The CSDI model uses uniform sampling (𝛽= 1.0) as its larger training set provides adequate coverage. 2) Optimization of Kinematic Quality: The CSDI model development addressed three quality issues: (1) Highfrequency jitter from the standard imputation objective was resolved through the temporal smoothness loss ℒsmooth (weight 0.1), increased diffusion steps (100→200), and post-processing Gaussian smoothing; (2) Boundary and peak distortion from aggressive uniform smoothing was addressed by reducing kernel size and implementing conditional boundary ramps—only applied when endpoints were not already near zero; (3) Acceleration distribution mismatch was resolved through threshold-activated physics penalties (ℒaccel, ℒjerk, ℒaccel_dist), expanded 4D conditioning including vehicle dynamics, increased model capacity (4→6 transformer layers, 𝑑𝑓𝑓512→1024), and data augmentation for heavy vehicles. A cosine noise schedule replaced the linear schedule. These enhancements achieved WD Speed = 0.30 and strict physical validity. 3) CSDI Sampling and Post-Processing: CSDI uses the same DDPM reverse diffusion as the U-Net model but with a cosine noise schedule (Nichol and Dhariwal 2021): ̄ 𝛼𝑡= 𝑓(𝑡) 𝑓(0), 𝑓(𝑡) = cos2 (𝑡/𝑇𝑑𝑖𝑓𝑓+ 𝑠 1 + 𝑠 ⋅𝜋 2 ) where 𝑠= 0.008 is a small offset preventing singularity. The cosine schedule concentrates more diffusion steps in the high-SNR", "quality": {"lang": "en", "gibberish_score": 0.0143, "char_len": 2456, "word_len": 350}, "dedupe": {"exact_hash": "679b5561d42dd628bd82d1e17b1193ac70fe4821302333069c72d82462b471b9", "simhash64": 4953218864860987650}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:dae924b912408c418f6e15ac3aa224d630ebfcae87a9c747af009fc129b922ff", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 16, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "reverse diffusion as the U-Net model but with a cosine noise schedule (Nichol and Dhariwal 2021): ̄ 𝛼𝑡= 𝑓(𝑡) 𝑓(0), 𝑓(𝑡) = cos2 (𝑡/𝑇𝑑𝑖𝑓𝑓+ 𝑠 1 + 𝑠 ⋅𝜋 2 ) where 𝑠= 0.008 is a small offset preventing singularity. The cosine schedule concentrates more diffusion steps in the high-SNR regime, improving sample quality. Post-processing consists of three steps: 1. Gaussian smoothing: 1D Gaussian filter with 𝜎= 1.5, kernel size 7, removing high-frequency sampling artifacts while preserving acceleration peaks. 2. Conditional boundary ramps: If 𝑣0 > 0.5 or 𝑣𝑇> 0.5, apply linear ramps over 3 seconds to enforce zero endpoints. If already near zero, skip to avoid over-smoothing. 3. Vehicle-aware smoothing: For heavy vehicles (𝑑𝑣𝑒ℎ< 0.4), apply additional smoothing (kernel=9) to reflect slower dynamics. 4. Correlated Noise (Optional): To ensure heavy acceleration tails match real data, we allow adding temporally-correlated Gaussian noise after diffusion sampling. This noise is generated by smoothing white noise with a correlation length of 10s and scaling to a small amplitude (𝜎𝑐𝑜𝑟𝑟≈0.03), preventing the distribution from becoming “too safe” or narrow while maintaining temporal coherence. In our final model, physics training reduced the need for this step (default 𝜎𝑐𝑜𝑟𝑟= 0.0), but it remains an effective knob for fine-tuning variance. Generation of 1000 trajectories with CSDI requires approximately 3 minutes on A100, faster than the U-Net diffusion due to fewer model parameters and more eﬀicient transformer inference. D. Alternative Generative Approaches We also evaluated DoppelGANger (Lin et al. 2020), SDV’s PARSynthesizer (Patki, Wedge, and Veeramachaneni 2016), and Chronos (Ansari et al. 2025) as baselines. DoppelGANger exhibited mode collapse (87% highwayregime trajectories); SDV suffered from exposure bias causing temporal discontinuities; and Chronos lacked conditioning mechanisms for trajectory synthesis. Quantitative results for all baselines are reported in Table IV. V. Results We split the 6,367 micro-trips into 80% training (5,094 trips) and 20% test (1,273 trips) sets, stratified by cluster to ensure balanced representation of driving regimes. All models were trained on the training set and evaluated by generating 1,273 synthetic trajectories conditioned on the test set’s actual ( ̄ 𝑣, 𝑇) values. This protocol ensures fair comparison: each", "quality": {"lang": "en", "gibberish_score": 0.0097, "char_len": 2376, "word_len": 350}, "dedupe": {"exact_hash": "52ee9bf0437af131402ae82c031e30a62d191e080b990ddf8bd43b7d4c1e37b6", "simhash64": 5457617624534714134}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f32a1342ca723a76100c4ca199b160b0ec7b6b2f6bbda81bc8df169adaa7305b", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 17, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "(5,094 trips) and 20% test (1,273 trips) sets, stratified by cluster to ensure balanced representation of driving regimes. All models were trained on the training set and evaluated by generating 1,273 synthetic trajectories conditioned on the test set’s actual ( ̄ 𝑣, 𝑇) values. This protocol ensures fair comparison: each model attempts to recreate the test distribution given only aggregate conditioning information. All training and trajectory generation experiments were conducted on an NVIDIA A100 GPU (40GB) using CUDA 12.1 and PyTorch 2.0 with mixed-precision (FP16) computation. The Markov baseline was implemented with CPUonly NumPy operations. For evaluation, we employed a comprehensive framework spanning three dimensions. Distributional fidelity was assessed using Wasserstein distance (WD) for speed, acceleration, and Vehicle Specific Power (VSP) distributions, as well as 2D WD for the joint Speed-Acceleration Frequency Distribution (SAFD), Maximum Mean Discrepancy (MMD) with an RBF kernel (bandwidth 1.0), and the Kolmogorov-Smirnov statistic for VSP. VSP is a vehicle power demand metric (in kW/ton) that accounts for aerodynamic drag, rolling resistance, and road grade, commonly used for emissions and energy assessment. Kinematic validity was measured through the boundary violation rate (percentage of trips not starting or ending within 0.1 m/s of zero), Log Dimensionless Jerk (LDLJ) to quantify smoothness (where lower values indicate smoother trajectories), maximum speed, and acceleration standard deviation. Realism and utility were evaluated via a discriminative score (a Random Forest classifier trained to distinguish real from synthetic, where a score of 0.5 indicates perfect indistinguishability) and Train on Synthetic, Test on Real (TSTR) mean absolute error (MAE), where a predictive model trained on synthetic data is tested on real data to measure utility for downstream tasks. A. Main Results Table IV presents the comprehensive comparison across all models and metrics. Bold indicates best performance (closest to real or target value). CSDI achieves the best overall distribution matching, with WD Speed of 0.30—nearly 2x better than U-Net Diffusion’s 0.5622 and significantly better than the Markov baseline. The acceleration distribution match is exceptional (WD Accel = 0.026), reflecting the careful physicsinformed training. U-Net Diffusion also performs well, particularly excelling in the 2D SAFD metric", "quality": {"lang": "en", "gibberish_score": 0.0033, "char_len": 2454, "word_len": 350}, "dedupe": {"exact_hash": "17ac095656b60d933fb0a017c4e58655f223f309c1ba184fb7009f1b34997828", "simhash64": 15707512408535565318}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:5c56a91e1ca15452967fccb19baa82196982c65fc3505d195f3ad768fcdd9d4e", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 18, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "the best overall distribution matching, with WD Speed of 0.30—nearly 2x better than U-Net Diffusion’s 0.5622 and significantly better than the Markov baseline. The acceleration distribution match is exceptional (WD Accel = 0.026), reflecting the careful physicsinformed training. U-Net Diffusion also performs well, particularly excelling in the 2D SAFD metric (0.0005), indicating accurate capture of the joint speed-acceleration manifold. The Markov baseline achieves respectable WD Speed (1.82) but struggles with acceleration (0.145), confirming that discretization artifacts and memoryless transitions fail to capture kinematic smoothness. Chronos, despite 0 5 10 15 20 25 30 35 Speed (m/s) 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 Density CSDI Speed Real CSDI 0 5 10 15 20 25 30 35 Speed (m/s) Diffusion Speed Real Diffusion 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Accel (m/s²) CSDI Accel Real CSDI 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Accel (m/s²) Diffusion Accel Real Diffusion (a) Speed/Acceleration Distributions (CSDI vs Diffusion) 0 200 400 Time (s) 0 10 20 30 Speed (m/s) CSDI Highway 0 200 400 Time (s) 0 10 20 30 CSDI Arterial 0 100 200 300 400 Time (s) 0 10 20 30 CSDI Congested (b) CSDI Trajectories (Highway, Arterial, Congested) 0 200 400 Time (s) 0 10 20 30 Speed (m/s) Diff Highway 0 200 400 Time (s) 0 10 20 30 Diff Arterial 0 100 200 300 Time (s) 0 10 20 30 Diff Congested (c) Diffusion Trajectories (Highway, Arterial, Congested) 0 10 20 30 Speed (m/s) 4 2 0 2 4 Accel (m/s²) Real SAFD 0 10 20 30 Speed (m/s) CSDI SAFD 0 10 20 30 Speed (m/s) Diffusion SAFD (d) SAFD Heatmaps (Real, CSDI, Diffusion) Figure 6: Main results comprehensive comparison. Row 1: (a-b) Speed and acceleration distributions showing CSDI’s superior matching. Row 2: (c-e) CSDI sample trajectories across regimes. Row 3: (f-h) Diffusion sample trajectories. Row 4: (i-k) Speed-Acceleration Frequency Distributions (SAFD) showing Real, CSDI, and Diffusion joint distributions. its foundation model pretraining, performs worse than Markov (WD Speed 2.15), suggesting zero-shot transfer without domain-specific fine-tuning is insuﬀicient. DoppelGANger and SDV fail", "quality": {"lang": "en", "gibberish_score": 0.007, "char_len": 2156, "word_len": 350}, "dedupe": {"exact_hash": "98c7916499eedaa4e092441d5d2da9c430c4adf8ba83bd7a86c4a8e9832e0e01", "simhash64": 4953215566342900074}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:5f25bc0ff3a5a28bd48584e782eb55b0f14bba816b352f7ec337d31917245064", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 19, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "2: (c-e) CSDI sample trajectories across regimes. Row 3: (f-h) Diffusion sample trajectories. Row 4: (i-k) Speed-Acceleration Frequency Distributions (SAFD) showing Real, CSDI, and Diffusion joint distributions. its foundation model pretraining, performs worse than Markov (WD Speed 2.15), suggesting zero-shot transfer without domain-specific fine-tuning is insuﬀicient. DoppelGANger and SDV fail dramatically (WD Speed > 2.8), with DoppelGANger’s mode collapse producing unrealistic highway-heavy distributions. All physics-aware models (U-Net Diffusion, CSDI, Markov) achieve perfect boundary condition satisfaction through their respective enforcement mechanisms (inpainting, postprocessing, bridge sampling). In contrast, autoregressive models (Chronos 23.4%, SDV 23.0%) and adversarial methods (DoppelGANger 12.1%) struggle with hard constraints. CSDI produces the smoothest trajectories (LDLJ = - 3.85, closest to real -3.92), validating the effectiveness of its temporal smoothness and jerk penalties. U-Net Table IV: Comparative Performance of Generative Models Metric Real U-Net Diffusion CSDI Markov Chain Chronos DoppelGANger SDV Distributional Fidelity WD Speed - 0.5622 0.30 1.82 2.15 3.42 2.87 WD Accel - 0.0800 0.026 0.145 0.198 0.312 0.421 WD VSP - 1.5517 1.89 2.34 3.12 4.87 3.65 WD SAFD (2D) - 0.0005 0.0008 0.0023 - - - MMD (×10−3) - <0.1 0.12 2.34 3.87 8.92 6.15 KS VSP - 0.0631 0.072 0.123 0.187 0.298 0.245 Kinematic Validity Boundary Violations 0% 0% 0% 0% 23.4% 12.1% 23.0% LDLJ (Smoothness) -3.92 -3.50 -3.85 -0.15 -2.12 -4.10 -1.15 Max Speed (m/s) 31.6 32.1 31.8 34.5 32.5 31.9 45.2 Accel Std. (m/s2) 0.51 0.53 0.49 2.10 0.85 0.35 4.12 Utility Discrim. Score (0.5 is ideal) 0.50 0.62 0.49 0.85 0.78 0.51 0.99 TSTR MAE (km/h) - 4.2 2.1 8.5 5.4 12.3 15.2 Diffusion’s LDLJ (-3.50) indicates comparable smoothness, with acceleration variance remaining accurate (0.53 vs 0.51 real). The Markov baseline produces less smooth trajectories (LDLJ = -0.15) due to discretization, while GAN/autoregressive methods exhibit varied smoothness characteristics. Maximum speeds in the table show that all physics-aware models (U-Net Diffusion 32.1 m/s, CSDI 31.8 m/s) remain close to the real data maximum (31.6 m/s). SDV produces excessive speeds (45.2 m/s), indicating inadequate constraint enforcement. CSDI offers the best training eﬀiciency among deep models (12 hours vs 21.5 for U-Net Diffusion), benefiting", "quality": {"lang": "en", "gibberish_score": 0.0107, "char_len": 2424, "word_len": 350}, "dedupe": {"exact_hash": "66a7ae4ef8750a7d4e3e25ff560d70fa95fb2688271f3e8d8bbd9e335713e529", "simhash64": 4953215566275759360}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c7ec3e41fc9a1a38de0e1d896cfc71636efecec6c485d2f4325af8790d5016da", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 20, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "the table show that all physics-aware models (U-Net Diffusion 32.1 m/s, CSDI 31.8 m/s) remain close to the real data maximum (31.6 m/s). SDV produces excessive speeds (45.2 m/s), indicating inadequate constraint enforcement. CSDI offers the best training eﬀiciency among deep models (12 hours vs 21.5 for U-Net Diffusion), benefiting from the transformer’s global receptive field requiring fewer training epochs. Generation is also faster (3 min vs 5 min), important for large-scale simulations. The Markov baseline remains superior for pure compute eﬀiciency, suitable when interpretability outweighs distribution fidelity. Chronos’s inference is slow despite no training cost, as the large 60M parameter model generates autoregressively. The discriminative score quantifies how diﬀicult it is to distinguish synthetic from real trajectories. CSDI achieves 0.49, demonstrating high indistinguishability (target is 0.5). U-Net Diffusion scores 0.62, indicating synthetic samples are somewhat easier to identify but still highly realistic. The Markov baseline (0.85) produces trajectories with more detectable artifacts, while DoppelGANger (0.51) achieves good indistinguishability despite other quality issues. TSTR (Train on Synthetic, Test on Real) measures downstream utility: can models trained on synthetic data perform well on real data? CSDI achieves the lowest MAE (2.1 km/h), with U-Net Diffusion close behind (4.2 km/h), demonstrating that synthetic trajectories preserve the statistical properties needed for predictive modeling. This validates their use for energy assessment and simulation tasks. We evaluated conditional control by generating trajectories at different target average speeds while fixing duration. Figure 6 shows that both CSDI and Diffusion accurately match target speeds (mean absolute error < 0.5 m/s) across the range 10-25 m/s. CSDI’s vehicle dynamics conditioning enables additional control: generating with 𝑑𝑣𝑒ℎ = 0.2 (heavy truck) versus 𝑑𝑣𝑒ℎ = 0.8 (sports car) produces distinct acceleration profiles, with truck trajectories exhibiting slower accelerations (mean 1.2 m/s² vs 3.1 m/s²) and smoother dynamics (LDLJ -12.3 vs -10.1). Finally, in Table V we compare training and generation computational requirements. VI. Discussion This work demonstrates that diffusion models— particularly the transformer-based CSDI architecture— represent a suitable method for intelligent transportation system applications requiring synthetic driving data. Through systematic comparison against traditional baselines and modern methods like DoppelGANger and", "quality": {"lang": "en", "gibberish_score": 0.0074, "char_len": 2581, "word_len": 350}, "dedupe": {"exact_hash": "61e708ba735e0391ca4acc1e8dad62622aeb5d3f7b60c7bb870489204be152e9", "simhash64": 5097258186651290886}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:075b17800b48c06a85bd770734901a82d73a5c837e083e9a8bb6bb775e8ad596", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 21, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "-10.1). Finally, in Table V we compare training and generation computational requirements. VI. Discussion This work demonstrates that diffusion models— particularly the transformer-based CSDI architecture— represent a suitable method for intelligent transportation system applications requiring synthetic driving data. Through systematic comparison against traditional baselines and modern methods like DoppelGANger and SDV, we established that CSDI with physics-informed training achieves strong performance: Wasserstein distance metrics 2–6× better than baselines, strong boundary condition satisfaction, and accurate smoothness matching real driving dynamics. CSDI’s success is driven by its ability to generate entire sequences jointly, avoiding autoregressive error accumulation, and its use of transformer self-attention to capture long-range kinematic dependencies. Furthermore, our investigation documented critical lessons for practitioners. Physics constraints must be integrated through soft, threshold-activated penalties rather than hard enforcement to avoid optimization conflicts, and transformer architectures’ global receptive fields better accommodate long-range kinematic dependencies than local convolutions. We also observed that adversarial train- Table V: Computational Eﬀiciency Model Parameters Training Time Generation Time GPU Memory Hardware U-Net Diffusion 8.2M 21.5 hrs approx. 5 min 12 GB A100 CSDI 5.5M 12 hrs approx. 3 min 8 GB A100 Markov - <5 min (CPU) <1 min (CPU) - CPU Chronos 60M (frozen) 0 (pretrained) approx. 10 min 16 GB A100 DoppelGANger 12M 8 hrs approx. 8 min 10 GB A100 SDV 15M 4 hrs approx. 2 min 6 GB A100 ing and autoregressive factorization are architecturally ill-suited for high-fidelity, physics-constrained synthesis, often leading to mode collapse or temporal discontinuities. Beyond providing validated tools for energy assessment and traﬀic simulation, the released implementation and trained models establish diffusion-based synthesis as a robust framework for intelligent transportation system applications. Several constraints limit this work. The 512-second trajectory limit excludes longer journeys, requiring slidingwindow generation for highway commutes. The models were trained primarily on passenger vehicles and need validation against truck and bus datasets. Road-type conditioning is implicit rather than explicit. Finally, while the 2007 CMAP dataset is dated, the model captures microscopic kinematics—acceleration capabilities and carfollowing dynamics—governed by invariant vehicle physics rather than evolving traﬀic patterns. TSTR results confirm these physical principles transfer to downstream tasks.", "quality": {"lang": "en", "gibberish_score": 0.0037, "char_len": 2678, "word_len": 350}, "dedupe": {"exact_hash": "cd3d2f0b02900404c20f57d3037ad5303e49573d955db4f3dd6c73c952e6e425", "simhash64": 769088937171158102}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2391f129e51a54d05faafab30cc913cd164bc05a30a9639de241740f535fd9c4", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 22, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "need validation against truck and bus datasets. Road-type conditioning is implicit rather than explicit. Finally, while the 2007 CMAP dataset is dated, the model captures microscopic kinematics—acceleration capabilities and carfollowing dynamics—governed by invariant vehicle physics rather than evolving traﬀic patterns. TSTR results confirm these physical principles transfer to downstream tasks. Several extensions would enhance the models’ capabilities and eﬀiciency. First, implementing faster sampling techniques such as Denoising Diffusion Implicit Models (DDIM) (J. Song, Meng, and Ermon 2021) could reduce generation steps from 1000 to 50–100, achieving the 10– 20× speedups necessary for real-time applications. Second, introducing multi-modal road conditioning (e.g., highway, arterial, local) would enable explicit control over driving regimes, supporting the generation of realistic multisegment trips. Third, extending the framework to multiagent scenarios with interaction modeling would allow for traﬀic simulation applications, building on recent advances in diffusion for autonomous driving (Feng et al. 2023) and multi-scale generative transformers for ITS networks (Adam et al. 2025). Fourth, fine-tuning foundation models such as Chronos on vehicle trajectory data—with modifications for boundary constraints—could leverage large-scale pretraining to reduce training time. Finally, incorporating road elevation and grade profiles as additional conditioning would significantly improve the accuracy of energy consumption modeling, as grade is a primary driver of vehicle power demand. Together, these directions advance toward comprehensive synthetic traﬀic generation capable of supporting the full spectrum of intelligent transportation system evaluation and design tasks. VII. Data and Code Availability All code, trained model weights (U-Net Diffusion, CSDI), and evaluation scripts are available at https://github.c om/VadimSokolov/diffusion-trajectory-generation. The CMAP 2007 dataset is publicly available through NREL’s Transportation Secure Data Center (TSDC). Adam, Abuzar B. M., Tahir Kamal, Mohammed A. M. Elhassan, Abdullah Alshahrani, Saeed Hamood Alsamhi, and Ahmed Aziz. 2025. “Multi-Scale Generative Transformer-Based Primal-Dual PPO Framework for AAV-aided Intelligent Transportation Networks.” IEEE Transactions on Intelligent Transportation Systems, 1–16. Alcaraz, Juan Miguel Lopez, and Nils Strodthoff. 2022. “Diffusion-Based Time Series Imputation and Forecasting with Structured State Space Models.” arXiv Preprint arXiv:2208.09399. https://arxiv.org/abs/22 08.09399. Altché, Florent, and Arnaud de La Fortelle. 2017. “An LSTM Network for Highway Trajectory Prediction.” In 2017 IEEE 20th International Conference on Intelligent", "quality": {"lang": "en", "gibberish_score": 0.0087, "char_len": 2773, "word_len": 350}, "dedupe": {"exact_hash": "6686186d3aefb6b8189975095c5fafa3c1b623ba4e6e44e2c3dff280506a31ca", "simhash64": 11524181757901472518}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:05db6e7f1279ecef236acce0b176b9033ae44390c90d552adaea806c83233e23", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 23, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "Transportation Systems, 1–16. Alcaraz, Juan Miguel Lopez, and Nils Strodthoff. 2022. “Diffusion-Based Time Series Imputation and Forecasting with Structured State Space Models.” arXiv Preprint arXiv:2208.09399. https://arxiv.org/abs/22 08.09399. Altché, Florent, and Arnaud de La Fortelle. 2017. “An LSTM Network for Highway Trajectory Prediction.” In 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC), 353–59. IEEE. Ansari, Abdul Fatir, Oleksandr Shchur, Jaris Küken, Andreas Auer, Boran Han, Pedro Mercado, Syama Sundar Rangapuram, et al. 2025. “Chronos-2: From Univariate to Universal Forecasting.” arXiv. https://arxiv.org/ab s/2510.15821. Auld, Joshua, Michael Hope, Hubert Ley, Vadim Sokolov, Bo Xu, and Kuilin Zhang. 2016. “POLARIS: Agentbased Modeling Framework Development and Implementation for Integrated Travel Demand and Network and Operations Simulations.” Transportation Research Part C: Emerging Technologies 64: 101–16. Auld, Joshua, Michael Hope, Hubert Ley, Bo Xu, Kuilin Zhang, and Vadim Sokolov. 2013. “Modelling Framework for Regional Integrated Simulation of Transportation Network and Activity-Based Demand (Polaris).” In International Symposium for Next Generation Infrastructure. Auld, Joshua, Dominik Karbowski, Vadim Sokolov, and Namwook Kim. 2016. “A Disaggregate Model System for Assessing the Energy Impact of Transportation at the Regional Level.” In Transportation Research Board 95th Annual Meeting. Behnia, Farnaz, Dominik Karbowski, and Vadim Sokolov. 2023. “Deep Generative Models for Vehicle Speed Tra- jectories.” Applied Stochastic Models in Business and Industry 39 (5): 701–19. Chen, Jun, Michal Weiszer, Paul Stewart, and Masihalah Shabani. 2016. “Toward a More Realistic, CostEffective, and Greener Ground Movement Through Active Routing—Part I: Optimal Speed Profile Generation.” IEEE Transactions on Intelligent Transportation Systems 17 (5): 1196–1209. Chicago Metropolitan Agency for Planning. 2008. “CMAP Regional Household Travel Inventory.” Transportation Secure Data Center, National Renewable Energy Laboratory. Dauner, Daniel, Marcel Hallgarten, Andreas Geiger, and Kashyap Chitta. 2023. “Parting with Misconceptions about Learning-based Vehicle Motion Planning.” arXiv. https://arxiv.org/abs/2306.07962. Durham, Garland B, and A Ronald Gallant. 2002. “Numerical Techniques for Maximum Likelihood Estimation of Continuous-Time Diffusion Processes.” Journal of Business & Economic Statistics 20 (3): 297–338. Feng, Lan, Quanyi Li, Zhenghao Peng, Shuhan Tan, and Bolei Zhou. 2023. “TraﬀicGen: Learning to Generate Diverse and Realistic Traﬀic Scenarios.” In 2023 IEEE International Conference on Robotics and Automation (ICRA), 3567–75. IEEE. Gillespie, Thomas", "quality": {"lang": "en", "gibberish_score": 0.0164, "char_len": 2747, "word_len": 350}, "dedupe": {"exact_hash": "6c5c8bb26b9614f5d856a7f39e6edf1b8d5272da4d52b90d0ff8da4fab892225", "simhash64": 11199355305451521748}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:20e5c06fb1e60a9b2c4b95f5ade391ee877a738e6b7ee35eddd8afb55e8ea84a", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 24, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "Likelihood Estimation of Continuous-Time Diffusion Processes.” Journal of Business & Economic Statistics 20 (3): 297–338. Feng, Lan, Quanyi Li, Zhenghao Peng, Shuhan Tan, and Bolei Zhou. 2023. “TraﬀicGen: Learning to Generate Diverse and Realistic Traﬀic Scenarios.” In 2023 IEEE International Conference on Robotics and Automation (ICRA), 3567–75. IEEE. Gillespie, Thomas D. 1992. Fundamentals of Vehicle Dynamics. Warrendale, PA: Society of Automotive Engineers. Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. “Denoising Diffusion Probabilistic Models.” In Advances in Neural Information Processing Systems, 33:6840–51. Ho, Jonathan, and Tim Salimans. 2022. “Classifier-Free Diffusion Guidance.” arXiv Preprint arXiv:2207.12598. https://arxiv.org/abs/2207.12598. Huang, Xianan, Boqi Li, Huei Peng, Joshua A Auld, and Vadim O Sokolov. 2020. “Eco-Mobility-on-Demand Fleet Control with Ride-Sharing.” IEEE Transactions on Intelligent Transportation Systems 23 (4): 3158–68. Huang, Yanjun, Jian Chen, Chang Huang, Xinggang Wang, Wenyu Liu, and Jianqiang Huang. 2022. “A Survey on Trajectory-Prediction Methods for Autonomous Driving.” IEEE Transactions on Intelligent Transportation Systems. Karbowski, Dominik, Namwook Kim, Joshua Auld, and Vadim Sokolov. 2016. “Assessing the Energy Impact of Traﬀic Management and Vehicle Hybridisation.” International Journal of Complexity in Applied Science and Technology 1 (1): 107–24. Karbowski, Dominik, Aymeric Rousseau, Vivien SmisMichel, and Valentin Vermeulen. n.d. “Trip Prediction Using GIS for Vehicle Energy Eﬀiciency.” In 21st World Congress on Intelligent Transportation Systems (Detroit, MI, 09/07/2014 - 09/11/2014), –,. Karbowski, Dominik, Vadim Sokolov, and Jeong Jongryeol. 2016. “Fuel Saving Potential of Optimal Route-Based Control for Plug-in Hybrid Electric Vehicle.” IFACPapersOnLine 49 (11): 128–33. Karbowski, Dominik, Vadim Sokolov, and Aymeric Rousseau. 2015. “Vehicle Energy Management Optimization Through Digital Maps and Connectivity.” Argonne National Lab.(ANL), Argonne, IL (United States). Lan, Wenxing, Jialin Liu, Bo Yuan, and Xin Yao. 2025. “Controllable Multimodal Motion Behavior Generation for Autonomous Driving.” IEEE Transactions on Intelligent Transportation Systems, 1–16. Li, Quanyi, Zhenghao Peng, Lan Feng, Zhizheng Liu, Chenda Duan, Wenjie Mo, and Bolei Zhou. 2023. “ScenarioNet: Open-Source Platform for Large-Scale Traffic Scenario Simulation and Modeling.” arXiv. https: //arxiv.org/abs/2306.12241. Liao, Bencheng, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. 2024. “DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving.” arXiv Preprint arXiv:2406.07806. https://arxiv.org/abs/24 06.07806. Lin, Zinan, Alankar Jain,", "quality": {"lang": "en", "gibberish_score": 0.0181, "char_len": 2769, "word_len": 350}, "dedupe": {"exact_hash": "bafeac854ad2302921a340ebd45b61494020496cf16c8f9e9feb9a1b1d8ac8a8", "simhash64": 11235952549998200532}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c243c7ec54bf54886a900ff910fe1583cc1f9fd667fa9fda958a2bef87ed9cde", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 25, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "and Bolei Zhou. 2023. “ScenarioNet: Open-Source Platform for Large-Scale Traffic Scenario Simulation and Modeling.” arXiv. https: //arxiv.org/abs/2306.12241. Liao, Bencheng, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. 2024. “DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving.” arXiv Preprint arXiv:2406.07806. https://arxiv.org/abs/24 06.07806. Lin, Zinan, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. 2020. “Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions.” In Proceedings of the ACM Internet Measurement Conference (IMC), 464–83. ACM. Moawad, Ayman, Zhijian Li, Ines Pancorbo, Krishna Murthy Gurumurthy, Vincent Freyermuth, Ehsan Islam, Ram Vijayagopal, Monique Stinson, and Aymeric Rousseau. 2021. “A Real-Time Energy and Cost Eﬀicient Vehicle Route Assignment Neural Recommender System.” arXiv. https://arxiv.org/abs/2110.10887. Mozaffari, Sajjad, Omar Y Al-Jarrah, Alexandros Mouzakitis, Phil Jennings, and Stratis Kanarachos. 2020. “Deep Learning-Based Vehicle Behaviour Prediction for Autonomous Driving Applications: A Review.” IEEE Transactions on Intelligent Transportation Systems. Nichol, Alexander Quinn, and Prafulla Dhariwal. 2021. “Improved Denoising Diffusion Probabilistic Models.” In International Conference on Machine Learning, 8162–71. PMLR. Papamakarios, George, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. 2019. “Normalizing Flows for Probabilistic Modeling and Inference.” arXiv:1912.02762 [Cs, Stat], December. https://arxiv.org/abs/1912.02762. Patki, Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016. “The Synthetic Data Vault.” In 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA), 399–410. IEEE. Perez, Ethan, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. 2018. “FiLM: Visual Reasoning with a General Conditioning Layer.” In Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. 1. Qian, Yu, Xunhao Li, Jian ... Zhang, and Maoze Wang. 2025. “A Diffusion-TGAN Framework for SpatioTemporal Speed Imputation and Trajectory Reconstruction.” IEEE Transactions on Intelligent Transportation Systems 26 (11): 18948–62. Rasul, Kashif, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. “Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting.” In International Conference on Machine Learning (ICML), 8857–68. PMLR. Rong, Yi, Yingchi Mao, Yinqiu ... Liu, and Dusit Niyato. 2025. “ICST-DNET: An Interpretable Causal SpatioTemporal Diffusion Network for Traﬀic Speed Prediction.” IEEE Transactions on Intelligent Transportation Systems 26 (7): 9781–98. Schultz, Laura, and Vadim Sokolov. 2018. “Deep Reinforcement", "quality": {"lang": "en", "gibberish_score": 0.0167, "char_len": 2821, "word_len": 350}, "dedupe": {"exact_hash": "f4bed86d224ff9a5e2049c1f865e74a79852884978017176019ad2460238ec18", "simhash64": 11091273312457262550}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:292ec9dfc81c0543ec0bc80ec5c735725f0e482d9800bb4ed1a44e31f0021fca", "doc_id": "sha256:7a4b944d16b32d3696fc180735395c3b3bf55a984670f15f9d1e5ca54cb321f7", "chunk_index": 26, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Physics-Informed Diffusion Models for Vehicle Speed Trajectory Generation.pdf", "page_start": null, "page_end": null, "text": "Series Forecasting.” In International Conference on Machine Learning (ICML), 8857–68. PMLR. Rong, Yi, Yingchi Mao, Yinqiu ... Liu, and Dusit Niyato. 2025. “ICST-DNET: An Interpretable Causal SpatioTemporal Diffusion Network for Traﬀic Speed Prediction.” IEEE Transactions on Intelligent Transportation Systems 26 (7): 9781–98. Schultz, Laura, and Vadim Sokolov. 2018. “Deep Reinforcement Learning for Dynamic Urban Transportation Problems.” arXiv Preprint arXiv:1806.05310. https: //arxiv.org/abs/1806.05310. Sokolov, Vadim, Joshua Auld, and Michael Hope. 2012. “A Flexible Framework for Developing Integrated Models of Transportation Systems Using an Agent-Based Approach.” Procedia Computer Science 10: 854–59. Song, Jiaming, Chenlin Meng, and Stefano Ermon. 2021. “Denoising Diffusion Implicit Models.” In International Conference on Learning Representations. Song, Yang, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020. “Score-Based Generative Modeling Through Stochastic Differential Equations.” arXiv Preprint arXiv:2011.13456. https://arxiv.org/abs/2011.13456. Suo, Simon, Sebastian Regalado, Sergio Casas, and Raquel Urtasun. 2021. “TraﬀicSim: Learning to Simulate Realistic Multi-Agent Behaviors.” arXiv. https://arxiv.or g/abs/2101.06557. Tashiro, Yusuke, Jiaming Song, Yang Song, and Stefano Ermon. 2021. “CSDI: Conditional Score-Based Diffusion Models for Probabilistic Time Series Imputation.” In Advances in Neural Information Processing Systems, 34:24804–16. Wu, Jian, Carol Flannagan, Ulrich Sander, and Jonas Bärgman. 2025. “Model-Based Generation of Representative Rear-End Crash Scenarios Across the Full Severity Range Using Pre-Crash Data.” IEEE Transactions on Intelligent Transportation Systems 26 (10): 15932– 50. Yoon, Jinsung, Daniel Jarrett, and Mihaela van der Schaar. 2019. “Time-Series Generative Adversarial Networks.” In Advances in Neural Information Processing Systems 32, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\textquotesingle Alché-Buc, E. Fox, and R. Garnett, 5508–18. Curran Associates, Inc. Zhao, Tianyang, Yifei Xu, Mathew Monfort, Wongun Choi, Chris Baker, Yibiao Zhao, Yizhou Wang, and Ying Nian Wu. 2019. “Multi-Agent Tensor Fusion for Contextual Trajectory Prediction.” In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 5559–68.", "quality": {"lang": "en", "gibberish_score": 0.0176, "char_len": 2380, "word_len": 297}, "dedupe": {"exact_hash": "fdb510e9ad45f0a04a54d36bbfcc7ad9e293cce43c8a63019b8970ffd2289de8", "simhash64": 11235530337533134416}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:782a098f9beb3d6c014e214cac6273f2180778ba06222901f306b1793de29a21", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 0, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "116 STAT. 2899 Public Law 107–347 107th Congress An Act To enhance the management and promotion of electronic Government services and processes by establishing a Federal Chief Information Officer within the Office of Management and Budget, and by establishing a broad framework of measures that require using Internet-based information technology to enhance citizen access to Government information and services, and for other purposes. Be it enacted by the Senate and House of Representatives of the United States of America in Congress assembled, SECTION 1. SHORT TITLE; TABLE OF CONTENTS. (a) SHORT TITLE.—This Act may be cited as the ‘‘E-Government Act of 2002’’. (b) TABLE OF CONTENTS.—The table of contents for this Act is as follows: Sec. 1. Short title; table of contents. Sec. 2. Findings and purposes. TITLE I—OFFICE OF MANAGEMENT AND BUDGET ELECTRONIC GOVERNMENT SERVICES Sec. 101. Management and promotion of electronic government services. Sec. 102. Conforming amendments. TITLE II—FEDERAL MANAGEMENT AND PROMOTION OF ELECTRONIC GOVERNMENT SERVICES Sec. 201. Definitions. Sec. 202. Federal agency responsibilities. Sec. 203. Compatibility of executive agency methods for use and acceptance of electronic signatures. Sec. 204. Federal Internet portal. Sec. 205. Federal courts. Sec. 206. Regulatory agencies. Sec. 207. Accessibility, usability, and preservation of government information. Sec. 208. Privacy provisions. Sec. 209. Federal information technology workforce development. Sec. 210. Share-in-savings initiatives. Sec. 211. Authorization for acquisition of information technology by State and local governments through Federal supply schedules. Sec. 212. Integrated reporting study and pilot projects. Sec. 213. Community technology centers. Sec. 214. Enhancing crisis management through advanced information technology. Sec. 215. Disparities in access to the Internet. Sec. 216. Common protocols for geographic information systems. TITLE III—INFORMATION SECURITY Sec. 301. Information security. Sec. 302. Management of information technology. Sec. 303. National Institute of Standards and Technology. Sec. 304. Information Security and Privacy Advisory Board. Sec. 305. Technical and conforming amendments. TITLE IV—AUTHORIZATION OF APPROPRIATIONS AND EFFECTIVE DATES Sec. 401. Authorization of appropriations. 44 USC 101 note. E-Government Act of 2002. Dec. 17, 2002 [H.R. 2458] VerDate 11-MAY-2000 22:27 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00001 Fmt 6580 Sfmt 6582 E:\\PUBLAW\\PUBL347.107 116", "quality": {"lang": "en", "gibberish_score": 0.0052, "char_len": 2515, "word_len": 350}, "dedupe": {"exact_hash": "a839ad7615cd684c3f604b81d40939aaaf72f3446901e33f15174cb180982b60", "simhash64": 11192394297293406212}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:97bb353d651bea733478ed496a00f3300d66346f52e32820281dc8e87d4f0302", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 1, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "Board. Sec. 305. Technical and conforming amendments. TITLE IV—AUTHORIZATION OF APPROPRIATIONS AND EFFECTIVE DATES Sec. 401. Authorization of appropriations. 44 USC 101 note. E-Government Act of 2002. Dec. 17, 2002 [H.R. 2458] VerDate 11-MAY-2000 22:27 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00001 Fmt 6580 Sfmt 6582 E:\\PUBLAW\\PUBL347.107 116 STAT. 2900 Sec. 402. Effective dates. TITLE V—CONFIDENTIAL INFORMATION PROTECTION AND STATISTICAL EFFICIENCY Sec. 501. Short title. Sec. 502. Definitions. Sec. 503. Coordination and oversight of policies. Sec. 504. Effect on other laws. Subtitle A—Confidential Information Protection Sec. 511. Findings and purposes. Sec. 512. Limitations on use and disclosure of data and information. Sec. 513. Fines and penalties. Subtitle B—Statistical Efficiency Sec. 521. Findings and purposes. Sec. 522. Designation of statistical agencies. Sec. 523. Responsibilities of designated statistical agencies. Sec. 524. Sharing of business data among designated statistical agencies. Sec. 525. Limitations on use of business data provided by designated statistical agencies. Sec. 526. Conforming amendments. SEC. 2. FINDINGS AND PURPOSES. (a) FINDINGS.—Congress finds the following: (1) The use of computers and the Internet is rapidly transforming societal interactions and the relationships among citizens, private businesses, and the Government. (2) The Federal Government has had uneven success in applying advances in information technology to enhance governmental functions and services, achieve more efficient performance, increase access to Government information, and increase citizen participation in Government. (3) Most Internet-based services of the Federal Government are developed and presented separately, according to the jurisdictional boundaries of an individual department or agency, rather than being integrated cooperatively according to function or topic. (4) Internet-based Government services involving interagency cooperation are especially difficult to develop and promote, in part because of a lack of sufficient funding mechanisms to support such interagency cooperation. (5) Electronic Government has its impact through improved Government performance and outcomes within and across agencies. (6) Electronic Government is a critical element in the management of Government, to be implemented as part of a management framework that also addresses finance, procurement, human capital, and other challenges to improve the performance of Government. (7) To take full advantage of the improved Government performance", "quality": {"lang": "en", "gibberish_score": 0.0027, "char_len": 2566, "word_len": 350}, "dedupe": {"exact_hash": "ad671908c9142584657ba9fe49789dffcf94f32b14133bb693137635ad1b6d16", "simhash64": 14667748242554177028}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:120072499d7c6f4f124cd8e7c9f797969e27cde3b059dc3a4d982651a005fbee", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 2, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "and across agencies. (6) Electronic Government is a critical element in the management of Government, to be implemented as part of a management framework that also addresses finance, procurement, human capital, and other challenges to improve the performance of Government. (7) To take full advantage of the improved Government performance that can be achieved through the use of Internetbased technology requires strong leadership, better organization, improved interagency collaboration, and more focused oversight of agency compliance with statutes related to information resource management. (b) PURPOSES.—The purposes of this Act are the following: (1) To provide effective leadership of Federal Government efforts to develop and promote electronic Government services and processes by establishing an Administrator of a new Office of Electronic Government within the Office of Management and Budget. 44 USC 3601 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00002 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2901 (2) To promote use of the Internet and other information technologies to provide increased opportunities for citizen participation in Government. (3) To promote interagency collaboration in providing electronic Government services, where this collaboration would improve the service to citizens by integrating related functions, and in the use of internal electronic Government processes, where this collaboration would improve the efficiency and effectiveness of the processes. (4) To improve the ability of the Government to achieve agency missions and program performance goals. (5) To promote the use of the Internet and emerging technologies within and across Government agencies to provide citizen-centric Government information and services. (6) To reduce costs and burdens for businesses and other Government entities. (7) To promote better informed decisionmaking by policy makers. (8) To promote access to high quality Government information and services across multiple channels. (9) To make the Federal Government more transparent and accountable. (10) To transform agency operations by utilizing, where appropriate, best practices from public and private sector organizations. (11) To provide enhanced access to Government information and services in a manner consistent with laws regarding protection of personal privacy, national security, records retention, access for persons with disabilities, and other relevant laws.", "quality": {"lang": "en", "gibberish_score": 0.0012, "char_len": 2485, "word_len": 350}, "dedupe": {"exact_hash": "87c9ddc601171472e4af458c6f3a1d4c0b3ef69854f4fb59acf67bfaaa8cdafd", "simhash64": 11239115844925867718}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d4629178b1060c45131073af47e3c77d67c9443eae5d5ca22f157b82a172340c", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 3, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "To transform agency operations by utilizing, where appropriate, best practices from public and private sector organizations. (11) To provide enhanced access to Government information and services in a manner consistent with laws regarding protection of personal privacy, national security, records retention, access for persons with disabilities, and other relevant laws. TITLE I—OFFICE OF MANAGEMENT AND BUDGET ELECTRONIC GOVERNMENT SERVICES SEC. 101. MANAGEMENT AND PROMOTION OF ELECTRONIC GOVERNMENT SERVICES. (a) IN GENERAL.—Title 44, United States Code, is amended by inserting after chapter 35 the following: ‘‘CHAPTER 36—MANAGEMENT AND PROMOTION OF ELECTRONIC GOVERNMENT SERVICES ‘‘Sec. ‘‘3601. Definitions. ‘‘3602. Office of Electronic Government. ‘‘3603. Chief Information Officers Council. ‘‘3604. E-Government Fund. ‘‘3605. Program to encourage innovative solutions to enhance electronic Government services and processes. ‘‘3606. E-Government report. ‘‘§ 3601. Definitions ‘‘In this chapter, the definitions under section 3502 shall apply, and the term— ‘‘(1) ‘Administrator’ means the Administrator of the Office of Electronic Government established under section 3602; VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00003 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2902 ‘‘(2) ‘Council’ means the Chief Information Officers Council established under section 3603; ‘‘(3) ‘electronic Government’ means the use by the Government of web-based Internet applications and other information technologies, combined with processes that implement these technologies, to— ‘‘(A) enhance the access to and delivery of Government information and services to the public, other agencies, and other Government entities; or ‘‘(B) bring about improvements in Government operations that may include effectiveness, efficiency, service quality, or transformation; ‘‘(4) ‘enterprise architecture’— ‘‘(A) means— ‘‘(i) a strategic information asset base, which defines the mission; ‘‘(ii) the information necessary to perform the mission; ‘‘(iii) the technologies necessary to perform the mission; and ‘‘(iv) the transitional processes for implementing new technologies in response to changing mission needs; and ‘‘(B) includes— ‘‘(i) a baseline architecture; ‘‘(ii) a target architecture; and ‘‘(iii) a sequencing plan; ‘‘(5) ‘Fund’ means the E-Government Fund established under section 3604; ‘‘(6) ‘interoperability’ means the ability of different operating and software systems, applications, and services to communicate and exchange data in an accurate, effective, and consistent manner; ‘‘(7) ‘integrated", "quality": {"lang": "en", "gibberish_score": 0.0305, "char_len": 2620, "word_len": 350}, "dedupe": {"exact_hash": "25e25a67e04bbae1ccdfaa6df519fd9943df374e8b9bfbcc9979aabcf1cd73e3", "simhash64": 11503697924954112022}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:52657b04f770771dbdc16a1063f13b0e457a801759e5768ce743e35e5c42e89d", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 4, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "a baseline architecture; ‘‘(ii) a target architecture; and ‘‘(iii) a sequencing plan; ‘‘(5) ‘Fund’ means the E-Government Fund established under section 3604; ‘‘(6) ‘interoperability’ means the ability of different operating and software systems, applications, and services to communicate and exchange data in an accurate, effective, and consistent manner; ‘‘(7) ‘integrated service delivery’ means the provision of Internet-based Federal Government information or services integrated according to function or topic rather than separated according to the boundaries of agency jurisdiction; and ‘‘(8) ‘tribal government’ means— ‘‘(A) the governing body of any Indian tribe, band, nation, or other organized group or community located in the continental United States (excluding the State of Alaska) that is recognized as eligible for the special programs and services provided by the United States to Indians because of their status as Indians, and ‘‘(B) any Alaska Native regional or village corporation established pursuant to the Alaska Native Claims Settlement Act (43 U.S.C. 1601 et seq.). ‘‘§ 3602. Office of Electronic Government ‘‘(a) There is established in the Office of Management and Budget an Office of Electronic Government. ‘‘(b) There shall be at the head of the Office an Administrator who shall be appointed by the President. ‘‘(c) The Administrator shall assist the Director in carrying out— ‘‘(1) all functions under this chapter; President. Establishment. Government organization. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00004 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2903 ‘‘(2) all of the functions assigned to the Director under title II of the E-Government Act of 2002; and ‘‘(3) other electronic government initiatives, consistent with other statutes. ‘‘(d) The Administrator shall assist the Director and the Deputy Director for Management and work with the Administrator of the Office of Information and Regulatory Affairs in setting strategic direction for implementing electronic Government, under relevant statutes, including— ‘‘(1) chapter 35; ‘‘(2) subtitle III of title 40, United States Code; ‘‘(3) section 552a of title 5 (commonly referred to as the ‘Privacy Act’); ‘‘(4) the Government Paperwork Elimination Act (44 U.S.C. 3504 note); and ‘‘(5) the Federal Information Security Management Act of 2002. ‘‘(e)", "quality": {"lang": "en", "gibberish_score": 0.0253, "char_len": 2376, "word_len": 350}, "dedupe": {"exact_hash": "8bba6eb9a1428e38ec326922946a565251e64077f163f0bfabc450c580a1e79c", "simhash64": 11222229545313207686}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0875edc19a68e3aa0a2c28b06be245e69a85aa3f1d455c60497b15210d376405", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 5, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "under relevant statutes, including— ‘‘(1) chapter 35; ‘‘(2) subtitle III of title 40, United States Code; ‘‘(3) section 552a of title 5 (commonly referred to as the ‘Privacy Act’); ‘‘(4) the Government Paperwork Elimination Act (44 U.S.C. 3504 note); and ‘‘(5) the Federal Information Security Management Act of 2002. ‘‘(e) The Administrator shall work with the Administrator of the Office of Information and Regulatory Affairs and with other offices within the Office of Management and Budget to oversee implementation of electronic Government under this chapter, chapter 35, the E-Government Act of 2002, and other relevant statutes, in a manner consistent with law, relating to— ‘‘(1) capital planning and investment control for information technology; ‘‘(2) the development of enterprise architectures; ‘‘(3) information security; ‘‘(4) privacy; ‘‘(5) access to, dissemination of, and preservation of Government information; ‘‘(6) accessibility of information technology for persons with disabilities; and ‘‘(7) other areas of electronic Government. ‘‘(f) Subject to requirements of this chapter, the Administrator shall assist the Director by performing electronic Government functions as follows: ‘‘(1) Advise the Director on the resources required to develop and effectively administer electronic Government initiatives. ‘‘(2) Recommend to the Director changes relating to Governmentwide strategies and priorities for electronic Government. ‘‘(3) Provide overall leadership and direction to the executive branch on electronic Government. ‘‘(4) Promote innovative uses of information technology by agencies, particularly initiatives involving multiagency collaboration, through support of pilot projects, research, experimentation, and the use of innovative technologies. ‘‘(5) Oversee the distribution of funds from, and ensure appropriate administration and coordination of, the E-Government Fund established under section 3604. ‘‘(6) Coordinate with the Administrator of General Services regarding programs undertaken by the General Services Administration to promote electronic government and the efficient use of information technologies by agencies. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00005 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2904 ‘‘(7) Lead the activities of the Chief Information Officers Council established under section 3603 on behalf of the Deputy Director for Management, who shall chair the council. ‘‘(8) Assist the Director in establishing policies which shall set the framework", "quality": {"lang": "en", "gibberish_score": 0.0197, "char_len": 2544, "word_len": 350}, "dedupe": {"exact_hash": "738d09655b6fddd4c99f379f7fd14e4a2386c3684a7f8afe8ccde2af76024a8c", "simhash64": 15256328911563900102}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ae533ba1b8960b6fa7e57bccf6ffa0b868f1cb0ae940fd23c0a8f8e16e5b44dd", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 6, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "00347 Frm 00005 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2904 ‘‘(7) Lead the activities of the Chief Information Officers Council established under section 3603 on behalf of the Deputy Director for Management, who shall chair the council. ‘‘(8) Assist the Director in establishing policies which shall set the framework for information technology standards for the Federal Government developed by the National Institute of Standards and Technology and promulgated by the Secretary of Commerce under section 11331 of title 40, taking into account, if appropriate, recommendations of the Chief Information Officers Council, experts, and interested parties from the private and nonprofit sectors and State, local, and tribal governments, and maximizing the use of commercial standards as appropriate, including the following: ‘‘(A) Standards and guidelines for interconnectivity and interoperability as described under section 3504. ‘‘(B) Consistent with the process under section 207(d) of the E-Government Act of 2002, standards and guidelines for categorizing Federal Government electronic information to enable efficient use of technologies, such as through the use of extensible markup language. ‘‘(C) Standards and guidelines for Federal Government computer system efficiency and security. ‘‘(9) Sponsor ongoing dialogue that— ‘‘(A) shall be conducted among Federal, State, local, and tribal government leaders on electronic Government in the executive, legislative, and judicial branches, as well as leaders in the private and nonprofit sectors, to encourage collaboration and enhance understanding of best practices and innovative approaches in acquiring, using, and managing information resources; ‘‘(B) is intended to improve the performance of governments in collaborating on the use of information technology to improve the delivery of Government information and services; and ‘‘(C) may include— ‘‘(i) development of innovative models— ‘‘(I) for electronic Government management and Government information technology contracts; and ‘‘(II) that may be developed through focused discussions or using separately sponsored research; ‘‘(ii) identification of opportunities for public-private collaboration in using Internet-based technology to increase the efficiency of Government-to-business transactions; ‘‘(iii) identification of mechanisms for providing incentives to program managers and other Government employees to develop and implement innovative uses of information technologies; and ‘‘(iv) identification of opportunities for public, private, and intergovernmental collaboration", "quality": {"lang": "en", "gibberish_score": 0.0134, "char_len": 2604, "word_len": 350}, "dedupe": {"exact_hash": "469f51710920e467a749012e0537dcac49fc0e2f818012d2fb92ca2c7db8c9c3", "simhash64": 10663484126503994070}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:3bd9428ea9993f5667993016226782368c42a31a0bccf3a2c5a2a6db5d27b604", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 7, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "of opportunities for public-private collaboration in using Internet-based technology to increase the efficiency of Government-to-business transactions; ‘‘(iii) identification of mechanisms for providing incentives to program managers and other Government employees to develop and implement innovative uses of information technologies; and ‘‘(iv) identification of opportunities for public, private, and intergovernmental collaboration in addressing the disparities in access to the Internet and information technology. ‘‘(10) Sponsor activities to engage the general public in the development and implementation of policies and programs, particularly activities aimed at fulfilling the goal of using the most effective citizen-centered strategies and those activities VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00006 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2905 which engage multiple agencies providing similar or related information and services. ‘‘(11) Oversee the work of the General Services Administration and other agencies in developing the integrated Internetbased system under section 204 of the E-Government Act of 2002. ‘‘(12) Coordinate with the Administrator for Federal Procurement Policy to ensure effective implementation of electronic procurement initiatives. ‘‘(13) Assist Federal agencies, including the General Services Administration, the Department of Justice, and the United States Access Board in— ‘‘(A) implementing accessibility standards under section 508 of the Rehabilitation Act of 1973 (29 U.S.C. 794d); and ‘‘(B) ensuring compliance with those standards through the budget review process and other means. ‘‘(14) Oversee the development of enterprise architectures within and across agencies. ‘‘(15) Assist the Director and the Deputy Director for Management in overseeing agency efforts to ensure that electronic Government activities incorporate adequate, risk-based, and cost-effective security compatible with business processes. ‘‘(16) Administer the Office of Electronic Government established under this section. ‘‘(17) Assist the Director in preparing the E-Government report established under section 3606. ‘‘(g) The Director shall ensure that the Office of Management and Budget, including the Office of Electronic Government, the Office of Information and Regulatory Affairs, and other relevant offices, have adequate staff and resources to properly fulfill all functions under the E-Government Act of 2002. ‘‘§ 3603. Chief Information Officers Council ‘‘(a) There is established in the executive branch a Chief Information Officers", "quality": {"lang": "en", "gibberish_score": 0.0131, "char_len": 2605, "word_len": 350}, "dedupe": {"exact_hash": "f31a7fa06df3cd9cb3d6c2ab010aca9f5cfd4fed37862e003a7065cdad249669", "simhash64": 11226451669964128518}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:9845595e9c7447c330f4b3cd3a13143b82626459fc37016721c60d4798c5ed6a", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 8, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "the Office of Electronic Government, the Office of Information and Regulatory Affairs, and other relevant offices, have adequate staff and resources to properly fulfill all functions under the E-Government Act of 2002. ‘‘§ 3603. Chief Information Officers Council ‘‘(a) There is established in the executive branch a Chief Information Officers Council. ‘‘(b) The members of the Council shall be as follows: ‘‘(1) The Deputy Director for Management of the Office of Management and Budget, who shall act as chairperson of the Council. ‘‘(2) The Administrator of the Office of Electronic Government. ‘‘(3) The Administrator of the Office of Information and Regulatory Affairs. ‘‘(4) The chief information officer of each agency described under section 901(b) of title 31. ‘‘(5) The chief information officer of the Central Intelligence Agency. ‘‘(6) The chief information officer of the Department of the Army, the Department of the Navy, and the Department of the Air Force, if chief information officers have been designated for such departments under section 3506(a)(2)(B). ‘‘(7) Any other officer or employee of the United States designated by the chairperson. ‘‘(c)(1) The Administrator of the Office of Electronic Government shall lead the activities of the Council on behalf of the Deputy Director for Management. Establishment. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00007 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2906 ‘‘(2)(A) The Vice Chairman of the Council shall be selected by the Council from among its members. ‘‘(B) The Vice Chairman shall serve a 1-year term, and may serve multiple terms. ‘‘(3) The Administrator of General Services shall provide administrative and other support for the Council. ‘‘(d) The Council is designated the principal interagency forum for improving agency practices related to the design, acquisition, development, modernization, use, operation, sharing, and performance of Federal Government information resources. ‘‘(e) In performing its duties, the Council shall consult regularly with representatives of State, local, and tribal governments. ‘‘(f) The Council shall perform functions that include the following: ‘‘(1) Develop recommendations for the Director on Government information resources management policies and requirements. ‘‘(2) Share experiences, ideas, best practices, and innovative approaches related", "quality": {"lang": "en", "gibberish_score": 0.0171, "char_len": 2393, "word_len": 350}, "dedupe": {"exact_hash": "e89425dad51086da0eef113f293afd3f348a110fd43cf78f9c23ffff4dd82db0", "simhash64": 10852633109684928902}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7ef48aa4e3cd549d03efb28bd7de3662afab96c991ec5e501f4f1c68f2377365", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 9, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "performing its duties, the Council shall consult regularly with representatives of State, local, and tribal governments. ‘‘(f) The Council shall perform functions that include the following: ‘‘(1) Develop recommendations for the Director on Government information resources management policies and requirements. ‘‘(2) Share experiences, ideas, best practices, and innovative approaches related to information resources management. ‘‘(3) Assist the Administrator in the identification, development, and coordination of multiagency projects and other innovative initiatives to improve Government performance through the use of information technology. ‘‘(4) Promote the development and use of common performance measures for agency information resources management under this chapter and title II of the E-Government Act of 2002. ‘‘(5) Work as appropriate with the National Institute of Standards and Technology and the Administrator to develop recommendations on information technology standards developed under section 20 of the National Institute of Standards and Technology Act (15 U.S.C. 278g–3) and promulgated under section 11331 of title 40, and maximize the use of commercial standards as appropriate, including the following: ‘‘(A) Standards and guidelines for interconnectivity and interoperability as described under section 3504. ‘‘(B) Consistent with the process under section 207(d) of the E-Government Act of 2002, standards and guidelines for categorizing Federal Government electronic information to enable efficient use of technologies, such as through the use of extensible markup language. ‘‘(C) Standards and guidelines for Federal Government computer system efficiency and security. ‘‘(6) Work with the Office of Personnel Management to assess and address the hiring, training, classification, and professional development needs of the Government related to information resources management. ‘‘(7) Work with the Archivist of the United States to assess how the Federal Records Act can be addressed effectively by Federal information resources management activities. ‘‘§ 3604. E-Government Fund ‘‘(a)(1) There is established in the Treasury of the United States the E-Government Fund. ‘‘(2) The Fund shall be administered by the Administrator of the General Services Administration to support projects approved by the Director, assisted by the Administrator of the Office of VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00008 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2907", "quality": {"lang": "en", "gibberish_score": 0.0127, "char_len": 2518, "word_len": 350}, "dedupe": {"exact_hash": "479ade5d581839170485dd85ed3077b8dd0e4aa00bf05bf132fc1927b2b0d376", "simhash64": 10662375816635728022}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:97b17f6012c0826d68dfdf09f4120c710c51182da4e8c4e388f3fa2cc84eb0ac", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 10, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "Fund. ‘‘(2) The Fund shall be administered by the Administrator of the General Services Administration to support projects approved by the Director, assisted by the Administrator of the Office of VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00008 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2907 Electronic Government, that enable the Federal Government to expand its ability, through the development and implementation of innovative uses of the Internet or other electronic methods, to conduct activities electronically. ‘‘(3) Projects under this subsection may include efforts to— ‘‘(A) make Federal Government information and services more readily available to members of the public (including individuals, businesses, grantees, and State and local governments); ‘‘(B) make it easier for the public to apply for benefits, receive services, pursue business opportunities, submit information, and otherwise conduct transactions with the Federal Government; and ‘‘(C) enable Federal agencies to take advantage of information technology in sharing information and conducting transactions with each other and with State and local governments. ‘‘(b)(1) The Administrator shall— ‘‘(A) establish procedures for accepting and reviewing proposals for funding; ‘‘(B) consult with interagency councils, including the Chief Information Officers Council, the Chief Financial Officers Council, and other interagency management councils, in establishing procedures and reviewing proposals; and ‘‘(C) assist the Director in coordinating resources that agencies receive from the Fund with other resources available to agencies for similar purposes. ‘‘(2) When reviewing proposals and managing the Fund, the Administrator shall observe and incorporate the following procedures: ‘‘(A) A project requiring substantial involvement or funding from an agency shall be approved by a senior official with agencywide authority on behalf of the head of the agency, who shall report directly to the head of the agency. ‘‘(B) Projects shall adhere to fundamental capital planning and investment control processes. ‘‘(C) Agencies shall identify in their proposals resource commitments from the agencies involved and how these resources would be coordinated with support from the Fund, and include plans for potential continuation of projects after all funds made available from the Fund are expended. ‘‘(D) After considering the recommendations of the interagency councils, the", "quality": {"lang": "en", "gibberish_score": 0.0129, "char_len": 2474, "word_len": 350}, "dedupe": {"exact_hash": "af9d91471d4ebe2e83b16204928c5c0410aa3df084b18533558741ca3b59a4f9", "simhash64": 15256029844367328278}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:54b6372621618295a4a5c4acff9e53a544d02654507e841b7c8d97300468d628", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 11, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "identify in their proposals resource commitments from the agencies involved and how these resources would be coordinated with support from the Fund, and include plans for potential continuation of projects after all funds made available from the Fund are expended. ‘‘(D) After considering the recommendations of the interagency councils, the Director, assisted by the Administrator, shall have final authority to determine which of the candidate projects shall be funded from the Fund. ‘‘(E) Agencies shall assess the results of funded projects. ‘‘(c) In determining which proposals to recommend for funding, the Administrator— ‘‘(1) shall consider criteria that include whether a proposal— ‘‘(A) identifies the group to be served, including citizens, businesses, the Federal Government, or other governments; ‘‘(B) indicates what service or information the project will provide that meets needs of groups identified under subparagraph (A); ‘‘(C) ensures proper security and protects privacy; Procedures. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00009 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2908 ‘‘(D) is interagency in scope, including projects implemented by a primary or single agency that— ‘‘(i) could confer benefits on multiple agencies; and ‘‘(ii) have the support of other agencies; and ‘‘(E) has performance objectives that tie to agency missions and strategic goals, and interim results that relate to the objectives; and ‘‘(2) may also rank proposals based on criteria that include whether a proposal— ‘‘(A) has Governmentwide application or implications; ‘‘(B) has demonstrated support by the public to be served; ‘‘(C) integrates Federal with State, local, or tribal approaches to service delivery; ‘‘(D) identifies resource commitments from nongovernmental sectors; ‘‘(E) identifies resource commitments from the agencies involved; ‘‘(F) uses web-based technologies to achieve objectives; ‘‘(G) identifies records management and records access strategies; ‘‘(H) supports more effective citizen participation in and interaction with agency activities that further progress toward a more citizen-centered Government; ‘‘(I) directly delivers Government information and services to the public or provides the infrastructure for delivery; ‘‘(J) supports integrated service delivery; ‘‘(K) describes how business processes across agencies will reflect appropriate transformation simultaneous to technology implementation; and ‘‘(L) is new or innovative and", "quality": {"lang": "en", "gibberish_score": 0.0216, "char_len": 2498, "word_len": 350}, "dedupe": {"exact_hash": "5f745aca52b2a999e92f6aab13ea23ffaba0ad4316a723446bbab7417c0688e3", "simhash64": 11150154359122545798}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:479ee0b899906b5e47750b430b7d070cf19ebeff35e97825e2df215d64ab539b", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 12, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "further progress toward a more citizen-centered Government; ‘‘(I) directly delivers Government information and services to the public or provides the infrastructure for delivery; ‘‘(J) supports integrated service delivery; ‘‘(K) describes how business processes across agencies will reflect appropriate transformation simultaneous to technology implementation; and ‘‘(L) is new or innovative and does not supplant existing funding streams within agencies. ‘‘(d) The Fund may be used to fund the integrated Internetbased system under section 204 of the E-Government Act of 2002. ‘‘(e) None of the funds provided from the Fund may be transferred to any agency until 15 days after the Administrator of the General Services Administration has submitted to the Committees on Appropriations of the Senate and the House of Representatives, the Committee on Governmental Affairs of the Senate, the Committee on Government Reform of the House of Representatives, and the appropriate authorizing committees of the Senate and the House of Representatives, a notification and description of how the funds are to be allocated and how the expenditure will further the purposes of this chapter. ‘‘(f)(1) The Director shall report annually to Congress on the operation of the Fund, through the report established under section 3606. ‘‘(2) The report under paragraph (1) shall describe— ‘‘(A) all projects which the Director has approved for funding from the Fund; and ‘‘(B) the results that have been achieved to date for these funded projects. ‘‘(g)(1) There are authorized to be appropriated to the Fund— ‘‘(A) $45,000,000 for fiscal year 2003; ‘‘(B) $50,000,000 for fiscal year 2004; ‘‘(C) $100,000,000 for fiscal year 2005; ‘‘(D) $150,000,000 for fiscal year 2006; and Reports. Notification. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00010 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2909 ‘‘(E) such sums as are necessary for fiscal year 2007. ‘‘(2) Funds appropriated under this subsection shall remain available until expended. ‘‘§ 3605. Program to encourage innovative solutions to enhance electronic Government services and processes ‘‘(a) ESTABLISHMENT OF PROGRAM.—The Administrator shall establish and promote a Governmentwide program to encourage contractor innovation and excellence in facilitating the development and enhancement of electronic Government services", "quality": {"lang": "en", "gibberish_score": 0.0201, "char_len": 2388, "word_len": 350}, "dedupe": {"exact_hash": "08d97529d27b57b66a95960e4cf23390755041af296a7a008d9585190f6658df", "simhash64": 11158880083408962582}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e3015cdc28cead30afa68941f03495a0e08a48af8fda58ba62cc686be798893d", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 13, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "under this subsection shall remain available until expended. ‘‘§ 3605. Program to encourage innovative solutions to enhance electronic Government services and processes ‘‘(a) ESTABLISHMENT OF PROGRAM.—The Administrator shall establish and promote a Governmentwide program to encourage contractor innovation and excellence in facilitating the development and enhancement of electronic Government services and processes. ‘‘(b) ISSUANCE OF ANNOUNCEMENTS SEEKING INNOVATIVE SOLUTIONS.—Under the program, the Administrator, in consultation with the Council and the Administrator for Federal Procurement Policy, shall issue announcements seeking unique and innovative solutions to facilitate the development and enhancement of electronic Government services and processes. ‘‘(c) MULTIAGENCY TECHNICAL ASSISTANCE TEAM.—(1) The Administrator, in consultation with the Council and the Administrator for Federal Procurement Policy, shall convene a multiagency technical assistance team to assist in screening proposals submitted to the Administrator to provide unique and innovative solutions to facilitate the development and enhancement of electronic Government services and processes. The team shall be composed of employees of the agencies represented on the Council who have expertise in scientific and technical disciplines that would facilitate the assessment of the feasibility of the proposals. ‘‘(2) The technical assistance team shall— ‘‘(A) assess the feasibility, scientific and technical merits, and estimated cost of each proposal; and ‘‘(B) submit each proposal, and the assessment of the proposal, to the Administrator. ‘‘(3) The technical assistance team shall not consider or evaluate proposals submitted in response to a solicitation for offers for a pending procurement or for a specific agency requirement. ‘‘(4) After receiving proposals and assessments from the technical assistance team, the Administrator shall consider recommending appropriate proposals for funding under the E-Government Fund established under section 3604 or, if appropriate, forward the proposal and the assessment of it to the executive agency whose mission most coincides with the subject matter of the proposal. ‘‘§ 3606. E-Government report ‘‘(a) Not later than March 1 of each year, the Director shall submit an E-Government status report to the Committee on Governmental Affairs of the Senate and the Committee on Government Reform of the House of Representatives. ‘‘(b) The report under subsection (a) shall contain— ‘‘(1) a summary", "quality": {"lang": "en", "gibberish_score": 0.0131, "char_len": 2526, "word_len": 350}, "dedupe": {"exact_hash": "9c753cb0012856cb8acc24712ffa8ea4bd57530c76de02a69ed6f0bc3d9e7ccd", "simhash64": 10573409932821237766}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e761e7c67cd50a60c129bcd7bf39c03253670391be1d7253386f31eed30d7ce9", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 14, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "E-Government report ‘‘(a) Not later than March 1 of each year, the Director shall submit an E-Government status report to the Committee on Governmental Affairs of the Senate and the Committee on Government Reform of the House of Representatives. ‘‘(b) The report under subsection (a) shall contain— ‘‘(1) a summary of the information reported by agencies under section 202(f) of the E-Government Act of 2002; ‘‘(2) the information required to be reported by section 3604(f); and ‘‘(3) a description of compliance by the Federal Government with other goals and provisions of the E-Government Act of 2002.’’. Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00011 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2910 (b) TECHNICAL AND CONFORMING AMENDMENT.—The table of chapters for title 44, United States Code, is amended by inserting after the item relating to chapter 35 the following: ‘‘36. Management and Promotion of Electronic Government Services 3601’’. SEC. 102. CONFORMING AMENDMENTS. (a) ELECTRONIC GOVERNMENT AND INFORMATION TECHNOLOGIES.— (1) IN GENERAL.—Chapter 3 of title 40, United States Code, is amended by inserting after section 304 the following new section: ‘‘§ 305. Electronic Government and information technologies ‘‘The Administrator of General Services shall consult with the Administrator of the Office of Electronic Government on programs undertaken by the General Services Administration to promote electronic Government and the efficient use of information technologies by Federal agencies.’’. (2) TECHNICAL AND CONFORMING AMENDMENT.—The table of sections for chapter 3 of such title is amended by inserting after the item relating to section 304 the following: ‘‘305. Electronic Government and information technologies.’’. (b) MODIFICATION OF DEPUTY DIRECTOR FOR MANAGEMENT FUNCTIONS.—Section 503(b) of title 31, United States Code, is amended— (1) by redesignating paragraphs (5), (6), (7), (8), and (9), as paragraphs (6), (7), (8), (9), and (10), respectively; and (2) by inserting after paragraph (4) the following: ‘‘(5) Chair the Chief Information Officers Council established under section 3603 of title 44.’’. (c) OFFICE OF ELECTRONIC GOVERNMENT.— (1) IN GENERAL.—Chapter 5 of title 31, United States Code, is amended by inserting after section 506 the following: ‘‘§ 507. Office of Electronic", "quality": {"lang": "en", "gibberish_score": 0.0191, "char_len": 2359, "word_len": 350}, "dedupe": {"exact_hash": "8d726a6f72777eab8c71e1ec5b78fa0bbc3a583455646bdd9a75703b8ad0ed5d", "simhash64": 11217725945719669910}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0264f8994c1620f7b1c3909095a8d3295c26fcbe35efdb07a07ded5088424c23", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 15, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "by inserting after paragraph (4) the following: ‘‘(5) Chair the Chief Information Officers Council established under section 3603 of title 44.’’. (c) OFFICE OF ELECTRONIC GOVERNMENT.— (1) IN GENERAL.—Chapter 5 of title 31, United States Code, is amended by inserting after section 506 the following: ‘‘§ 507. Office of Electronic Government ‘‘The Office of Electronic Government, established under section 3602 of title 44, is an office in the Office of Management and Budget.’’. (2) TECHNICAL AND CONFORMING AMENDMENT.—The table of sections for chapter 5 of title 31, United States Code, is amended by inserting after the item relating to section 506 the following: ‘‘507. Office of Electronic Government.’’. TITLE II—FEDERAL MANAGEMENT AND PROMOTION OF ELECTRONIC GOVERNMENT SERVICES SEC. 201. DEFINITIONS. Except as otherwise provided, in this title the definitions under sections 3502 and 3601 of title 44, United States Code, shall apply. 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00012 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2911 SEC. 202. FEDERAL AGENCY RESPONSIBILITIES. (a) IN GENERAL.—The head of each agency shall be responsible for— (1) complying with the requirements of this Act (including the amendments made by this Act), the related information resource management policies and guidance established by the Director of the Office of Management and Budget, and the related information technology standards promulgated by the Secretary of Commerce; (2) ensuring that the information resource management policies and guidance established under this Act by the Director, and the related information technology standards promulgated by the Secretary of Commerce are communicated promptly and effectively to all relevant officials within their agency; and (3) supporting the efforts of the Director and the Administrator of the General Services Administration to develop, maintain, and promote an integrated Internet-based system of delivering Federal Government information and services to the public under section 204. (b) PERFORMANCE INTEGRATION.— (1) Agencies shall develop performance measures that demonstrate how electronic government enables progress toward agency objectives, strategic goals, and statutory mandates. (2) In measuring performance under this section, agencies shall rely on existing data collections to the extent practicable. (3)", "quality": {"lang": "en", "gibberish_score": 0.0099, "char_len": 2415, "word_len": 350}, "dedupe": {"exact_hash": "f9dcdbde4b33b8071664b84efc06d95b9f61f82a520a90abdb319f856cfbc799", "simhash64": 11234614375568950294}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8eda6d343fad6416cda0857ae19c00a69125f6527497925631a49fb6b934d945", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 16, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "services to the public under section 204. (b) PERFORMANCE INTEGRATION.— (1) Agencies shall develop performance measures that demonstrate how electronic government enables progress toward agency objectives, strategic goals, and statutory mandates. (2) In measuring performance under this section, agencies shall rely on existing data collections to the extent practicable. (3) Areas of performance measurement that agencies should consider include— (A) customer service; (B) agency productivity; and (C) adoption of innovative information technology, including the appropriate use of commercial best practices. (4) Agencies shall link their performance goals, as appropriate, to key groups, including citizens, businesses, and other governments, and to internal Federal Government operations. (5) As appropriate, agencies shall work collectively in linking their performance goals to groups identified under paragraph (4) and shall use information technology in delivering Government information and services to those groups. (c) AVOIDING DIMINISHED ACCESS.—When promulgating policies and implementing programs regarding the provision of Government information and services over the Internet, agency heads shall consider the impact on persons without access to the Internet, and shall, to the extent practicable— (1) ensure that the availability of Government information and services has not been diminished for individuals who lack access to the Internet; and (2) pursue alternate modes of delivery that make Government information and services more accessible to individuals who do not own computers or lack access to the Internet. (d) ACCESSIBILITY TO PEOPLE WITH DISABILITIES.—All actions taken by Federal departments and agencies under this Act shall be in compliance with section 508 of the Rehabilitation Act of 1973 (29 U.S.C. 794d). (e) SPONSORED ACTIVITIES.—Agencies shall sponsor activities that use information technology to engage the public in the development and implementation of policies and programs. 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00013 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2912 (f) CHIEF INFORMATION OFFICERS.—The Chief Information Officer of each of the agencies designated under chapter 36 of title 44, United States Code (as added by this Act) shall be responsible for— (1) participating in the functions of the Chief Information Officers Council; and (2) monitoring the", "quality": {"lang": "en", "gibberish_score": 0.0041, "char_len": 2457, "word_len": 350}, "dedupe": {"exact_hash": "070eb7922b1b2c22aec14bc87898f96dd662cc4d62fa804beb7537009aaa20c7", "simhash64": 11163666257483251910}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:57c9df30198079e9e4f84d12d092c098194a7a3e7fbec71d87404d7788d96ffa", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 17, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "E:\\PUBLAW\\PUBL347.107 116 STAT. 2912 (f) CHIEF INFORMATION OFFICERS.—The Chief Information Officer of each of the agencies designated under chapter 36 of title 44, United States Code (as added by this Act) shall be responsible for— (1) participating in the functions of the Chief Information Officers Council; and (2) monitoring the implementation, within their respective agencies, of information technology standards promulgated by the Secretary of Commerce, including common standards for interconnectivity and interoperability, categorization of Federal Government electronic information, and computer system efficiency and security. (g) E-GOVERNMENT STATUS REPORT.— (1) IN GENERAL.—Each agency shall compile and submit to the Director an annual E-Government Status Report on— (A) the status of the implementation by the agency of electronic government initiatives; (B) compliance by the agency with this Act; and (C) how electronic Government initiatives of the agency improve performance in delivering programs to constituencies. (2) SUBMISSION.—Each agency shall submit an annual report under this subsection— (A) to the Director at such time and in such manner as the Director requires; (B) consistent with related reporting requirements; and (C) which addresses any section in this title relevant to that agency. (h) USE OF TECHNOLOGY.—Nothing in this Act supersedes the responsibility of an agency to use or manage information technology to deliver Government information and services that fulfill the statutory mission and programs of the agency. (i) NATIONAL SECURITY SYSTEMS.— (1) INAPPLICABILITY.—Except as provided under paragraph (2), this title does not apply to national security systems as defined in section 11103 of title 40, United States Code. (2) APPLICABILITY.—This section, section 203, and section 214 do apply to national security systems to the extent practicable and consistent with law. SEC. 203. COMPATIBILITY OF EXECUTIVE AGENCY METHODS FOR USE AND ACCEPTANCE OF ELECTRONIC SIGNATURES. (a) PURPOSE.—The purpose of this section is to achieve interoperable implementation of electronic signatures for appropriately secure electronic transactions with Government. (b) ELECTRONIC SIGNATURES.—In order to fulfill the objectives of the Government Paperwork Elimination Act (Public Law 105– 277; 112 Stat. 2681–749 through 2681–751), each Executive agency (as defined under section 105 of title 5, United States Code) shall ensure that", "quality": {"lang": "en", "gibberish_score": 0.0073, "char_len": 2461, "word_len": 350}, "dedupe": {"exact_hash": "89110326d0d4412777092fbc42acca0da8d09b58df4acc3f3a05703fcd41a44a", "simhash64": 11240227382462365846}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:74b329fd6197a51d14b3548837d99a7fa263556d404bb1f87e3df082ff657cf4", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 18, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "of electronic signatures for appropriately secure electronic transactions with Government. (b) ELECTRONIC SIGNATURES.—In order to fulfill the objectives of the Government Paperwork Elimination Act (Public Law 105– 277; 112 Stat. 2681–749 through 2681–751), each Executive agency (as defined under section 105 of title 5, United States Code) shall ensure that its methods for use and acceptance of electronic signatures are compatible with the relevant policies and procedures issued by the Director. (c) AUTHORITY FOR ELECTRONIC SIGNATURES.—The Administrator of General Services shall support the Director by establishing a framework to allow efficient interoperability among Executive agencies when using electronic signatures, including processing of digital signatures. 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00014 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2913 (d) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated to the General Services Administration, to ensure the development and operation of a Federal bridge certification authority for digital signature compatibility, and for other activities consistent with this section, $8,000,000 or such sums as are necessary in fiscal year 2003, and such sums as are necessary for each fiscal year thereafter. SEC. 204. FEDERAL INTERNET PORTAL. (a) IN GENERAL.— (1) PUBLIC ACCESS.—The Director shall work with the Administrator of the General Services Administration and other agencies to maintain and promote an integrated Internet-based system of providing the public with access to Government information and services. (2) CRITERIA.—To the extent practicable, the integrated system shall be designed and operated according to the following criteria: (A) The provision of Internet-based Government information and services directed to key groups, including citizens, business, and other governments, and integrated according to function or topic rather than separated according to the boundaries of agency jurisdiction. (B) An ongoing effort to ensure that Internet-based Government services relevant to a given citizen activity are available from a single point. (C) Access to Federal Government information and services consolidated, as appropriate, with Internet-based information and services provided by State, local, and tribal governments. (D) Access to Federal Government information held by 1 or more agencies shall be made available in a manner", "quality": {"lang": "en", "gibberish_score": 0.0048, "char_len": 2481, "word_len": 350}, "dedupe": {"exact_hash": "f9c2f556efcc06ef0fcbaeecbdd231d58634f7e189288b0fbc28843cea1b9f00", "simhash64": 15761854671168013830}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:154ba341fddc2406a1688a6344649f482853113cc21dccaf34a8eb2de2d0a99d", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 19, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "citizen activity are available from a single point. (C) Access to Federal Government information and services consolidated, as appropriate, with Internet-based information and services provided by State, local, and tribal governments. (D) Access to Federal Government information held by 1 or more agencies shall be made available in a manner that protects privacy, consistent with law. (b) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated to the General Services Administration $15,000,000 for the maintenance, improvement, and promotion of the integrated Internet-based system for fiscal year 2003, and such sums as are necessary for fiscal years 2004 through 2007. SEC. 205. FEDERAL COURTS. (a) INDIVIDUAL COURT WEBSITES.—The Chief Justice of the United States, the chief judge of each circuit and district and of the Court of Federal Claims, and the chief bankruptcy judge of each district shall cause to be established and maintained, for the court of which the judge is chief justice or judge, a website that contains the following information or links to websites with the following information: (1) Location and contact information for the courthouse, including the telephone numbers and contact names for the clerk’s office and justices’ or judges’ chambers. (2) Local rules and standing or general orders of the court. (3) Individual rules, if in existence, of each justice or judge in that court. (4) Access to docket information for each case. (5) Access to the substance of all written opinions issued by the court, regardless of whether such opinions are to be published in the official court reporter, in a text searchable format. 44 USC 3501 note. 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00015 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2914 (6) Access to documents filed with the courthouse in electronic form, to the extent provided under subsection (c). (7) Any other information (including forms in a format that can be downloaded) that the court determines useful to the public. (b) MAINTENANCE OF DATA ONLINE.— (1) UPDATE OF INFORMATION.—The information and rules on each website shall be updated regularly and kept reasonably current. (2) CLOSED", "quality": {"lang": "en", "gibberish_score": 0.0045, "char_len": 2241, "word_len": 350}, "dedupe": {"exact_hash": "32a94c417b43879b65a315adb2ee0b57a5bb2eb8006b3bb4d7b9517677f2ff55", "simhash64": 11217444470708864134}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6baa065a1a1b72f32612e8193137f0822bc873f13258d13462ffd3193eda4206", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 20, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "provided under subsection (c). (7) Any other information (including forms in a format that can be downloaded) that the court determines useful to the public. (b) MAINTENANCE OF DATA ONLINE.— (1) UPDATE OF INFORMATION.—The information and rules on each website shall be updated regularly and kept reasonably current. (2) CLOSED CASES.—Electronic files and docket information for cases closed for more than 1 year are not required to be made available online, except all written opinions with a date of issuance after the effective date of this section shall remain available online. (c) ELECTRONIC FILINGS.— (1) IN GENERAL.—Except as provided under paragraph (2) or in the rules prescribed under paragraph (3), each court shall make any document that is filed electronically publicly available online. A court may convert any document that is filed in paper form to electronic form. To the extent such conversions are made, all such electronic versions of the document shall be made available online. (2) EXCEPTIONS.—Documents that are filed that are not otherwise available to the public, such as documents filed under seal, shall not be made available online. (3) PRIVACY AND SECURITY CONCERNS.—(A)(i) The Supreme Court shall prescribe rules, in accordance with sections 2072 and 2075 of title 28, United States Code, to protect privacy and security concerns relating to electronic filing of documents and the public availability under this subsection of documents filed electronically. (ii) Such rules shall provide to the extent practicable for uniform treatment of privacy and security issues throughout the Federal courts. (iii) Such rules shall take into consideration best practices in Federal and State courts to protect private information or otherwise maintain necessary information security. (iv) To the extent that such rules provide for the redaction of certain categories of information in order to protect privacy and security concerns, such rules shall provide that a party that wishes to file an otherwise proper document containing such information may file an unredacted document under seal, which shall be retained by the court as part of the record, and which, at the discretion of the court and subject to any applicable rules issued", "quality": {"lang": "en", "gibberish_score": 0.0031, "char_len": 2255, "word_len": 350}, "dedupe": {"exact_hash": "91bb01051d76bfca91f89abb6ce294066443617711f41911c89e89d9a7a4a6ed", "simhash64": 15847144818959827158}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ae34c8333cb63abe90bb842e29052f9f72fa028c769ecdf629889a64c1fad96c", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 21, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "shall provide that a party that wishes to file an otherwise proper document containing such information may file an unredacted document under seal, which shall be retained by the court as part of the record, and which, at the discretion of the court and subject to any applicable rules issued in accordance with chapter 131 of title 28, United States Code, shall be either in lieu of, or in addition, to, a redacted copy in the public file. (B)(i) Subject to clause (ii), the Judicial Conference of the United States may issue interim rules, and interpretive statements relating to the application of such rules, which conform to the requirements of this paragraph and which shall cease to have effect upon the effective date of the rules required under subparagraph (A). (ii) Pending issuance of the rules required under subparagraph (A), any rule or order of any court, or of the Judicial Conference, providing for the redaction of certain categories of information in order to protect privacy and security concerns Regulations. Public information. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00016 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2915 arising from electronic filing shall comply with, and be construed in conformity with, subparagraph (A)(iv). (C) Not later than 1 year after the rules prescribed under subparagraph (A) take effect, and every 2 years thereafter, the Judicial Conference shall submit to Congress a report on the adequacy of those rules to protect privacy and security. (d) DOCKETS WITH LINKS TO DOCUMENTS.—The Judicial Conference of the United States shall explore the feasibility of technology to post online dockets with links allowing all filings, decisions, and rulings in each case to be obtained from the docket sheet of that case. (e) COST OF PROVIDING ELECTRONIC DOCKETING INFORMATION.—Section 303(a) of the Judiciary Appropriations Act, 1992 (28 U.S.C. 1913 note) is amended in the first sentence by striking ‘‘shall hereafter’’ and inserting ‘‘may, only to the extent necessary,’’. (f) TIME REQUIREMENTS.—Not later than 2 years after the effective date of this title, the websites under subsection (a) shall be established, except that", "quality": {"lang": "en", "gibberish_score": 0.0059, "char_len": 2202, "word_len": 350}, "dedupe": {"exact_hash": "effa88dedd49153e533eafaac86637e116b21e7f3edf110ef2b84e8de25c89bd", "simhash64": 11239944807973752838}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:803fb398ab7a6c8ef7d28319a0c28ed6cc4242068bc7734480630a89bb11ef3a", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 22, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "Judiciary Appropriations Act, 1992 (28 U.S.C. 1913 note) is amended in the first sentence by striking ‘‘shall hereafter’’ and inserting ‘‘may, only to the extent necessary,’’. (f) TIME REQUIREMENTS.—Not later than 2 years after the effective date of this title, the websites under subsection (a) shall be established, except that access to documents filed in electronic form shall be established not later than 4 years after that effective date. (g) DEFERRAL.— (1) IN GENERAL.— (A) ELECTION.— (i) NOTIFICATION.—The Chief Justice of the United States, a chief judge, or chief bankruptcy judge may submit a notification to the Administrative Office of the United States Courts to defer compliance with any requirement of this section with respect to the Supreme Court, a court of appeals, district, or the bankruptcy court of a district. (ii) CONTENTS.—A notification submitted under this subparagraph shall state— (I) the reasons for the deferral; and (II) the online methods, if any, or any alternative methods, such court or district is using to provide greater public access to information. (B) EXCEPTION.—To the extent that the Supreme Court, a court of appeals, district, or bankruptcy court of a district maintains a website under subsection (a), the Supreme Court or that court of appeals or district shall comply with subsection (b)(1). (2) REPORT.—Not later than 1 year after the effective date of this title, and every year thereafter, the Judicial Conference of the United States shall submit a report to the Committees on Governmental Affairs and the Judiciary of the Senate and the Committees on Government Reform and the Judiciary of the House of Representatives that— (A) contains all notifications submitted to the Administrative Office of the United States Courts under this subsection; and (B) summarizes and evaluates all notifications. SEC. 206. REGULATORY AGENCIES. (a) PURPOSES.—The purposes of this section are to— (1) improve performance in the development and issuance of agency regulations by using information technology to increase access, accountability, and transparency; and 44 USC 3501 note. Deadline. Deadlines. Deadlines. Reports. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00017 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT.", "quality": {"lang": "en", "gibberish_score": 0.0096, "char_len": 2283, "word_len": 350}, "dedupe": {"exact_hash": "86a8f781495378f7cf32a3f8156f79869a9ae2d303cde40bc245408b12cf1a47", "simhash64": 11514660055916320006}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4e3e8e647a01c50991461ce3e2f0aa178875ad320f4b18611b85523ef9706883", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 23, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "to— (1) improve performance in the development and issuance of agency regulations by using information technology to increase access, accountability, and transparency; and 44 USC 3501 note. Deadline. Deadlines. Deadlines. Reports. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00017 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2916 (2) enhance public participation in Government by electronic means, consistent with requirements under subchapter II of chapter 5 of title 5, United States Code, (commonly referred to as the ‘‘Administrative Procedures Act’’). (b) INFORMATION PROVIDED BY AGENCIES ONLINE.—To the extent practicable as determined by the agency in consultation with the Director, each agency (as defined under section 551 of title 5, United States Code) shall ensure that a publicly accessible Federal Government website includes all information about that agency required to be published in the Federal Register under paragraphs (1) and (2) of section 552(a) of title 5, United States Code. (c) SUBMISSIONS BY ELECTRONIC MEANS.—To the extent practicable, agencies shall accept submissions under section 553(c) of title 5, United States Code, by electronic means. (d) ELECTRONIC DOCKETING.— (1) IN GENERAL.—To the extent practicable, as determined by the agency in consultation with the Director, agencies shall ensure that a publicly accessible Federal Government website contains electronic dockets for rulemakings under section 553 of title 5, United States Code. (2) INFORMATION AVAILABLE.—Agency electronic dockets shall make publicly available online to the extent practicable, as determined by the agency in consultation with the Director— (A) all submissions under section 553(c) of title 5, United States Code; and (B) other materials that by agency rule or practice are included in the rulemaking docket under section 553(c) of title 5, United States Code, whether or not submitted electronically. (e) TIME LIMITATION.—Agencies shall implement the requirements of this section consistent with a timetable established by the Director and reported to Congress in the first annual report under section 3606 of title 44 (as added by this Act). SEC. 207. ACCESSIBILITY, USABILITY, AND PRESERVATION OF GOVERNMENT INFORMATION. (a) PURPOSE.—The purpose of this section is to improve the methods by which Government information, including information on", "quality": {"lang": "en", "gibberish_score": 0.0063, "char_len": 2391, "word_len": 350}, "dedupe": {"exact_hash": "292fc587b029fddc50b0f4bb4053d0940512027f673bbf10e2cbcf6ed5518e35", "simhash64": 10659279523206512774}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:44e4d19dfff381604ba00977b5becc37379565d2ba6aa235fc45946a2a4555ef", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 25, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "before the Committee submits all recommendations required under this section. (d) CATEGORIZING OF INFORMATION.— (1) COMMITTEE FUNCTIONS.—Not later than 2 years after the date of enactment of this Act, the Committee shall submit recommendations to the Director on— (A) the adoption of standards, which are open to the maximum extent feasible, to enable the organization and categorization of Government information— (i) in a way that is searchable electronically, including by searchable identifiers; and (ii) in ways that are interoperable across agencies; (B) the definition of categories of Government information which should be classified under the standards; and (C) determining priorities and developing schedules for the initial implementation of the standards by agencies. (2) FUNCTIONS OF THE DIRECTOR.—Not later than 1 year after the submission of recommendations under paragraph (1), the Director shall issue policies— (A) requiring that agencies use standards, which are open to the maximum extent feasible, to enable the organization and categorization of Government information— (i) in a way that is searchable electronically, including by searchable identifiers; (ii) in ways that are interoperable across agencies; and (iii) that are, as appropriate, consistent with the provisions under section 3602(f)(8) of title 44, United States Code; (B) defining categories of Government information which shall be required to be classified under the standards; and (C) determining priorities and developing schedules for the initial implementation of the standards by agencies. (3) MODIFICATION OF POLICIES.—After the submission of agency reports under paragraph (4), the Director shall modify Deadline. Policies. Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00019 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2918 the policies, as needed, in consultation with the Committee and interested parties. (4) AGENCY FUNCTIONS.—Each agency shall report annually to the Director, in the report established under section 202(g), on compliance of that agency with the policies issued under paragraph (2)(A). (e) PUBLIC ACCESS TO ELECTRONIC INFORMATION.— (1) COMMITTEE FUNCTIONS.—Not later than 2 years after the date of enactment of this Act, the Committee shall submit recommendations to the Director and the Archivist of the United States on— (A) the adoption by agencies", "quality": {"lang": "en", "gibberish_score": 0.0058, "char_len": 2404, "word_len": 350}, "dedupe": {"exact_hash": "c0800c677071bba0db2fee658b9071d7341806962f52f45d3ee624fb6be3e7f1", "simhash64": 10587204405701230598}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:778e4dca857e12ed89cbe1adb259b75671db6ac65731be1d217202f3d9dfcc7a", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 26, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "agency with the policies issued under paragraph (2)(A). (e) PUBLIC ACCESS TO ELECTRONIC INFORMATION.— (1) COMMITTEE FUNCTIONS.—Not later than 2 years after the date of enactment of this Act, the Committee shall submit recommendations to the Director and the Archivist of the United States on— (A) the adoption by agencies of policies and procedures to ensure that chapters 21, 25, 27, 29, and 31 of title 44, United States Code, are applied effectively and comprehensively to Government information on the Internet and to other electronic records; and (B) the imposition of timetables for the implementation of the policies and procedures by agencies. (2) FUNCTIONS OF THE ARCHIVIST.—Not later than 1 year after the submission of recommendations by the Committee under paragraph (1), the Archivist of the United States shall issue policies— (A) requiring the adoption by agencies of policies and procedures to ensure that chapters 21, 25, 27, 29, and 31 of title 44, United States Code, are applied effectively and comprehensively to Government information on the Internet and to other electronic records; and (B) imposing timetables for the implementation of the policies, procedures, and technologies by agencies. (3) MODIFICATION OF POLICIES.—After the submission of agency reports under paragraph (4), the Archivist of the United States shall modify the policies, as needed, in consultation with the Committee and interested parties. (4) AGENCY FUNCTIONS.—Each agency shall report annually to the Director, in the report established under section 202(g), on compliance of that agency with the policies issued under paragraph (2)(A). (f) AGENCY WEBSITES.— (1) STANDARDS FOR AGENCY WEBSITES.—Not later than 2 years after the effective date of this title, the Director shall promulgate guidance for agency websites that includes— (A) requirements that websites include direct links to— (i) descriptions of the mission and statutory authority of the agency; (ii) information made available to the public under subsections (a)(1) and (b) of section 552 of title 5, United States Code (commonly referred to as the ‘‘Freedom of Information Act’’); (iii) information about the organizational structure of the agency; and (iv) the strategic plan of the agency developed under section 306 of title", "quality": {"lang": "en", "gibberish_score": 0.0065, "char_len": 2291, "word_len": 350}, "dedupe": {"exact_hash": "08cc68143bfb7a33955fc6543b30e42085db356ec43eadee388d34fdc371f1fd", "simhash64": 10573711130285686790}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:68f359ad06a915bda6d1c631e3d134edbe0b6ce31cf2ba1b8c9b3c6432abda08", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 27, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "available to the public under subsections (a)(1) and (b) of section 552 of title 5, United States Code (commonly referred to as the ‘‘Freedom of Information Act’’); (iii) information about the organizational structure of the agency; and (iv) the strategic plan of the agency developed under section 306 of title 5, United States Code; and (B) minimum agency goals to assist public users to navigate agency websites, including— (i) speed of retrieval of search results; (ii) the relevance of the results; Deadline. Guidelines. Reports. Deadline. Policies. Deadline. Reports. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00020 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2919 (iii) tools to aggregate and disaggregate data; and (iv) security protocols to protect information. (2) AGENCY REQUIREMENTS.—(A) Not later than 2 years after the date of enactment of this Act, each agency shall— (i) consult with the Committee and solicit public comment; (ii) establish a process for determining which Government information the agency intends to make available and accessible to the public on the Internet and by other means; (iii) develop priorities and schedules for making Government information available and accessible; (iv) make such final determinations, priorities, and schedules available for public comment; (v) post such final determinations, priorities, and schedules on the Internet; and (vi) submit such final determinations, priorities, and schedules to the Director, in the report established under section 202(g). (B) Each agency shall update determinations, priorities, and schedules of the agency, as needed, after consulting with the Committee and soliciting public comment, if appropriate. (3) PUBLIC DOMAIN DIRECTORY OF PUBLIC FEDERAL GOVERNMENT WEBSITES.— (A) ESTABLISHMENT.—Not later than 2 years after the effective date of this title, the Director and each agency shall— (i) develop and establish a public domain directory of public Federal Government websites; and (ii) post the directory on the Internet with a link to the integrated Internet-based system established under section 204. (B) DEVELOPMENT.—With the assistance of each agency, the Director shall— (i) direct the development of the directory through a collaborative effort, including input from— (I) agency librarians; (II) information technology managers; (III) program", "quality": {"lang": "en", "gibberish_score": 0.0063, "char_len": 2376, "word_len": 350}, "dedupe": {"exact_hash": "6ce139982b563944389ed21537dde7da39d85af751ebd8f9e0e5b22bf7b49efe", "simhash64": 14113540506119143430}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d2b90aad7e1e2cde124c8732d6e8416400d086ff51c9600a193d87e392daaf25", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 28, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "the directory on the Internet with a link to the integrated Internet-based system established under section 204. (B) DEVELOPMENT.—With the assistance of each agency, the Director shall— (i) direct the development of the directory through a collaborative effort, including input from— (I) agency librarians; (II) information technology managers; (III) program managers; (IV) records managers; (V) Federal depository librarians; and (VI) other interested parties; and (ii) develop a public domain taxonomy of subjects used to review and categorize public Federal Government websites. (C) UPDATE.—With the assistance of each agency, the Administrator of the Office of Electronic Government shall— (i) update the directory as necessary, but not less than every 6 months; and (ii) solicit interested persons for improvements to the directory. (g) ACCESS TO FEDERALLY FUNDED RESEARCH AND DEVELOPMENT.— (1) DEVELOPMENT AND MAINTENANCE OF GOVERNMENTWIDE REPOSITORY AND WEBSITE.— Deadline. Deadline. Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00021 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2920 (A) REPOSITORY AND WEBSITE.—The Director of the Office of Management and Budget (or the Director’s delegate), in consultation with the Director of the Office of Science and Technology Policy and other relevant agencies, shall ensure the development and maintenance of— (i) a repository that fully integrates, to the maximum extent feasible, information about research and development funded by the Federal Government, and the repository shall— (I) include information about research and development funded by the Federal Government, consistent with any relevant protections for the information under section 552 of title 5, United States Code, and performed by— (aa) institutions not a part of the Federal Government, including State, local, and foreign governments; industrial firms; educational institutions; not-for-profit organizations; federally funded research and development centers; and private individuals; and (bb) entities of the Federal Government, including research and development laboratories, centers, and offices; and (II) integrate information about each separate research and development task or award, including— (aa) the dates upon which the task or award is expected to start and end; (bb) a brief summary describing the objective and the scientific and technical focus of the task or award; (cc)", "quality": {"lang": "en", "gibberish_score": 0.0061, "char_len": 2446, "word_len": 350}, "dedupe": {"exact_hash": "3e1ff034c56e015b3ac8c5e85a47ef3356f8a5a0057d0921093c360ab5135a46", "simhash64": 10655602825545221254}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:627daf49a0e7039f5e4d9d183dd2cc4a44b5bad09bb709e48e26f157fa7bada0", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 29, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "and offices; and (II) integrate information about each separate research and development task or award, including— (aa) the dates upon which the task or award is expected to start and end; (bb) a brief summary describing the objective and the scientific and technical focus of the task or award; (cc) the entity or institution performing the task or award and its contact information; (dd) the total amount of Federal funds expected to be provided to the task or award over its lifetime and the amount of funds expected to be provided in each fiscal year in which the work of the task or award is ongoing; (ee) any restrictions attached to the task or award that would prevent the sharing with the general public of any or all of the information required by this subsection, and the reasons for such restrictions; and (ff) such other information as may be determined to be appropriate; and (ii) 1 or more websites upon which all or part of the repository of Federal research and development shall be made available to and searchable by Federal agencies and non-Federal entities, including the general public, to facilitate— (I) the coordination of Federal research and development activities; (II) collaboration among those conducting Federal research and development; Public information. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00022 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2921 (III) the transfer of technology among Federal agencies and between Federal agencies and nonFederal entities; and (IV) access by policymakers and the public to information concerning Federal research and development activities. (B) OVERSIGHT.—The Director of the Office of Management and Budget shall issue any guidance determined necessary to ensure that agencies provide all information requested under this subsection. (2) AGENCY FUNCTIONS.—Any agency that funds Federal research and development under this subsection shall provide the information required to populate the repository in the manner prescribed by the Director of the Office of Management and Budget. (3) COMMITTEE FUNCTIONS.—Not later than 18 months after the date of enactment of this Act, working with the Director of the Office of Science and", "quality": {"lang": "en", "gibberish_score": 0.0031, "char_len": 2227, "word_len": 350}, "dedupe": {"exact_hash": "d990d39a40e7597b966409a81e5a759254020c121d4de87be081a3087022e90c", "simhash64": 11234315308439471238}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:cf314ffc53823485242a66c6db9fa65fa4e24aa322c77ea122585b821707fda8", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 30, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "this subsection shall provide the information required to populate the repository in the manner prescribed by the Director of the Office of Management and Budget. (3) COMMITTEE FUNCTIONS.—Not later than 18 months after the date of enactment of this Act, working with the Director of the Office of Science and Technology Policy, and after consultation with interested parties, the Committee shall submit recommendations to the Director on— (A) policies to improve agency reporting of information for the repository established under this subsection; and (B) policies to improve dissemination of the results of research performed by Federal agencies and federally funded research and development centers. (4) FUNCTIONS OF THE DIRECTOR.—After submission of recommendations by the Committee under paragraph (3), the Director shall report on the recommendations of the Committee and Director to Congress, in the E-Government report under section 3606 of title 44 (as added by this Act). (5) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated for the development, maintenance, and operation of the Governmentwide repository and website under this subsection— (A) $2,000,000 in each of the fiscal years 2003 through 2005; and (B) such sums as are necessary in each of the fiscal years 2006 and 2007. SEC. 208. PRIVACY PROVISIONS. (a) PURPOSE.—The purpose of this section is to ensure sufficient protections for the privacy of personal information as agencies implement citizen-centered electronic Government. (b) PRIVACY IMPACT ASSESSMENTS.— (1) RESPONSIBILITIES OF AGENCIES.— (A) IN GENERAL.—An agency shall take actions described under subparagraph (B) before— (i) developing or procuring information technology that collects, maintains, or disseminates information that is in an identifiable form; or (ii) initiating a new collection of information that— (I) will be collected, maintained, or disseminated using information technology; and (II) includes any information in an identifiable form permitting the physical or online contacting of a specific individual, if identical questions have been posed to, or identical reporting requirements 44 USC 3501 note. Reports. Deadline. Guidelines. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00023 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2922 imposed on, 10 or more persons, other than agencies, instrumentalities, or", "quality": {"lang": "en", "gibberish_score": 0.0058, "char_len": 2410, "word_len": 350}, "dedupe": {"exact_hash": "226abd0a3b3ffc02d625f44613a29b9abd558ad68799ed541a7dd64979ed4581", "simhash64": 11235723782835503238}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:497c99bbf861136039cffc33545d16b94ede0c8bc8e5d0fc076c1102150877f0", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 31, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "individual, if identical questions have been posed to, or identical reporting requirements 44 USC 3501 note. Reports. Deadline. Guidelines. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00023 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2922 imposed on, 10 or more persons, other than agencies, instrumentalities, or employees of the Federal Government. (B) AGENCY ACTIVITIES.—To the extent required under subparagraph (A), each agency shall— (i) conduct a privacy impact assessment; (ii) ensure the review of the privacy impact assessment by the Chief Information Officer, or equivalent official, as determined by the head of the agency; and (iii) if practicable, after completion of the review under clause (ii), make the privacy impact assessment publicly available through the website of the agency, publication in the Federal Register, or other means. (C) SENSITIVE INFORMATION.—Subparagraph (B)(iii) may be modified or waived for security reasons, or to protect classified, sensitive, or private information contained in an assessment. (D) COPY TO DIRECTOR.—Agencies shall provide the Director with a copy of the privacy impact assessment for each system for which funding is requested. (2) CONTENTS OF A PRIVACY IMPACT ASSESSMENT.— (A) IN GENERAL.—The Director shall issue guidance to agencies specifying the required contents of a privacy impact assessment. (B) GUIDANCE.—The guidance shall— (i) ensure that a privacy impact assessment is commensurate with the size of the information system being assessed, the sensitivity of information that is in an identifiable form in that system, and the risk of harm from unauthorized release of that information; and (ii) require that a privacy impact assessment address— (I) what information is to be collected; (II) why the information is being collected; (III) the intended use of the agency of the information; (IV) with whom the information will be shared; (V) what notice or opportunities for consent would be provided to individuals regarding what information is collected and how that information is shared; (VI) how the information will be secured; and (VII) whether a system of records is being created under section 552a of title 5, United States Code, (commonly referred to as the ‘‘Privacy Act’’). (3) RESPONSIBILITIES OF", "quality": {"lang": "en", "gibberish_score": 0.0065, "char_len": 2307, "word_len": 350}, "dedupe": {"exact_hash": "68843210a2009e2aaeed6059376d8ce5d353e4109747db6d601263b69775b7e9", "simhash64": 11207575254337683462}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ca99b2805746e707ca0c9d1aad152cb3f2641bed77efa7fb7577e3c679a832b3", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 32, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "provided to individuals regarding what information is collected and how that information is shared; (VI) how the information will be secured; and (VII) whether a system of records is being created under section 552a of title 5, United States Code, (commonly referred to as the ‘‘Privacy Act’’). (3) RESPONSIBILITIES OF THE DIRECTOR.—The Director shall— (A) develop policies and guidelines for agencies on the conduct of privacy impact assessments; (B) oversee the implementation of the privacy impact assessment process throughout the Government; and (C) require agencies to conduct privacy impact assessments of existing information systems or ongoing collections of information that is in an identifiable form as the Director determines appropriate. (c) PRIVACY PROTECTIONS ON AGENCY WEBSITES.— Guidelines. Public information. Federal Register, publication. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00024 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2923 (1) PRIVACY POLICIES ON WEBSITES.— (A) GUIDELINES FOR NOTICES.—The Director shall develop guidance for privacy notices on agency websites used by the public. (B) CONTENTS.—The guidance shall require that a privacy notice address, consistent with section 552a of title 5, United States Code— (i) what information is to be collected; (ii) why the information is being collected; (iii) the intended use of the agency of the information; (iv) with whom the information will be shared; (v) what notice or opportunities for consent would be provided to individuals regarding what information is collected and how that information is shared; (vi) how the information will be secured; and (vii) the rights of the individual under section 552a of title 5, United States Code (commonly referred to as the ‘‘Privacy Act’’), and other laws relevant to the protection of the privacy of an individual. (2) PRIVACY POLICIES IN MACHINE-READABLE FORMATS.— The Director shall issue guidance requiring agencies to translate privacy policies into a standardized machine-readable format. (d) DEFINITION.—In this section, the term ‘‘identifiable form’’ means any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means. SEC. 209. FEDERAL INFORMATION TECHNOLOGY WORKFORCE DEVELOPMENT. (a) PURPOSE.—The purpose", "quality": {"lang": "en", "gibberish_score": 0.0101, "char_len": 2381, "word_len": 350}, "dedupe": {"exact_hash": "b336f21b097ad13be2817e9b618d5557a884851a740b3ed761aefe217f177d63", "simhash64": 11225309208663932934}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:fbe32c1ddd097c30d63eae33a1e754df66fc5c5f5afd591d8dfc372b484899d0", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 33, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "into a standardized machine-readable format. (d) DEFINITION.—In this section, the term ‘‘identifiable form’’ means any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means. SEC. 209. FEDERAL INFORMATION TECHNOLOGY WORKFORCE DEVELOPMENT. (a) PURPOSE.—The purpose of this section is to improve the skills of the Federal workforce in using information technology to deliver Government information and services. (b) WORKFORCE DEVELOPMENT.— (1) IN GENERAL.—In consultation with the Director of the Office of Management and Budget, the Chief Information Officers Council, and the Administrator of General Services, the Director of the Office of Personnel Management shall— (A) analyze, on an ongoing basis, the personnel needs of the Federal Government related to information technology and information resource management; (B) identify where current information technology and information resource management training do not satisfy the personnel needs described in subparagraph (A); (C) oversee the development of curricula, training methods, and training priorities that correspond to the projected personnel needs of the Federal Government related to information technology and information resource management; and (D) assess the training of Federal employees in information technology disciplines in order to ensure that the information resource management needs of the Federal Government are addressed. (2) INFORMATION TECHNOLOGY TRAINING PROGRAMS.—The head of each Executive agency, after consultation with the Director of the Office of Personnel Management, the Chief 44 USC 3501 note. Guidelines. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00025 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2924 Information Officers Council, and the Administrator of General Services, shall establish and operate information technology training programs consistent with the requirements of this subsection. Such programs shall— (A) have curricula covering a broad range of information technology disciplines corresponding to the specific information technology and information resource management needs of the agency involved; (B) be developed and applied according to rigorous standards; and (C) be designed to maximize efficiency, through the use of self-paced courses, online courses, on-the-job training, and the use of remote instructors, wherever such features can be applied without reducing", "quality": {"lang": "en", "gibberish_score": 0.0052, "char_len": 2524, "word_len": 350}, "dedupe": {"exact_hash": "a1f28a76dbad03c4969822212232a51c86ca47795494eeab0f407f5b63cdf184", "simhash64": 10649130000057139334}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:3c8dc86550b56f3a3085261424936ad6e025572dbace1ed0fa298090799906c4", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 34, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "technology and information resource management needs of the agency involved; (B) be developed and applied according to rigorous standards; and (C) be designed to maximize efficiency, through the use of self-paced courses, online courses, on-the-job training, and the use of remote instructors, wherever such features can be applied without reducing the effectiveness of the training or negatively impacting academic standards. (3) GOVERNMENTWIDE POLICIES AND EVALUATION.—The Director of the Office of Personnel Management, in coordination with the Director of the Office of Management and Budget, shall issue policies to promote the development of performance standards for training and uniform implementation of this subsection by Executive agencies, with due regard for differences in program requirements among agencies that may be appropriate and warranted in view of the agency mission. The Director of the Office of Personnel Management shall evaluate the implementation of the provisions of this subsection by Executive agencies. (4) CHIEF INFORMATION OFFICER AUTHORITIES AND RESPONSIBILITIES.—Subject to the authority, direction, and control of the head of an Executive agency, the chief information officer of such agency shall carry out all powers, functions, and duties of the head of the agency with respect to implementation of this subsection. The chief information officer shall ensure that the policies of the agency head established in accordance with this subsection are implemented throughout the agency. (5) INFORMATION TECHNOLOGY TRAINING REPORTING.—The Director of the Office of Management and Budget shall ensure that the heads of Executive agencies collect and maintain standardized information on the information technology and information resources management workforce related to the implementation of this subsection. (6) AUTHORITY TO DETAIL EMPLOYEES TO NON-FEDERAL EMPLOYERS.—In carrying out the preceding provisions of this subsection, the Director of the Office of Personnel Management may provide for a program under which a Federal employee may be detailed to a non-Federal employer. The Director of the Office of Personnel Management shall prescribe regulations for such program, including the conditions for service and duties as the Director considers necessary. (7) COORDINATION PROVISION.—An assignment described in section 3703 of title 5, United States Code, may not be made unless a program", "quality": {"lang": "en", "gibberish_score": 0.0021, "char_len": 2432, "word_len": 350}, "dedupe": {"exact_hash": "a15a97d85ee37294acfc60d881297d04729e52a0c30d7ee2ab71a18066f767a1", "simhash64": 11234316476640247942}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:b28bd4b02e6995849f8dcd1e4012646453a78c8799bb980ae4dca097b45619ac", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 35, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "a non-Federal employer. The Director of the Office of Personnel Management shall prescribe regulations for such program, including the conditions for service and duties as the Director considers necessary. (7) COORDINATION PROVISION.—An assignment described in section 3703 of title 5, United States Code, may not be made unless a program under paragraph (6) is established, and the assignment is made in accordance with the requirements of such program. (8) EMPLOYEE PARTICIPATION.—Subject to information resource management needs and the limitations imposed by resource needs in other occupational areas, and consistent with their overall workforce development strategies, agencies shall Regulations. Records. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00026 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2925 encourage employees to participate in occupational information technology training. (9) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated to the Office of Personnel Management for the implementation of this subsection, $15,000,000 in fiscal year 2003, and such sums as are necessary for each fiscal year thereafter. (10) EXECUTIVE AGENCY DEFINED.—For purposes of this subsection, the term ‘‘Executive agency’’ has the meaning given the term ‘‘agency’’ under section 3701 of title 5, United States Code (as added by subsection (c)). (c) INFORMATION TECHNOLOGY EXCHANGE PROGRAM.— (1) IN GENERAL.—Subpart B of part III of title 5, United States Code, is amended by adding at the end the following: ‘‘CHAPTER 37—INFORMATION TECHNOLOGY EXCHANGE PROGRAM ‘‘Sec. ‘‘3701. Definitions. ‘‘3702. General provisions. ‘‘3703. Assignment of employees to private sector organizations. ‘‘3704. Assignment of employees from private sector organizations. ‘‘3705. Application to Office of the Chief Technology Officer of the District of Columbia. ‘‘3706. Reporting requirement. ‘‘3707. Regulations. ‘‘§ 3701. Definitions ‘‘For purposes of this chapter— ‘‘(1) the term ‘agency’ means an Executive agency, but does not include the General Accounting Office; and ‘‘(2) the term ‘detail’ means— ‘‘(A) the assignment or loan of an employee of an agency to a private sector organization without a change of position from the agency that employs the individual, or ‘‘(B) the assignment or loan of an employee of a private sector organization to an agency without a change of", "quality": {"lang": "en", "gibberish_score": 0.0229, "char_len": 2407, "word_len": 350}, "dedupe": {"exact_hash": "3c068b80f8496646c3a2b7495df20a8116960062a5f36ba169530305bc06f594", "simhash64": 11208702185071543302}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a7d1dcfa93826fa1d64dd7e7382de28fd494d773d4f7e9ff98d3f74a01a5b928", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 36, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "means— ‘‘(A) the assignment or loan of an employee of an agency to a private sector organization without a change of position from the agency that employs the individual, or ‘‘(B) the assignment or loan of an employee of a private sector organization to an agency without a change of position from the private sector organization that employs the individual, whichever is appropriate in the context in which such term is used. ‘‘§ 3702. General provisions ‘‘(a) ASSIGNMENT AUTHORITY.—On request from or with the agreement of a private sector organization, and with the consent of the employee concerned, the head of an agency may arrange for the assignment of an employee of the agency to a private sector organization or an employee of a private sector organization to the agency. An eligible employee is an individual who— ‘‘(1) works in the field of information technology management; ‘‘(2) is considered an exceptional performer by the individual’s current employer; and ‘‘(3) is expected to assume increased information technology management responsibilities in the future. An employee of an agency shall be eligible to participate in this program only if the employee is employed at the GS–11 level Eligibility. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00027 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2926 or above (or equivalent) and is serving under a career or careerconditional appointment or an appointment of equivalent tenure in the excepted service, and applicable requirements of section 209(b) of the E-Government Act of 2002 are met with respect to the proposed assignment of such employee. ‘‘(b) AGREEMENTS.—Each agency that exercises its authority under this chapter shall provide for a written agreement between the agency and the employee concerned regarding the terms and conditions of the employee’s assignment. In the case of an employee of the agency, the agreement shall— ‘‘(1) require the employee to serve in the civil service, upon completion of the assignment, for a period equal to the length of the assignment; and ‘‘(2) provide that, in the event the employee fails to carry out the agreement (except for good and sufficient", "quality": {"lang": "en", "gibberish_score": 0.0142, "char_len": 2188, "word_len": 350}, "dedupe": {"exact_hash": "53072236bf3ac664dfee9dbe858d55e133c56758a835a332fcbbd26e2e7ea337", "simhash64": 11217427978067738886}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ac694afd3423da3fce632b4602dafe41365e3cf89b0ea65dee76dffd285e092e", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 37, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "the agency, the agreement shall— ‘‘(1) require the employee to serve in the civil service, upon completion of the assignment, for a period equal to the length of the assignment; and ‘‘(2) provide that, in the event the employee fails to carry out the agreement (except for good and sufficient reason, as determined by the head of the agency from which assigned) the employee shall be liable to the United States for payment of all expenses of the assignment. An amount under paragraph (2) shall be treated as a debt due the United States. ‘‘(c) TERMINATION.—Assignments may be terminated by the agency or private sector organization concerned for any reason at any time. ‘‘(d) DURATION.—Assignments under this chapter shall be for a period of between 3 months and 1 year, and may be extended in 3-month increments for a total of not more than 1 additional year, except that no assignment under this chapter may commence after the end of the 5-year period beginning on the date of the enactment of this chapter. ‘‘(e) ASSISTANCE.—The Chief Information Officers Council, by agreement with the Office of Personnel Management, may assist in the administration of this chapter, including by maintaining lists of potential candidates for assignment under this chapter, establishing mentoring relationships for the benefit of individuals who are given assignments under this chapter, and publicizing the program. ‘‘(f) CONSIDERATIONS.—In exercising any authority under this chapter, an agency shall take into consideration— ‘‘(1) the need to ensure that small business concerns are appropriately represented with respect to the assignments described in sections 3703 and 3704, respectively; and ‘‘(2) how assignments described in section 3703 might best be used to help meet the needs of the agency for the training of employees in information technology management. ‘‘§ 3703. Assignment of employees to private sector organizations ‘‘(a) IN GENERAL.—An employee of an agency assigned to a private sector organization under this chapter is deemed, during the period of the assignment, to be on detail to a regular work assignment in his agency. ‘‘(b) COORDINATION WITH CHAPTER 81.—Notwithstanding any other provision of law, an employee", "quality": {"lang": "en", "gibberish_score": 0.0139, "char_len": 2229, "word_len": 350}, "dedupe": {"exact_hash": "e23a410609bc9c42c42099e078a3788248d29817cb118fdd824e243ab6f3d89e", "simhash64": 11220804509557440838}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:5902175831842800310cf52b831bcf2793ede4af4691561a92e01734a6026d66", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 38, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "sector organizations ‘‘(a) IN GENERAL.—An employee of an agency assigned to a private sector organization under this chapter is deemed, during the period of the assignment, to be on detail to a regular work assignment in his agency. ‘‘(b) COORDINATION WITH CHAPTER 81.—Notwithstanding any other provision of law, an employee of an agency assigned to a private sector organization under this chapter is entitled to retain coverage, rights, and benefits under subchapter I of chapter 81, and employment during the assignment is deemed employment by the United States, except that, if the employee or the employee’s VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00028 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2927 dependents receive from the private sector organization any payment under an insurance policy for which the premium is wholly paid by the private sector organization, or other benefit of any kind on account of the same injury or death, then, the amount of such payment or benefit shall be credited against any compensation otherwise payable under subchapter I of chapter 81. ‘‘(c) REIMBURSEMENTS.—The assignment of an employee to a private sector organization under this chapter may be made with or without reimbursement by the private sector organization for the travel and transportation expenses to or from the place of assignment, subject to the same terms and conditions as apply with respect to an employee of a Federal agency or a State or local government under section 3375, and for the pay, or a part thereof, of the employee during assignment. Any reimbursements shall be credited to the appropriation of the agency used for paying the travel and transportation expenses or pay. ‘‘(d) TORT LIABILITY; SUPERVISION.—The Federal Tort Claims Act and any other Federal tort liability statute apply to an employee of an agency assigned to a private sector organization under this chapter. The supervision of the duties of an employee of an agency so assigned to a private sector organization may be governed by an agreement between the agency and the organization. ‘‘(e) SMALL BUSINESS CONCERNS.— ‘‘(1) IN GENERAL.—The head of each agency shall", "quality": {"lang": "en", "gibberish_score": 0.0096, "char_len": 2187, "word_len": 350}, "dedupe": {"exact_hash": "a703b738f5db7fd14950794c1a106c52680fa9c36fecdf116006dfcdd8902b4b", "simhash64": 11217426878556111942}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0a64dcf00f587d0142dacb7c4175e874658a31afc4cf218e9ff963e08e4efacb", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 39, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "to a private sector organization under this chapter. The supervision of the duties of an employee of an agency so assigned to a private sector organization may be governed by an agreement between the agency and the organization. ‘‘(e) SMALL BUSINESS CONCERNS.— ‘‘(1) IN GENERAL.—The head of each agency shall take such actions as may be necessary to ensure that, of the assignments made under this chapter from such agency to private sector organizations in each year, at least 20 percent are to small business concerns. ‘‘(2) DEFINITIONS.—For purposes of this subsection— ‘‘(A) the term ‘small business concern’ means a business concern that satisfies the definitions and standards specified by the Administrator of the Small Business Administration under section 3(a)(2) of the Small Business Act (as from time to time amended by the Administrator); ‘‘(B) the term ‘year’ refers to the 12-month period beginning on the date of the enactment of this chapter, and each succeeding 12-month period in which any assignments under this chapter may be made; and ‘‘(C) the assignments ‘made’ in a year are those commencing in such year. ‘‘(3) REPORTING REQUIREMENT.—An agency which fails to comply with paragraph (1) in a year shall, within 90 days after the end of such year, submit a report to the Committees on Government Reform and Small Business of the House of Representatives and the Committees on Governmental Affairs and Small Business of the Senate. The report shall include— ‘‘(A) the total number of assignments made under this chapter from such agency to private sector organizations in the year; ‘‘(B) of that total number, the number (and percentage) made to small business concerns; and ‘‘(C) the reasons for the agency’s noncompliance with paragraph (1). ‘‘(4) EXCLUSION.—This subsection shall not apply to an agency in any year in which it makes fewer than 5 assignments under this chapter to private sector organizations. Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00029 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2928 ‘‘§ 3704. Assignment of employees from private sector organizations ‘‘(a) IN GENERAL.—An employee of a private sector organization", "quality": {"lang": "en", "gibberish_score": 0.02, "char_len": 2200, "word_len": 350}, "dedupe": {"exact_hash": "eedd74281811902bd8f137cd361c4a7b600941815db512edc198bdf078c08425", "simhash64": 11218272402998149190}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8b0841aff5082a195ed522d7506635fbf79326bf13dffd18a1537626f929de4d", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 41, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "for any contribution of the private sector organization to employee benefit systems. ‘‘(c) COORDINATION WITH CHAPTER 81.—An employee of a private sector organization assigned to an agency under this chapter who suffers disability or dies as a result of personal injury sustained while performing duties during the assignment shall be treated, for the purpose of subchapter I of chapter 81, as an employee as defined by section 8101 who had sustained the injury in the performance of duty, except that, if the employee or the employee’s dependents receive from the private sector organization any payment under an insurance policy for which the premium is wholly paid by the private sector organization, or other benefit of any kind on account of the same injury or death, then, the amount of such payment or benefit shall be credited against any compensation otherwise payable under subchapter I of chapter 81. ‘‘(d) PROHIBITION AGAINST CHARGING CERTAIN COSTS TO THE FEDERAL GOVERNMENT.—A private sector organization may not charge the Federal Government, as direct or indirect costs under a Federal contract, the costs of pay or benefits paid by the organization to an employee assigned to an agency under this chapter for the period of the assignment. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00030 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2929 ‘‘§ 3705. Application to Office of the Chief Technology Officer of the District of Columbia ‘‘(a) IN GENERAL.—The Chief Technology Officer of the District of Columbia may arrange for the assignment of an employee of the Office of the Chief Technology Officer to a private sector organization, or an employee of a private sector organization to such Office, in the same manner as the head of an agency under this chapter. ‘‘(b) TERMS AND CONDITIONS.—An assignment made pursuant to subsection (a) shall be subject to the same terms and conditions as an assignment made by the head of an agency under this chapter, except that in applying such terms and conditions to an assignment made pursuant to subsection (a), any reference in this chapter to a provision of law", "quality": {"lang": "en", "gibberish_score": 0.0084, "char_len": 2142, "word_len": 350}, "dedupe": {"exact_hash": "0432deb87cc412076affa970192a748a1151f61bdff69af5d7a5b1f9b6eada52", "simhash64": 11217426809837158406}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1fd5e2636d274de1d6dd92711d15e18eeacd073287cf001facfa6784eeb6f8ce", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 42, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "subsection (a) shall be subject to the same terms and conditions as an assignment made by the head of an agency under this chapter, except that in applying such terms and conditions to an assignment made pursuant to subsection (a), any reference in this chapter to a provision of law or regulation of the United States shall be deemed to be a reference to the applicable provision of law or regulation of the District of Columbia, including the applicable provisions of the District of Columbia Government Comprehensive Merit Personnel Act of 1978 (sec. 1–601.01 et seq., D.C. Official Code) and section 601 of the District of Columbia Campaign Finance Reform and Conflict of Interest Act (sec. 1– 1106.01, D.C. Official Code). ‘‘(c) DEFINITION.—For purposes of this section, the term ‘Office of the Chief Technology Officer’ means the office established in the executive branch of the government of the District of Columbia under the Office of the Chief Technology Officer Establishment Act of 1998 (sec. 1–1401 et seq., D.C. Official Code). ‘‘§ 3706. Reporting requirement ‘‘(a) IN GENERAL.—The Office of Personnel Management shall, not later than April 30 and October 31 of each year, prepare and submit to the Committee on Government Reform of the House of Representatives and the Committee on Governmental Affairs of the Senate a semiannual report summarizing the operation of this chapter during the immediately preceding 6-month period ending on March 31 and September 30, respectively. ‘‘(b) CONTENT.—Each report shall include, with respect to the 6-month period to which such report relates— ‘‘(1) the total number of individuals assigned to, and the total number of individuals assigned from, each agency during such period; ‘‘(2) a brief description of each assignment included under paragraph (1), including— ‘‘(A) the name of the assigned individual, as well as the private sector organization and the agency (including the specific bureau or other agency component) to or from which such individual was assigned; ‘‘(B) the respective positions to and from which the individual was assigned, including the duties and responsibilities and the pay grade or level associated with each; and ‘‘(C) the", "quality": {"lang": "en", "gibberish_score": 0.0131, "char_len": 2209, "word_len": 350}, "dedupe": {"exact_hash": "90c47be1a4227b88be4efaf474a4260c9468fa67a8e0bbf0349e4ba5aee6b482", "simhash64": 10646032675802300678}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:21b81422f2f89cca18cdc88af2c43e20b83627f25b61140f930106f603e280de", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 44, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "(1) ONE-YEAR RESTRICTION ON CERTAIN COMMUNICATIONS.— Section 207(c)(2)(A) of title 18, United States Code, is amended— (A) by striking ‘‘or’’ at the end of clause (iii); (B) by striking the period at the end of clause (iv) and inserting ‘‘; or’’; and (C) by adding at the end the following: ‘‘(v) assigned from a private sector organization to an agency under chapter 37 of title 5.’’. (2) DISCLOSURE OF CONFIDENTIAL INFORMATION.—Section 1905 of title 18, United States Code, is amended by inserting ‘‘or being an employee of a private sector organization who is or was assigned to an agency under chapter 37 of title 5,’’ after ‘‘(15 U.S.C. 1311–1314),’’. (3) CONTRACT ADVICE.—Section 207 of title 18, United States Code, is amended by adding at the end the following: ‘‘(l) CONTRACT ADVICE BY FORMER DETAILS.—Whoever, being an employee of a private sector organization assigned to an agency under chapter 37 of title 5, within one year after the end of that assignment, knowingly represents or aids, counsels, or assists in representing any other person (except the United States) in connection with any contract with that agency shall be punished as provided in section 216 of this title.’’. (4) RESTRICTION ON DISCLOSURE OF PROCUREMENT INFORMATION.—Section 27 of the Office of Federal Procurement Policy Act (41 U.S.C. 423) is amended in subsection (a)(1) by adding at the end the following new sentence: ‘‘In the case of an employee of a private sector organization assigned to an agency under chapter 37 of title 5, United States Code, in addition to the restriction in the preceding sentence, such employee shall not, other than as provided by law, knowingly disclose contractor bid or proposal information or source selection Deadline. Public information. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00032 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2931 information during the three-year period after the end of the assignment of such employee.’’. (e) REPORT ON EXISTING EXCHANGE PROGRAMS.— (1) EXCHANGE PROGRAM DEFINED.—For purposes of this subsection, the term ‘‘exchange program’’ means an executive exchange program, the program under subchapter VI of chapter 33", "quality": {"lang": "en", "gibberish_score": 0.0195, "char_len": 2201, "word_len": 350}, "dedupe": {"exact_hash": "a3fd99bcf8b289bd5c89182dd14d104487eb312ac5c22a303b2204c495666105", "simhash64": 11217427909349770262}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8b6930a2f9384addfbf21dbcff2bc19c4cbb3a233d6ef059dc47204e2b1fec9b", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 45, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2931 information during the three-year period after the end of the assignment of such employee.’’. (e) REPORT ON EXISTING EXCHANGE PROGRAMS.— (1) EXCHANGE PROGRAM DEFINED.—For purposes of this subsection, the term ‘‘exchange program’’ means an executive exchange program, the program under subchapter VI of chapter 33 of title 5, United States Code, and any other program which allows for— (A) the assignment of employees of the Federal Government to non-Federal employers; (B) the assignment of employees of non-Federal employers to the Federal Government; or (C) both. (2) REPORTING REQUIREMENT.—Not later than 1 year after the date of the enactment of this Act, the Office of Personnel Management shall prepare and submit to the Committee on Government Reform of the House of Representatives and the Committee on Governmental Affairs of the Senate a report identifying all existing exchange programs. (3) SPECIFIC INFORMATION.—The report shall, for each such program, include— (A) a brief description of the program, including its size, eligibility requirements, and terms or conditions for participation; (B) specific citation to the law or other authority under which the program is established; (C) the names of persons to contact for more information, and how they may be reached; and (D) any other information which the Office considers appropriate. (f) REPORT ON THE ESTABLISHMENT OF A GOVERNMENTWIDE INFORMATION TECHNOLOGY TRAINING PROGRAM.— (1) IN GENERAL.—Not later January 1, 2003, the Office of Personnel Management, in consultation with the Chief Information Officers Council and the Administrator of General Services, shall review and submit to the Committee on Government Reform of the House of Representatives and the Committee on Governmental Affairs of the Senate a written report on the following: (A) The adequacy of any existing information technology training programs available to Federal employees on a Governmentwide basis. (B)(i) If one or more such programs already exist, recommendations as to how they might be improved. (ii) If no such program yet exists, recommendations as to how such a program might be designed and established. (C) With respect to any recommendations under subparagraph (B), how the program under chapter 37 of title", "quality": {"lang": "en", "gibberish_score": 0.0069, "char_len": 2306, "word_len": 350}, "dedupe": {"exact_hash": "1b82bb4dd6bbc4c55d569be910e55adde8cb30b177ccca970c7cf50b306c4859", "simhash64": 10563280132194276358}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:fdacf5a66db1528f073e6ed27147edb152ba1e1c91756ef646a645b5b999623c", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 46, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "or more such programs already exist, recommendations as to how they might be improved. (ii) If no such program yet exists, recommendations as to how such a program might be designed and established. (C) With respect to any recommendations under subparagraph (B), how the program under chapter 37 of title 5, United States Code, might be used to help carry them out. (2) COST ESTIMATE.—The report shall, for any recommended program (or improvements) under paragraph (1)(B), include the estimated costs associated with the implementation and operation of such program as so established (or estimated difference in costs of any such program as so improved). (g) TECHNICAL AND CONFORMING AMENDMENTS.— Deadline. Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00033 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2932 (1) AMENDMENTS TO TITLE 5, UNITED STATES CODE.—Title 5, United States Code, is amended— (A) in section 3111, by adding at the end the following: ‘‘(d) Notwithstanding section 1342 of title 31, the head of an agency may accept voluntary service for the United States under chapter 37 of this title and regulations of the Office of Personnel Management.’’; (B) in section 4108, by striking subsection (d); and (C) in section 7353(b), by adding at the end the following: ‘‘(4) Nothing in this section precludes an employee of a private sector organization, while assigned to an agency under chapter 37, from continuing to receive pay and benefits from such organization in accordance with such chapter.’’. (2) AMENDMENT TO TITLE 18, UNITED STATES CODE.—Section 209 of title 18, United States Code, is amended by adding at the end the following: ‘‘(g)(1) This section does not prohibit an employee of a private sector organization, while assigned to an agency under chapter 37 of title 5, from continuing to receive pay and benefits from such organization in accordance with such chapter. ‘‘(2) For purposes of this subsection, the term ‘agency’ means an agency (as defined by section 3701 of title 5) and the Office of the Chief Technology Officer of the District of Columbia.’’. (3) OTHER AMENDMENTS.—Section 125(c)(1) of Public Law 100–238", "quality": {"lang": "en", "gibberish_score": 0.0115, "char_len": 2180, "word_len": 350}, "dedupe": {"exact_hash": "35953db28a3d68ff96035c46192afa1e1cb80ca73f263a6c7dddaada7213aa79", "simhash64": 11217444402560619542}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:25c66f391734fc8f524b98c09b56956b7f70e240d918cf7d297bd3995c31d489", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 48, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "019139 PO 00347 Frm 00034 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2933 competitively at a fair and reasonable price if the contract is limited in duration to a period of five years or less; and ‘‘(ii) usage of the information technology to be acquired is likely to continue for a period of time sufficient to generate reasonable benefit for the government. ‘‘(3) Contracts awarded pursuant to the authority of this section shall, to the maximum extent practicable, be performance-based contracts that identify objective outcomes and contain performance standards that will be used to measure achievement and milestones that must be met before payment is made. ‘‘(4) Contracts awarded pursuant to the authority of this section shall include a provision containing a quantifiable baseline that is to be the basis upon which a savings share ratio is established that governs the amount of payment a contractor is to receive under the contract. Before commencement of performance of such a contract, the senior procurement executive of the agency shall determine in writing that the terms of the provision are quantifiable and will likely yield value to the Government. ‘‘(5)(A) The head of the agency may retain savings realized through the use of a share-in-savings contract under this section that are in excess of the total amount of savings paid to the contractor under the contract, but may not retain any portion of such savings that is attributable to a decrease in the number of civilian employees of the Federal Government performing the function. Except as provided in subparagraph (B), savings shall be credited to the appropriation or fund against which charges were made to carry out the contract and shall be used for information technology. ‘‘(B) Amounts retained by the agency under this subsection shall— ‘‘(i) without further appropriation, remain available until expended; and ‘‘(ii) be applied first to fund any contingent liabilities associated with share-in-savings procurements that are not fully funded. ‘‘(b) CANCELLATION AND TERMINATION.—(1) If funds are not made available for the continuation of a share-in-savings contract entered into under this section in a subsequent fiscal year, the contract", "quality": {"lang": "en", "gibberish_score": 0.009, "char_len": 2233, "word_len": 350}, "dedupe": {"exact_hash": "5f1cefea17ce10791208fb73d001486282267d168d858fababd529933cbbf94d", "simhash64": 11236004158300864518}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:100e137843a01135f4749208ca2217931dc7b219494829f968d61256038795cd", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 49, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "until expended; and ‘‘(ii) be applied first to fund any contingent liabilities associated with share-in-savings procurements that are not fully funded. ‘‘(b) CANCELLATION AND TERMINATION.—(1) If funds are not made available for the continuation of a share-in-savings contract entered into under this section in a subsequent fiscal year, the contract shall be canceled or terminated. The costs of cancellation or termination may be paid out of— ‘‘(A) appropriations available for the performance of the contract; ‘‘(B) appropriations available for acquisition of the information technology procured under the contract, and not otherwise obligated; or ‘‘(C) funds subsequently appropriated for payments of costs of cancellation or termination, subject to the limitations in paragraph (3). ‘‘(2) The amount payable in the event of cancellation or termination of a share-in-savings contract shall be negotiated with the contractor at the time the contract is entered into. ‘‘(3)(A) Subject to subparagraph (B), the head of an agency may enter into share-in-savings contracts under this section in any given fiscal year even if funds are not made specifically available for the full costs of cancellation or termination of the contract if funds are available and sufficient to make payments with respect to the first fiscal year of the contract and the following conditions VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00035 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2934 are met regarding the funding of cancellation and termination liability: ‘‘(i) The amount of unfunded contingent liability for the contract does not exceed the lesser of— ‘‘(I) 25 percent of the estimated costs of a cancellation or termination; or ‘‘(II) $5,000,000. ‘‘(ii) Unfunded contingent liability in excess of $1,000,000 has been approved by the Director of the Office of Management and Budget or the Director’s designee. ‘‘(B) The aggregate number of share-in-savings contracts that may be entered into under subparagraph (A) by all agencies to which this chapter applies in a fiscal year may not exceed 5 in each of fiscal years 2003, 2004, and 2005. ‘‘(c) DEFINITIONS.—In this section: ‘‘(1) The term ‘contractor’ means a private entity that enters into a contract with", "quality": {"lang": "en", "gibberish_score": 0.0172, "char_len": 2273, "word_len": 350}, "dedupe": {"exact_hash": "a4ea5d56799eeb9058c7d92af3fcb98011cd9fbc0b69e49c4a56e4f1b49fc70e", "simhash64": 10650251501951620102}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1bb458f22d50c31ba2fcaf1ff597783c6d6a3a4f6a9f7f2a42637344bd7ffa6b", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 50, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "may be entered into under subparagraph (A) by all agencies to which this chapter applies in a fiscal year may not exceed 5 in each of fiscal years 2003, 2004, and 2005. ‘‘(c) DEFINITIONS.—In this section: ‘‘(1) The term ‘contractor’ means a private entity that enters into a contract with an agency. ‘‘(2) The term ‘savings’ means— ‘‘(A) monetary savings to an agency; or ‘‘(B) savings in time or other benefits realized by the agency, including enhanced revenues (other than enhanced revenues from the collection of fees, taxes, debts, claims, or other amounts owed the Federal Government). ‘‘(3) The term ‘share-in-savings contract’ means a contract under which— ‘‘(A) a contractor provides solutions for— ‘‘(i) improving the agency’s mission-related or administrative processes; or ‘‘(ii) accelerating the achievement of agency missions; and ‘‘(B) the head of the agency pays the contractor an amount equal to a portion of the savings derived by the agency from— ‘‘(i) any improvements in mission-related or administrative processes that result from implementation of the solution; or ‘‘(ii) acceleration of achievement of agency missions. ‘‘(d) TERMINATION.—No share-in-savings contracts may be entered into under this section after September 30, 2005.’’. (2) The table of sections at the beginning of such chapter is amended by adding at the end of the following new item: ‘‘2332. Share-in-savings contracts.’’. (b) OTHER CONTRACTS.—Title III of the Federal Property and Administrative Services Act of 1949 is amended by adding at the end the following: ‘‘SEC. 317. SHARE-IN-SAVINGS CONTRACTS. ‘‘(a) AUTHORITY TO ENTER INTO SHARE-IN-SAVINGS CONTRACTS.—(1) The head of an executive agency may enter into a share-in-savings contract for information technology (as defined in section 11101(6) of title 40, United States Code) in which the Government awards a contract to improve mission-related or administrative processes or to accelerate the achievement of its mission and share with the contractor in savings achieved through contract performance. 41 USC 266a. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00036 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2935 ‘‘(2)(A) Except as provided in subparagraph (B), a share-insavings contract shall be awarded for a period of not", "quality": {"lang": "en", "gibberish_score": 0.0239, "char_len": 2303, "word_len": 350}, "dedupe": {"exact_hash": "2c5c4f3882112ca66e5f086402efb3fe6d1600310538a04bd0c2bae943a46f0d", "simhash64": 11235722683324403718}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2f8a7aaacb45278fd7017863858bf308bb12f6052fb481d81c5820927f195f68", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 52, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "retain savings realized through the use of a share-in-savings contract under this section that are in excess of the total amount of savings paid to the contractor under the contract, but may not retain any portion of such savings that is attributable to a decrease in the number of civilian employees of the Federal Government performing the function. Except as provided in subparagraph (B), savings shall be credited to the appropriation or fund against which charges were made to carry out the contract and shall be used for information technology. ‘‘(B) Amounts retained by the agency under this subsection shall— ‘‘(i) without further appropriation, remain available until expended; and ‘‘(ii) be applied first to fund any contingent liabilities associated with share-in-savings procurements that are not fully funded. ‘‘(b) CANCELLATION AND TERMINATION.—(1) If funds are not made available for the continuation of a share-in-savings contract entered into under this section in a subsequent fiscal year, the contract shall be canceled or terminated. The costs of cancellation or termination may be paid out of— ‘‘(A) appropriations available for the performance of the contract; ‘‘(B) appropriations available for acquisition of the information technology procured under the contract, and not otherwise obligated; or VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00037 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2936 ‘‘(C) funds subsequently appropriated for payments of costs of cancellation or termination, subject to the limitations in paragraph (3). ‘‘(2) The amount payable in the event of cancellation or termination of a share-in-savings contract shall be negotiated with the contractor at the time the contract is entered into. ‘‘(3)(A) Subject to subparagraph (B), the head of an executive agency may enter into share-in-savings contracts under this section in any given fiscal year even if funds are not made specifically available for the full costs of cancellation or termination of the contract if funds are available and sufficient to make payments with respect to the first fiscal year of the contract and the following conditions are met regarding the funding of cancellation and termination liability: ‘‘(i) The amount of unfunded contingent liability", "quality": {"lang": "en", "gibberish_score": 0.0109, "char_len": 2297, "word_len": 350}, "dedupe": {"exact_hash": "b8ee112516f096ca5b8e848ee0118916be3afde7d483a0fb17ca1055f3750c45", "simhash64": 10649973325507629190}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:cb22d5773e6145e6f4f5aed9e00df27da2fbec7260101f47685e4a3a2e00ae37", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 53, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "the full costs of cancellation or termination of the contract if funds are available and sufficient to make payments with respect to the first fiscal year of the contract and the following conditions are met regarding the funding of cancellation and termination liability: ‘‘(i) The amount of unfunded contingent liability for the contract does not exceed the lesser of— ‘‘(I) 25 percent of the estimated costs of a cancellation or termination; or ‘‘(II) $5,000,000. ‘‘(ii) Unfunded contingent liability in excess of $1,000,000 has been approved by the Director of the Office of Management and Budget or the Director’s designee. ‘‘(B) The aggregate number of share-in-savings contracts that may be entered into under subparagraph (A) by all executive agencies to which this chapter applies in a fiscal year may not exceed 5 in each of fiscal years 2003, 2004, and 2005. ‘‘(c) DEFINITIONS.—In this section: ‘‘(1) The term ‘contractor’ means a private entity that enters into a contract with an agency. ‘‘(2) The term ‘savings’ means— ‘‘(A) monetary savings to an agency; or ‘‘(B) savings in time or other benefits realized by the agency, including enhanced revenues (other than enhanced revenues from the collection of fees, taxes, debts, claims, or other amounts owed the Federal Government). ‘‘(3) The term ‘share-in-savings contract’ means a contract under which— ‘‘(A) a contractor provides solutions for— ‘‘(i) improving the agency’s mission-related or administrative processes; or ‘‘(ii) accelerating the achievement of agency missions; and ‘‘(B) the head of the agency pays the contractor an amount equal to a portion of the savings derived by the agency from— ‘‘(i) any improvements in mission-related or administrative processes that result from implementation of the solution; or ‘‘(ii) acceleration of achievement of agency missions. ‘‘(d) TERMINATION.—No share-in-savings contracts may be entered into under this section after September 30, 2005.’’. (c) DEVELOPMENT OF INCENTIVES.—The Director of the Office of Management and Budget shall, in consultation with the Committee on Governmental Affairs of the Senate, the Committee on Government Reform of the House of Representatives, and executive agencies, develop techniques to permit an executive agency to retain VerDate 11-MAY-2000 21:45 Jan 02,", "quality": {"lang": "en", "gibberish_score": 0.0243, "char_len": 2309, "word_len": 350}, "dedupe": {"exact_hash": "cc26f24d56d56d228674730b0b07710ca96584d63dc602ea0beb45d3fa9d4fda", "simhash64": 10649974356302991366}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1b859e248b6f8e8f86673bc739bde56c5c94e340286210a7f92efe5266a94170", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 56, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "in subsection (b), by striking ‘‘LIMITATIONS.— ’’ and all that follows through ‘‘$750,000,000.’’ and inserting the following: ‘‘LIMITATION ON AMOUNT.—The total amount obligated for contracts entered into under the pilot program conducted under this chapter may not exceed $375,000,000.’’; and (v) in subsection (c)(1), by striking ‘‘a pilot’’ and inserting ‘‘the pilot’’. (B) The following provisions of chapter 115 of such title are each amended by striking ‘‘a pilot’’ each place it appears and inserting ‘‘the pilot’’: (i) Section 11502(a). (ii) Section 11502(b). (iii) Section 11503(a). (iv) Section 11504. (C) Section 11505 of such chapter is amended by striking ‘‘programs’’ and inserting ‘‘program’’. (3) ADDITIONAL CONFORMING AMENDMENTS.— (A) Section 11522 of title 40, United States Code, is redesignated as section 11521. (B) The chapter heading for chapter 115 of such title is amended by striking ‘‘PROGRAMS’’ and inserting ‘‘PROGRAM’’. (C) The subchapter heading for subchapter I and for subchapter II of such chapter are each amended by striking ‘‘PROGRAMS’’ and inserting ‘‘PROGRAM’’. (D) The item relating to subchapter I in the table of sections at the beginning of such chapter is amended to read as follows: ‘‘SUBCHAPTER I—CONDUCT OF PILOT PROGRAM’’. (E) The item relating to subchapter II in the table of sections at the beginning of such chapter is amended to read as follows: ‘‘SUBCHAPTER II—SPECIFIC PILOT PROGRAM’’. (F) The item relating to section 11501 in the table of sections at the beginning of such is amended by striking ‘‘programs’’ and inserting ‘‘program’’. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00040 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2939 (G) The table of sections at the beginning of such chapter is amended by striking the item relating to section 11521 and redesignating the item relating to section 11522 as section 11521. (H) The item relating to chapter 115 in the table of chapters for subtitle III of title 40, United States Code, is amended to read as follows: ‘‘115. INFORMATION TECHNOLOGY ACQUISITION PILOT PROGRAM ....11501’’. (i) DEFINITIONS.—In this section, the terms ‘‘contractor’’, ‘‘savings’’, and ‘‘share-in-savings contract’’ have the meanings given those terms in section 317", "quality": {"lang": "en", "gibberish_score": 0.0413, "char_len": 2274, "word_len": 350}, "dedupe": {"exact_hash": "59dd47799f33e0d27c5bf2e1f5ad42f0b396aee0b6ee363d9971ef7281dc8e59", "simhash64": 11235436810301064214}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:fa9afc57817fac3b87f0aef5464a0614efe4ce55c2b768b0f0d605df2e706409", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 57, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "to chapter 115 in the table of chapters for subtitle III of title 40, United States Code, is amended to read as follows: ‘‘115. INFORMATION TECHNOLOGY ACQUISITION PILOT PROGRAM ....11501’’. (i) DEFINITIONS.—In this section, the terms ‘‘contractor’’, ‘‘savings’’, and ‘‘share-in-savings contract’’ have the meanings given those terms in section 317 of the Federal Property and Administrative Services Act of 1949 (as added by subsection (b)). SEC. 211. AUTHORIZATION FOR ACQUISITION OF INFORMATION TECHNOLOGY BY STATE AND LOCAL GOVERNMENTS THROUGH FEDERAL SUPPLY SCHEDULES. (a) AUTHORITY TO USE CERTAIN SUPPLY SCHEDULES.—Section 502 of title 40, United States Code, is amended by adding at the end the following new subsection: ‘‘(c) USE OF CERTAIN SUPPLY SCHEDULES.— ‘‘(1) IN GENERAL.—The Administrator may provide for the use by State or local governments of Federal supply schedules of the General Services Administration for automated data processing equipment (including firmware), software, supplies, support equipment, and services (as contained in Federal supply classification code group 70). ‘‘(2) VOLUNTARY USE.—In any case of the use by a State or local government of a Federal supply schedule pursuant to paragraph (1), participation by a firm that sells to the Federal Government through the supply schedule shall be voluntary with respect to a sale to the State or local government through such supply schedule. ‘‘(3) DEFINITIONS.—In this subsection: ‘‘(A) The term ‘State or local government’ includes any State, local, regional, or tribal government, or any instrumentality thereof (including any local educational agency or institution of higher education). ‘‘(B) The term ‘tribal government’ means— ‘‘(i) the governing body of any Indian tribe, band, nation, or other organized group or community located in the continental United States (excluding the State of Alaska) that is recognized as eligible for the special programs and services provided by the United States to Indians because of their status as Indians, and ‘‘(ii) any Alaska Native regional or village corporation established pursuant to the Alaska Native Claims Settlement Act (43 U.S.C. 1601 et seq.). ‘‘(C) The term ‘local educational agency’ has the meaning given that term in section 8013 of the Elementary and Secondary Education Act of 1965", "quality": {"lang": "en", "gibberish_score": 0.0202, "char_len": 2328, "word_len": 350}, "dedupe": {"exact_hash": "9689de13473d0db61be287c30dbde37003e074215eead0c247444deef6b531c2", "simhash64": 11235437909813068934}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:07ec58e42776ee05ed1f4bc00d99e19d7a632a0fdb56a7aee95f921accdf77d1", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 59, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "any individual, trust, firm, joint stock company, corporation (including a government corporation), partnership, association, State, municipality, commission, political subdivision of a State, interstate body, or agency or component of the Federal Government. (c) REPORT.— (1) IN GENERAL.—Not later than 3 years after the date of enactment of this Act, the Director shall oversee a study, in consultation with agencies, the regulated community, public interest organizations, and the public, and submit a report to the Committee on Governmental Affairs of the Senate and the Committee on Government Reform of the House of Representatives on progress toward integrating Federal information systems across agencies. (2) CONTENTS.—The report under this section shall— (A) address the integration of data elements used in the electronic collection of information within databases established under Federal statute without reducing the quality, accessibility, scope, or utility of the information contained in each database; (B) address the feasibility of developing, or enabling the development of, software, including Internet-based tools, for use by reporting persons in assembling, documenting, and validating the accuracy of information electronically submitted to agencies under nonvoluntary, statutory, and regulatory requirements; (C) address the feasibility of developing a distributed information system involving, on a voluntary basis, at least 2 agencies, that— (i) provides consistent, dependable, and timely public access to the information holdings of 1 or more agencies, or some portion of such holdings, without requiring public users to know which agency holds the information; and (ii) allows the integration of public information held by the participating agencies; Deadline. 44 USC 3501 note. Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00042 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2941 (D) address the feasibility of incorporating other elements related to the purposes of this section at the discretion of the Director; and (E) make any recommendations that the Director deems appropriate on the use of integrated reporting and information systems, to reduce the burden on reporting and strengthen public access to databases within and across agencies. (d) PILOT PROJECTS TO ENCOURAGE INTEGRATED COLLECTION AND MANAGEMENT OF DATA AND INTEROPERABILITY OF FEDERAL INFORMATION SYSTEMS.— (1) IN", "quality": {"lang": "en", "gibberish_score": 0.0033, "char_len": 2449, "word_len": 350}, "dedupe": {"exact_hash": "03b185a2367c055d120eeb667ae1befcafbeaed1b57dfd14b5a6f5423c1f4ff7", "simhash64": 10658981555521553414}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0c0928dd33d088a565d37154a4974d6e3d154e5803d580d631bb16b0424491c7", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 60, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "recommendations that the Director deems appropriate on the use of integrated reporting and information systems, to reduce the burden on reporting and strengthen public access to databases within and across agencies. (d) PILOT PROJECTS TO ENCOURAGE INTEGRATED COLLECTION AND MANAGEMENT OF DATA AND INTEROPERABILITY OF FEDERAL INFORMATION SYSTEMS.— (1) IN GENERAL.—In order to provide input to the study under subsection (c), the Director shall designate, in consultation with agencies, a series of no more than 5 pilot projects that integrate data elements. The Director shall consult with agencies, the regulated community, public interest organizations, and the public on the implementation of the pilot projects. (2) GOALS OF PILOT PROJECTS.— (A) IN GENERAL.—Each goal described under subparagraph (B) shall be addressed by at least 1 pilot project each. (B) GOALS.—The goals under this paragraph are to— (i) reduce information collection burdens by eliminating duplicative data elements within 2 or more reporting requirements; (ii) create interoperability between or among public databases managed by 2 or more agencies using technologies and techniques that facilitate public access; and (iii) develop, or enable the development of, software to reduce errors in electronically submitted information. (3) INPUT.—Each pilot project shall seek input from users on the utility of the pilot project and areas for improvement. To the extent practicable, the Director shall consult with relevant agencies and State, tribal, and local governments in carrying out the report and pilot projects under this section. (e) PROTECTIONS.—The activities authorized under this section shall afford protections for— (1) confidential business information consistent with section 552(b)(4) of title 5, United States Code, and other relevant law; (2) personal privacy information under sections 552(b) (6) and (7)(C) and 552a of title 5, United States Code, and other relevant law; (3) other information consistent with section 552(b)(3) of title 5, United States Code, and other relevant law; and (4) confidential statistical information collected under a confidentiality pledge, solely for statistical purposes, consistent with the Office of Management and Budget’s Federal Statistical Confidentiality Order, and other relevant law. SEC. 213. COMMUNITY TECHNOLOGY CENTERS. (a) PURPOSES.—The purposes of this section are to— (1) study", "quality": {"lang": "en", "gibberish_score": 0.005, "char_len": 2418, "word_len": 350}, "dedupe": {"exact_hash": "66d9582ac4933b2e8445be54e44c315af5ea8aba7b5974edde70389cb37538d8", "simhash64": 14100025309190276630}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:41a61d46dc93a7743d54d9abc47d5f905db638bbb4d0e6d43812fa8de086e332", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 61, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "Code, and other relevant law; and (4) confidential statistical information collected under a confidentiality pledge, solely for statistical purposes, consistent with the Office of Management and Budget’s Federal Statistical Confidentiality Order, and other relevant law. SEC. 213. COMMUNITY TECHNOLOGY CENTERS. (a) PURPOSES.—The purposes of this section are to— (1) study and enhance the effectiveness of community technology centers, public libraries, and other institutions that provide computer and Internet access to the public; and 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00043 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2942 (2) promote awareness of the availability of on-line government information and services, to users of community technology centers, public libraries, and other public facilities that provide access to computer technology and Internet access to the public. (b) STUDY AND REPORT.—Not later than 2 years after the effective date of this title, the Administrator shall— (1) ensure that a study is conducted to evaluate the best practices of community technology centers that have received Federal funds; and (2) submit a report on the study to— (A) the Committee on Governmental Affairs of the Senate; (B) the Committee on Health, Education, Labor, and Pensions of the Senate; (C) the Committee on Government Reform of the House of Representatives; and (D) the Committee on Education and the Workforce of the House of Representatives. (c) CONTENTS.—The report under subsection (b) may consider— (1) an evaluation of the best practices being used by successful community technology centers; (2) a strategy for— (A) continuing the evaluation of best practices used by community technology centers; and (B) establishing a network to share information and resources as community technology centers evolve; (3) the identification of methods to expand the use of best practices to assist community technology centers, public libraries, and other institutions that provide computer and Internet access to the public; (4) a database of all community technology centers that have received Federal funds, including— (A) each center’s name, location, services provided, director, other points of contact, number of individuals served; and (B) other relevant information; (5) an analysis of", "quality": {"lang": "en", "gibberish_score": 0.0055, "char_len": 2354, "word_len": 350}, "dedupe": {"exact_hash": "e74813c1e91819781a33200afa41744923436a185b776f3819c6eafb8f2d1bc0", "simhash64": 9491980873248900374}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2d3fbecf9a317f86ae1eeefec6cea43b5f40571145f49f12aa2da0aadec5e8eb", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 62, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "other institutions that provide computer and Internet access to the public; (4) a database of all community technology centers that have received Federal funds, including— (A) each center’s name, location, services provided, director, other points of contact, number of individuals served; and (B) other relevant information; (5) an analysis of whether community technology centers have been deployed effectively in urban and rural areas throughout the Nation; and (6) recommendations of how to— (A) enhance the development of community technology centers; and (B) establish a network to share information and resources. (d) COOPERATION.—All agencies that fund community technology centers shall provide to the Administrator any information and assistance necessary for the completion of the study and the report under this section. (e) ASSISTANCE.— (1) IN GENERAL.—The Administrator, in consultation with the Secretary of Education, shall work with other relevant Federal agencies, and other interested persons in the private and nonprofit sectors to— (A) assist in the implementation of recommendations; and Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00044 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2943 (B) identify other ways to assist community technology centers, public libraries, and other institutions that provide computer and Internet access to the public. (2) TYPES OF ASSISTANCE.—Assistance under this subsection may include— (A) contribution of funds; (B) donations of equipment, and training in the use and maintenance of the equipment; and (C) the provision of basic instruction or training material in computer skills and Internet usage. (f) ONLINE TUTORIAL.— (1) IN GENERAL.—The Administrator, in consultation with the Secretary of Education, the Director of the Institute of Museum and Library Services, other relevant agencies, and the public, shall develop an online tutorial that— (A) explains how to access Government information and services on the Internet; and (B) provides a guide to available online resources. (2) DISTRIBUTION.—The Administrator, with assistance from the Secretary of Education, shall distribute information on the tutorial to community technology centers, public libraries, and other institutions that afford Internet access to the public. (g) PROMOTION OF COMMUNITY TECHNOLOGY CENTERS.—The Administrator, with assistance from the Department of Education and", "quality": {"lang": "en", "gibberish_score": 0.0065, "char_len": 2444, "word_len": 350}, "dedupe": {"exact_hash": "3120dbbb38047f86806c6f13f0b0657a6a2e19cab315e360e463beae2c76788b", "simhash64": 9511407044687922582}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8614bc7281da355da4f4a0d041a58c7fdd2de057a61cbcacbfba90295a837f59", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 63, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "available online resources. (2) DISTRIBUTION.—The Administrator, with assistance from the Secretary of Education, shall distribute information on the tutorial to community technology centers, public libraries, and other institutions that afford Internet access to the public. (g) PROMOTION OF COMMUNITY TECHNOLOGY CENTERS.—The Administrator, with assistance from the Department of Education and in consultation with other agencies and organizations, shall promote the availability of community technology centers to raise awareness within each community where such a center is located. (h) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated for the study of best practices at community technology centers, for the development and dissemination of the online tutorial, and for the promotion of community technology centers under this section— (1) $2,000,000 in fiscal year 2003; (2) $2,000,000 in fiscal year 2004; and (3) such sums as are necessary in fiscal years 2005 through 2007. SEC. 214. ENHANCING CRISIS MANAGEMENT THROUGH ADVANCED INFORMATION TECHNOLOGY. (a) PURPOSE.—The purpose of this section is to improve how information technology is used in coordinating and facilitating information on disaster preparedness, response, and recovery, while ensuring the availability of such information across multiple access channels. (b) IN GENERAL.— (1) STUDY ON ENHANCEMENT OF CRISIS RESPONSE.—Not later than 90 days after the date of enactment of this Act, the Administrator, in consultation with the Federal Emergency Management Agency, shall ensure that a study is conducted on using information technology to enhance crisis preparedness, response, and consequence management of natural and manmade disasters. (2) CONTENTS.—The study under this subsection shall address— (A) a research and implementation strategy for effective use of information technology in crisis response and Deadline. 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00045 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2944 consequence management, including the more effective use of technologies, management of information technology research initiatives, and incorporation of research advances into the information and communications systems of— (i) the Federal Emergency Management Agency; and (ii) other Federal, State, and local agencies responsible for crisis preparedness, response, and consequence management; and (B) opportunities for research and development on enhanced technologies", "quality": {"lang": "en", "gibberish_score": 0.0055, "char_len": 2529, "word_len": 350}, "dedupe": {"exact_hash": "aa2a52882ee24913358411c8574b727a0a354de2e36533a81dfe33de142927ab", "simhash64": 10658977226193732758}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f26a49657936eea9e5ebfb56b90b633591633c712ded0fa18939dd3fbb66f009", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 64, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "management of information technology research initiatives, and incorporation of research advances into the information and communications systems of— (i) the Federal Emergency Management Agency; and (ii) other Federal, State, and local agencies responsible for crisis preparedness, response, and consequence management; and (B) opportunities for research and development on enhanced technologies into areas of potential improvement as determined during the course of the study. (3) REPORT.—Not later than 2 years after the date on which a contract is entered into under paragraph (1), the Administrator shall submit a report on the study, including findings and recommendations to— (A) the Committee on Governmental Affairs of the Senate; and (B) the Committee on Government Reform of the House of Representatives. (4) INTERAGENCY COOPERATION.—Other Federal departments and agencies with responsibility for disaster relief and emergency assistance shall fully cooperate with the Administrator in carrying out this section. (5) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated for research under this subsection, such sums as are necessary for fiscal year 2003. (c) PILOT PROJECTS.—Based on the results of the research conducted under subsection (b), the Administrator, in consultation with the Federal Emergency Management Agency, shall initiate pilot projects or report to Congress on other activities that further the goal of maximizing the utility of information technology in disaster management. The Administrator shall cooperate with other relevant agencies, and, if appropriate, State, local, and tribal governments, in initiating such pilot projects. SEC. 215. DISPARITIES IN ACCESS TO THE INTERNET. (a) STUDY AND REPORT.— (1) STUDY.—Not later than 90 days after the date of enactment of this Act, the Administrator of General Services shall request that the National Academy of Sciences, acting through the National Research Council, enter into a contract to conduct a study on disparities in Internet access for online Government services. (2) REPORT.—Not later than 2 years after the date of enactment of this Act, the Administrator of General Services shall submit to the Committee on Governmental Affairs of the Senate and the Committee on Government Reform of the House of Representatives a final report of the study under this section, which shall set", "quality": {"lang": "en", "gibberish_score": 0.0038, "char_len": 2390, "word_len": 350}, "dedupe": {"exact_hash": "64bb1cd157301b9e4721c968ad763e9b4195a641324e4153c5893ab9a5437418", "simhash64": 10591427629863501974}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0150012a98a77bde6920317f280777fd5bfe5d02ebc5f555b2e9b22a9f76c4db", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 65, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "than 2 years after the date of enactment of this Act, the Administrator of General Services shall submit to the Committee on Governmental Affairs of the Senate and the Committee on Government Reform of the House of Representatives a final report of the study under this section, which shall set forth the findings, conclusions, and recommendations of the National Research Council. (b) CONTENTS.—The report under subsection (a) shall include a study of— (1) how disparities in Internet access influence the effectiveness of online Government services, including a review of— (A) the nature of disparities in Internet access; (B) the affordability of Internet service; Deadlines. 44 USC 3501 note. Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00046 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2945 (C) the incidence of disparities among different groups within the population; and (D) changes in the nature of personal and public Internet access that may alleviate or aggravate effective access to online Government services; (2) how the increase in online Government services is influencing the disparities in Internet access and how technology development or diffusion trends may offset such adverse influences; and (3) related societal effects arising from the interplay of disparities in Internet access and the increase in online Government services. (c) RECOMMENDATIONS.—The report shall include recommendations on actions to ensure that online Government initiatives shall not have the unintended result of increasing any deficiency in public access to Government services. (d) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated $950,000 in fiscal year 2003 to carry out this section. SEC. 216. COMMON PROTOCOLS FOR GEOGRAPHIC INFORMATION SYSTEMS. (a) PURPOSES.—The purposes of this section are to— (1) reduce redundant data collection and information; and (2) promote collaboration and use of standards for government geographic information. (b) DEFINITION.—In this section, the term ‘‘geographic information’’ means information systems that involve locational data, such as maps or other geospatial information resources. (c) IN GENERAL.— (1) COMMON PROTOCOLS.—The Administrator, in consultation with the Secretary of the Interior, working with the Director and through an interagency group, and working with private sector experts, State, local,", "quality": {"lang": "en", "gibberish_score": 0.0071, "char_len": 2411, "word_len": 350}, "dedupe": {"exact_hash": "418615eb2aa42268a50e8b83442ff065f7837dc53eba9cfc2e38acf5c3dbd4f8", "simhash64": 11154940533207302550}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4afac6728be651494d6515f876919aebb164f995474dfd9bf5e798584a319661", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 66, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "term ‘‘geographic information’’ means information systems that involve locational data, such as maps or other geospatial information resources. (c) IN GENERAL.— (1) COMMON PROTOCOLS.—The Administrator, in consultation with the Secretary of the Interior, working with the Director and through an interagency group, and working with private sector experts, State, local, and tribal governments, commercial and international standards groups, and other interested parties, shall facilitate the development of common protocols for the development, acquisition, maintenance, distribution, and application of geographic information. If practicable, the Administrator shall incorporate intergovernmental and public private geographic information partnerships into efforts under this subsection. (2) INTERAGENCY GROUP.—The interagency group referred to under paragraph (1) shall include representatives of the National Institute of Standards and Technology and other agencies. (d) DIRECTOR.—The Director shall oversee— (1) the interagency initiative to develop common protocols; (2) the coordination with State, local, and tribal governments, public private partnerships, and other interested persons on effective and efficient ways to align geographic information and develop common protocols; and (3) the adoption of common standards relating to the protocols. (e) COMMON PROTOCOLS.—The common protocols shall be designed to— 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00047 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2946 (1) maximize the degree to which unclassified geographic information from various sources can be made electronically compatible and accessible; and (2) promote the development of interoperable geographic information systems technologies that shall— (A) allow widespread, low-cost use and sharing of geographic data by Federal agencies, State, local, and tribal governments, and the public; and (B) enable the enhancement of services using geographic data. (f) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated such sums as are necessary to carry out this section, for each of the fiscal years 2003 through 2007. TITLE III—INFORMATION SECURITY SEC. 301. INFORMATION SECURITY. (a) SHORT TITLE.—This title may be cited as the ‘‘Federal Information Security Management Act of 2002’’. (b) INFORMATION SECURITY.— (1) IN GENERAL.—Chapter 35 of title 44, United States Code, is amended by adding at the end the following new subchapter: ‘‘SUBCHAPTER III—INFORMATION SECURITY ‘‘§", "quality": {"lang": "en", "gibberish_score": 0.0113, "char_len": 2556, "word_len": 350}, "dedupe": {"exact_hash": "a8593a844d1bac8e8896a0dfac6318bfbb9fce95a6967418eb504a7283bc2ab6", "simhash64": 15251522946205058758}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:bb16f88b1335d864c4b85bac6dc9cece21e4bd36c25cd296273abac9441856af", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 67, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "III—INFORMATION SECURITY SEC. 301. INFORMATION SECURITY. (a) SHORT TITLE.—This title may be cited as the ‘‘Federal Information Security Management Act of 2002’’. (b) INFORMATION SECURITY.— (1) IN GENERAL.—Chapter 35 of title 44, United States Code, is amended by adding at the end the following new subchapter: ‘‘SUBCHAPTER III—INFORMATION SECURITY ‘‘§ 3541. Purposes ‘‘The purposes of this subchapter are to— ‘‘(1) provide a comprehensive framework for ensuring the effectiveness of information security controls over information resources that support Federal operations and assets; ‘‘(2) recognize the highly networked nature of the current Federal computing environment and provide effective governmentwide management and oversight of the related information security risks, including coordination of information security efforts throughout the civilian, national security, and law enforcement communities; ‘‘(3) provide for development and maintenance of minimum controls required to protect Federal information and information systems; ‘‘(4) provide a mechanism for improved oversight of Federal agency information security programs; ‘‘(5) acknowledge that commercially developed information security products offer advanced, dynamic, robust, and effective information security solutions, reflecting market solutions for the protection of critical information infrastructures important to the national defense and economic security of the nation that are designed, built, and operated by the private sector; and ‘‘(6) recognize that the selection of specific technical hardware and software information security solutions should be left to individual agencies from among commercially developed products. 44 USC 101 note. Federal Information Security Management Act of 2002. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00048 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2947 ‘‘§ 3542. Definitions ‘‘(a) IN GENERAL.—Except as provided under subsection (b), the definitions under section 3502 shall apply to this subchapter. ‘‘(b) ADDITIONAL DEFINITIONS.—As used in this subchapter: ‘‘(1) The term ‘information security’ means protecting information and information systems from unauthorized access, use, disclosure, disruption, modification, or destruction in order to provide— ‘‘(A) integrity, which means guarding against improper information modification or destruction, and includes ensuring information nonrepudiation and authenticity; ‘‘(B) confidentiality, which means preserving authorized restrictions on access and disclosure, including means for protecting personal privacy and proprietary information; and ‘‘(C) availability, which", "quality": {"lang": "en", "gibberish_score": 0.0191, "char_len": 2672, "word_len": 350}, "dedupe": {"exact_hash": "635f9f1ed461b1230215f34ce6c8de4795df85a82ee05e94d0e8d1dc464b3b0e", "simhash64": 11495824253468083398}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:de051cee2489de7bcf3a4f13ec07de63a1ca0b55e6942f984f3de73612b64fd8", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 68, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "disruption, modification, or destruction in order to provide— ‘‘(A) integrity, which means guarding against improper information modification or destruction, and includes ensuring information nonrepudiation and authenticity; ‘‘(B) confidentiality, which means preserving authorized restrictions on access and disclosure, including means for protecting personal privacy and proprietary information; and ‘‘(C) availability, which means ensuring timely and reliable access to and use of information. ‘‘(2)(A) The term ‘national security system’ means any information system (including any telecommunications system) used or operated by an agency or by a contractor of an agency, or other organization on behalf of an agency— ‘‘(i) the function, operation, or use of which— ‘‘(I) involves intelligence activities; ‘‘(II) involves cryptologic activities related to national security; ‘‘(III) involves command and control of military forces; ‘‘(IV) involves equipment that is an integral part of a weapon or weapons system; or ‘‘(V) subject to subparagraph (B), is critical to the direct fulfillment of military or intelligence missions; or ‘‘(ii) is protected at all times by procedures established for information that have been specifically authorized under criteria established by an Executive order or an Act of Congress to be kept classified in the interest of national defense or foreign policy. ‘‘(B) Subparagraph (A)(i)(V) does not include a system that is to be used for routine administrative and business applications (including payroll, finance, logistics, and personnel management applications). ‘‘(3) The term ‘information technology’ has the meaning given that term in section 11101 of title 40. ‘‘§ 3543. Authority and functions of the Director ‘‘(a) IN GENERAL.—The Director shall oversee agency information security policies and practices, including— ‘‘(1) developing and overseeing the implementation of policies, principles, standards, and guidelines on information security, including through ensuring timely agency adoption of and compliance with standards promulgated under section 11331 of title 40; ‘‘(2) requiring agencies, consistent with the standards promulgated under such section 11331 and the requirements of this subchapter, to identify and provide information security protections commensurate with the risk and magnitude of the VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00049 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2948 harm resulting", "quality": {"lang": "en", "gibberish_score": 0.0184, "char_len": 2497, "word_len": 350}, "dedupe": {"exact_hash": "f1e110d9437435abc8165fc2b8d1c47529fb199a55f8770b43b83d5f206b9588", "simhash64": 11226430710557017734}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c5eff5957d7454e11f64b94459e260991dd9ae0c90e9134b67ee7b597a81528a", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 69, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "the standards promulgated under such section 11331 and the requirements of this subchapter, to identify and provide information security protections commensurate with the risk and magnitude of the VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00049 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2948 harm resulting from the unauthorized access, use, disclosure, disruption, modification, or destruction of— ‘‘(A) information collected or maintained by or on behalf of an agency; or ‘‘(B) information systems used or operated by an agency or by a contractor of an agency or other organization on behalf of an agency; ‘‘(3) coordinating the development of standards and guidelines under section 20 of the National Institute of Standards and Technology Act (15 U.S.C. 278g–3) with agencies and offices operating or exercising control of national security systems (including the National Security Agency) to assure, to the maximum extent feasible, that such standards and guidelines are complementary with standards and guidelines developed for national security systems; ‘‘(4) overseeing agency compliance with the requirements of this subchapter, including through any authorized action under section 11303 of title 40, to enforce accountability for compliance with such requirements; ‘‘(5) reviewing at least annually, and approving or disapproving, agency information security programs required under section 3544(b); ‘‘(6) coordinating information security policies and procedures with related information resources management policies and procedures; ‘‘(7) overseeing the operation of the Federal information security incident center required under section 3546; and ‘‘(8) reporting to Congress no later than March 1 of each year on agency compliance with the requirements of this subchapter, including— ‘‘(A) a summary of the findings of evaluations required by section 3545; ‘‘(B) an assessment of the development, promulgation, and adoption of, and compliance with, standards developed under section 20 of the National Institute of Standards and Technology Act (15 U.S.C. 278g-3) and promulgated under section 11331 of title 40; ‘‘(C) significant deficiencies in agency information security practices; ‘‘(D) planned remedial action to address such deficiencies; and ‘‘(E) a summary of, and the views of the Director on, the report prepared by the National Institute of Standards and Technology under", "quality": {"lang": "en", "gibberish_score": 0.0128, "char_len": 2426, "word_len": 350}, "dedupe": {"exact_hash": "0f39b2f0bd6209be9a42472658683f8dbc120cc1bee197e1b63696bd4fb44da0", "simhash64": 14117746138095912086}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4153ed5a61d99f756cb5ee63a1e11fe1b56f0a3264dbec7a0482a69853bf558b", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 70, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "U.S.C. 278g-3) and promulgated under section 11331 of title 40; ‘‘(C) significant deficiencies in agency information security practices; ‘‘(D) planned remedial action to address such deficiencies; and ‘‘(E) a summary of, and the views of the Director on, the report prepared by the National Institute of Standards and Technology under section 20(d)(10) of the National Institute of Standards and Technology Act (15 U.S.C. 278g– 3). ‘‘(b) NATIONAL SECURITY SYSTEMS.—Except for the authorities described in paragraphs (4) and (8) of subsection (a), the authorities of the Director under this section shall not apply to national security systems. ‘‘(c) DEPARTMENT OF DEFENSE AND CENTRAL INTELLIGENCE AGENCY SYSTEMS.—(1) The authorities of the Director described in paragraphs (1) and (2) of subsection (a) shall be delegated to the Secretary of Defense in the case of systems described in paragraph (2) and to the Director of Central Intelligence in the case of systems described in paragraph (3). VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00050 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2949 ‘‘(2) The systems described in this paragraph are systems that are operated by the Department of Defense, a contractor of the Department of Defense, or another entity on behalf of the Department of Defense that processes any information the unauthorized access, use, disclosure, disruption, modification, or destruction of which would have a debilitating impact on the mission of the Department of Defense. ‘‘(3) The systems described in this paragraph are systems that are operated by the Central Intelligence Agency, a contractor of the Central Intelligence Agency, or another entity on behalf of the Central Intelligence Agency that processes any information the unauthorized access, use, disclosure, disruption, modification, or destruction of which would have a debilitating impact on the mission of the Central Intelligence Agency. ‘‘§ 3544. Federal agency responsibilities ‘‘(a) IN GENERAL.—The head of each agency shall— ‘‘(1) be responsible for— ‘‘(A) providing information security protections commensurate with the risk and magnitude of the harm resulting from unauthorized access, use, disclosure, disruption, modification, or destruction of— ‘‘(i) information collected or maintained by or on behalf of the agency;", "quality": {"lang": "en", "gibberish_score": 0.0145, "char_len": 2347, "word_len": 350}, "dedupe": {"exact_hash": "a6401d36b80cebd29dc6003e96170d417e607977e02feeb7373a1e9856eef1e3", "simhash64": 10064506473461416070}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:88d9dfa8f948a56f0d2ec9f61be5e0e367a9b99c885751476bb1e816da2b1345", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 71, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "responsibilities ‘‘(a) IN GENERAL.—The head of each agency shall— ‘‘(1) be responsible for— ‘‘(A) providing information security protections commensurate with the risk and magnitude of the harm resulting from unauthorized access, use, disclosure, disruption, modification, or destruction of— ‘‘(i) information collected or maintained by or on behalf of the agency; and ‘‘(ii) information systems used or operated by an agency or by a contractor of an agency or other organization on behalf of an agency; ‘‘(B) complying with the requirements of this subchapter and related policies, procedures, standards, and guidelines, including— ‘‘(i) information security standards promulgated under section 11331 of title 40; and ‘‘(ii) information security standards and guidelines for national security systems issued in accordance with law and as directed by the President; and ‘‘(C) ensuring that information security management processes are integrated with agency strategic and operational planning processes; ‘‘(2) ensure that senior agency officials provide information security for the information and information systems that support the operations and assets under their control, including through— ‘‘(A) assessing the risk and magnitude of the harm that could result from the unauthorized access, use, disclosure, disruption, modification, or destruction of such information or information systems; ‘‘(B) determining the levels of information security appropriate to protect such information and information systems in accordance with standards promulgated under section 11331 of title 40, for information security classifications and related requirements; ‘‘(C) implementing policies and procedures to cost-effectively reduce risks to an acceptable level; and ‘‘(D) periodically testing and evaluating information security controls and techniques to ensure that they are effectively implemented; VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00051 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2950 ‘‘(3) delegate to the agency Chief Information Officer established under section 3506 (or comparable official in an agency not covered by such section) the authority to ensure compliance with the requirements imposed on the agency under this subchapter, including— ‘‘(A) designating a senior agency information security officer who shall— ‘‘(i) carry out the Chief Information Officer’s responsibilities under this section; ‘‘(ii) possess professional qualifications, including training and experience, required to administer", "quality": {"lang": "en", "gibberish_score": 0.0184, "char_len": 2548, "word_len": 350}, "dedupe": {"exact_hash": "318a056856b4e5e2517764537f62edfab97316eb8dd9ccac38121e567d26ac3d", "simhash64": 15243945043348259990}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ed38bc80811155a5f161d3635a554fa45dea81b36c7c42758e79d666cd085512", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 72, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "such section) the authority to ensure compliance with the requirements imposed on the agency under this subchapter, including— ‘‘(A) designating a senior agency information security officer who shall— ‘‘(i) carry out the Chief Information Officer’s responsibilities under this section; ‘‘(ii) possess professional qualifications, including training and experience, required to administer the functions described under this section; ‘‘(iii) have information security duties as that official’s primary duty; and ‘‘(iv) head an office with the mission and resources to assist in ensuring agency compliance with this section; ‘‘(B) developing and maintaining an agencywide information security program as required by subsection (b); ‘‘(C) developing and maintaining information security policies, procedures, and control techniques to address all applicable requirements, including those issued under section 3543 of this title, and section 11331 of title 40; ‘‘(D) training and overseeing personnel with significant responsibilities for information security with respect to such responsibilities; and ‘‘(E) assisting senior agency officials concerning their responsibilities under paragraph (2); ‘‘(4) ensure that the agency has trained personnel sufficient to assist the agency in complying with the requirements of this subchapter and related policies, procedures, standards, and guidelines; and ‘‘(5) ensure that the agency Chief Information Officer, in coordination with other senior agency officials, reports annually to the agency head on the effectiveness of the agency information security program, including progress of remedial actions. ‘‘(b) AGENCY PROGRAM.—Each agency shall develop, document, and implement an agencywide information security program, approved by the Director under section 3543(a)(5), to provide information security for the information and information systems that support the operations and assets of the agency, including those provided or managed by another agency, contractor, or other source, that includes— ‘‘(1) periodic assessments of the risk and magnitude of the harm that could result from the unauthorized access, use, disclosure, disruption, modification, or destruction of information and information systems that support the operations and assets of the agency; ‘‘(2) policies and procedures that— ‘‘(A) are based on the risk assessments required by paragraph (1); ‘‘(B) cost-effectively reduce information security risks to an acceptable level; ‘‘(C) ensure that information security is addressed throughout the life", "quality": {"lang": "en", "gibberish_score": 0.016, "char_len": 2560, "word_len": 350}, "dedupe": {"exact_hash": "f8e0f60eb97031ab63ebfe8101cc90180a7ed56d8116090695c06d2d1bb5c393", "simhash64": 14104252931365225558}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:630426506e98c4a4b78c9f8f0d92b0820b3a90d1de05737ec932e20eb53ae251", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 73, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "of information and information systems that support the operations and assets of the agency; ‘‘(2) policies and procedures that— ‘‘(A) are based on the risk assessments required by paragraph (1); ‘‘(B) cost-effectively reduce information security risks to an acceptable level; ‘‘(C) ensure that information security is addressed throughout the life cycle of each agency information system; and VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00052 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2951 ‘‘(D) ensure compliance with— ‘‘(i) the requirements of this subchapter; ‘‘(ii) policies and procedures as may be prescribed by the Director, and information security standards promulgated under section 11331 of title 40; ‘‘(iii) minimally acceptable system configuration requirements, as determined by the agency; and ‘‘(iv) any other applicable requirements, including standards and guidelines for national security systems issued in accordance with law and as directed by the President; ‘‘(3) subordinate plans for providing adequate information security for networks, facilities, and systems or groups of information systems, as appropriate; ‘‘(4) security awareness training to inform personnel, including contractors and other users of information systems that support the operations and assets of the agency, of— ‘‘(A) information security risks associated with their activities; and ‘‘(B) their responsibilities in complying with agency policies and procedures designed to reduce these risks; ‘‘(5) periodic testing and evaluation of the effectiveness of information security policies, procedures, and practices, to be performed with a frequency depending on risk, but no less than annually, of which such testing— ‘‘(A) shall include testing of management, operational, and technical controls of every information system identified in the inventory required under section 3505(c); and ‘‘(B) may include testing relied on in a evaluation under section 3545; ‘‘(6) a process for planning, implementing, evaluating, and documenting remedial action to address any deficiencies in the information security policies, procedures, and practices of the agency; ‘‘(7) procedures for detecting, reporting, and responding to security incidents, consistent with standards and guidelines issued pursuant to section 3546(b), including— ‘‘(A) mitigating risks associated with such incidents before substantial damage is done; ‘‘(B) notifying and consulting with the Federal information security", "quality": {"lang": "en", "gibberish_score": 0.0188, "char_len": 2506, "word_len": 350}, "dedupe": {"exact_hash": "a2575e50809ad8faf66b7aa0edd788aa6dc3ff1290518b2c5fd3f4a013cd05a4", "simhash64": 14105097356303746246}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:b65c08ef534e50176055a792fc2ce757c13eff3b6ba5d785c4925c99518a1570", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 74, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "security policies, procedures, and practices of the agency; ‘‘(7) procedures for detecting, reporting, and responding to security incidents, consistent with standards and guidelines issued pursuant to section 3546(b), including— ‘‘(A) mitigating risks associated with such incidents before substantial damage is done; ‘‘(B) notifying and consulting with the Federal information security incident center referred to in section 3546; and ‘‘(C) notifying and consulting with, as appropriate— ‘‘(i) law enforcement agencies and relevant Offices of Inspector General; ‘‘(ii) an office designated by the President for any incident involving a national security system; and ‘‘(iii) any other agency or office, in accordance with law or as directed by the President; and ‘‘(8) plans and procedures to ensure continuity of operations for information systems that support the operations and assets of the agency. ‘‘(c) AGENCY REPORTING.—Each agency shall— ‘‘(1) report annually to the Director, the Committees on Government Reform and Science of the House of Representatives, the Committees on Governmental Affairs and Commerce, Science, and Transportation of the Senate, the appropriate VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00053 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2952 authorization and appropriations committees of Congress, and the Comptroller General on the adequacy and effectiveness of information security policies, procedures, and practices, and compliance with the requirements of this subchapter, including compliance with each requirement of subsection (b); ‘‘(2) address the adequacy and effectiveness of information security policies, procedures, and practices in plans and reports relating to— ‘‘(A) annual agency budgets; ‘‘(B) information resources management under subchapter 1 of this chapter; ‘‘(C) information technology management under subtitle III of title 40; ‘‘(D) program performance under sections 1105 and 1115 through 1119 of title 31, and sections 2801 and 2805 of title 39; ‘‘(E) financial management under chapter 9 of title 31, and the Chief Financial Officers Act of 1990 (31 U.S.C. 501 note; Public Law 101–576) (and the amendments made by that Act); ‘‘(F) financial management systems under the Federal Financial Management Improvement Act (31 U.S.C. 3512 note); and ‘‘(G) internal accounting and administrative controls under section 3512 of title 31, (known as", "quality": {"lang": "en", "gibberish_score": 0.0181, "char_len": 2431, "word_len": 350}, "dedupe": {"exact_hash": "9cee52893d0c51fc4095e5c5c4db2cdd95274eee8e6cb492793ddb10e58742f6", "simhash64": 15258013363369510038}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:995947b5ea3eee0c1dac86b1d24a672bc1958b29bca2083d913299755e09d3d4", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 75, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "Chief Financial Officers Act of 1990 (31 U.S.C. 501 note; Public Law 101–576) (and the amendments made by that Act); ‘‘(F) financial management systems under the Federal Financial Management Improvement Act (31 U.S.C. 3512 note); and ‘‘(G) internal accounting and administrative controls under section 3512 of title 31, (known as the ‘Federal Managers Financial Integrity Act’); and ‘‘(3) report any significant deficiency in a policy, procedure, or practice identified under paragraph (1) or (2)— ‘‘(A) as a material weakness in reporting under section 3512 of title 31; and ‘‘(B) if relating to financial management systems, as an instance of a lack of substantial compliance under the Federal Financial Management Improvement Act (31 U.S.C. 3512 note). ‘‘(d) PERFORMANCE PLAN.—(1) In addition to the requirements of subsection (c), each agency, in consultation with the Director, shall include as part of the performance plan required under section 1115 of title 31 a description of— ‘‘(A) the time periods, and ‘‘(B) the resources, including budget, staffing, and training, that are necessary to implement the program required under subsection (b). ‘‘(2) The description under paragraph (1) shall be based on the risk assessments required under subsection (b)(2)(1). ‘‘(e) PUBLIC NOTICE AND COMMENT.—Each agency shall provide the public with timely notice and opportunities for comment on proposed information security policies and procedures to the extent that such policies and procedures affect communication with the public. ‘‘§ 3545. Annual independent evaluation ‘‘(a) IN GENERAL.—(1) Each year each agency shall have performed an independent evaluation of the information security program and practices of that agency to determine the effectiveness of such program and practices. ‘‘(2) Each evaluation under this section shall include— VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00054 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2953 ‘‘(A) testing of the effectiveness of information security policies, procedures, and practices of a representative subset of the agency’s information systems; ‘‘(B) an assessment (made on the basis of the results of the testing) of compliance with— ‘‘(i) the requirements of this subchapter; and ‘‘(ii) related information security policies, procedures, standards, and guidelines; and ‘‘(C) separate", "quality": {"lang": "en", "gibberish_score": 0.0211, "char_len": 2371, "word_len": 350}, "dedupe": {"exact_hash": "b1093416a42bf84ba776b86a3594764a27d01269796a0fe072a537a0f549efef", "simhash64": 14032176645680235542}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:491a4aeece1e8d0306e8506d95f4495e7f5600a994533e7f49ad0a380933caa4", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 76, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "information security policies, procedures, and practices of a representative subset of the agency’s information systems; ‘‘(B) an assessment (made on the basis of the results of the testing) of compliance with— ‘‘(i) the requirements of this subchapter; and ‘‘(ii) related information security policies, procedures, standards, and guidelines; and ‘‘(C) separate presentations, as appropriate, regarding information security relating to national security systems. ‘‘(b) INDEPENDENT AUDITOR.—Subject to subsection (c)— ‘‘(1) for each agency with an Inspector General appointed under the Inspector General Act of 1978, the annual evaluation required by this section shall be performed by the Inspector General or by an independent external auditor, as determined by the Inspector General of the agency; and ‘‘(2) for each agency to which paragraph (1) does not apply, the head of the agency shall engage an independent external auditor to perform the evaluation. ‘‘(c) NATIONAL SECURITY SYSTEMS.—For each agency operating or exercising control of a national security system, that portion of the evaluation required by this section directly relating to a national security system shall be performed— ‘‘(1) only by an entity designated by the agency head; and ‘‘(2) in such a manner as to ensure appropriate protection for information associated with any information security vulnerability in such system commensurate with the risk and in accordance with all applicable laws. ‘‘(d) EXISTING EVALUATIONS.—The evaluation required by this section may be based in whole or in part on an audit, evaluation, or report relating to programs or practices of the applicable agency. ‘‘(e) AGENCY REPORTING.—(1) Each year, not later than such date established by the Director, the head of each agency shall submit to the Director the results of the evaluation required under this section. ‘‘(2) To the extent an evaluation required under this section directly relates to a national security system, the evaluation results submitted to the Director shall contain only a summary and assessment of that portion of the evaluation directly relating to a national security system. ‘‘(f) PROTECTION OF INFORMATION.—Agencies and evaluators shall take appropriate steps to ensure the protection of information which, if disclosed, may adversely affect information security. Such protections", "quality": {"lang": "en", "gibberish_score": 0.0157, "char_len": 2364, "word_len": 350}, "dedupe": {"exact_hash": "dc97e389b625bdf7a63ad6bcb55dd18bdbfc86b8f9b2fbb119a91b2816aee7dc", "simhash64": 11222211886590050438}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:fb45fc9f6feed8c4298b7feaeef8ea879da4db00c2aceae8bb33b69978889508", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 77, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "submitted to the Director shall contain only a summary and assessment of that portion of the evaluation directly relating to a national security system. ‘‘(f) PROTECTION OF INFORMATION.—Agencies and evaluators shall take appropriate steps to ensure the protection of information which, if disclosed, may adversely affect information security. Such protections shall be commensurate with the risk and comply with all applicable laws and regulations. ‘‘(g) OMB REPORTS TO CONGRESS.—(1) The Director shall summarize the results of the evaluations conducted under this section in the report to Congress required under section 3543(a)(8). ‘‘(2) The Director’s report to Congress under this subsection shall summarize information regarding information security relating to national security systems in such a manner as to ensure appropriate protection for information associated with any information security vulnerability in such system commensurate with the risk and in accordance with all applicable laws. ‘‘(3) Evaluations and any other descriptions of information systems under the authority and control of the Director of Central Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00055 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2954 Intelligence or of National Foreign Intelligence Programs systems under the authority and control of the Secretary of Defense shall be made available to Congress only through the appropriate oversight committees of Congress, in accordance with applicable laws. ‘‘(h) COMPTROLLER GENERAL.—The Comptroller General shall periodically evaluate and report to Congress on— ‘‘(1) the adequacy and effectiveness of agency information security policies and practices; and ‘‘(2) implementation of the requirements of this subchapter. ‘‘§ 3546. Federal information security incident center ‘‘(a) IN GENERAL.—The Director shall ensure the operation of a central Federal information security incident center to— ‘‘(1) provide timely technical assistance to operators of agency information systems regarding security incidents, including guidance on detecting and handling information security incidents; ‘‘(2) compile and analyze information about incidents that threaten information security; ‘‘(3) inform operators of agency information systems about current and potential information security threats, and vulnerabilities; and ‘‘(4) consult with the National Institute of Standards and Technology, agencies or offices operating or exercising control of national security systems (including", "quality": {"lang": "en", "gibberish_score": 0.0142, "char_len": 2544, "word_len": 350}, "dedupe": {"exact_hash": "4b3bd21dc5ce79f271296ce2f656a87cf284942eac7dae5d353f8631f8030a83", "simhash64": 15847427462168937606}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f74ffbbe8206cb03c9c2b41c7326a16d0482328e4f18f7a6a568a8b0ab67257b", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 78, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "‘‘(2) compile and analyze information about incidents that threaten information security; ‘‘(3) inform operators of agency information systems about current and potential information security threats, and vulnerabilities; and ‘‘(4) consult with the National Institute of Standards and Technology, agencies or offices operating or exercising control of national security systems (including the National Security Agency), and such other agencies or offices in accordance with law and as directed by the President regarding information security incidents and related matters. ‘‘(b) NATIONAL SECURITY SYSTEMS.—Each agency operating or exercising control of a national security system shall share information about information security incidents, threats, and vulnerabilities with the Federal information security incident center to the extent consistent with standards and guidelines for national security systems, issued in accordance with law and as directed by the President. ‘‘§ 3547. National security systems ‘‘The head of each agency operating or exercising control of a national security system shall be responsible for ensuring that the agency— ‘‘(1) provides information security protections commensurate with the risk and magnitude of the harm resulting from the unauthorized access, use, disclosure, disruption, modification, or destruction of the information contained in such system; ‘‘(2) implements information security policies and practices as required by standards and guidelines for national security systems, issued in accordance with law and as directed by the President; and ‘‘(3) complies with the requirements of this subchapter. ‘‘§ 3548. Authorization of appropriations ‘‘There are authorized to be appropriated to carry out the provisions of this subchapter such sums as may be necessary for each of fiscal years 2003 through 2007. Reports. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00056 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2955 ‘‘§ 3549. Effect on existing law ‘‘Nothing in this subchapter, section 11331 of title 40, or section 20 of the National Standards and Technology Act (15 U.S.C. 278g– 3) may be construed as affecting the authority of the President, the Office of Management and Budget or the Director thereof, the National Institute of Standards and Technology, or the head of any agency, with respect to the authorized use or", "quality": {"lang": "en", "gibberish_score": 0.0142, "char_len": 2398, "word_len": 350}, "dedupe": {"exact_hash": "f322b2b9c4b01d6ae36ee556d3d447ff7327fc003c308a75305aea5f9e14f0b3", "simhash64": 15262217895808713862}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:75c4b452e1cdc6132a5442f6b8d670caa941428c1c34ae48d3b9da5160dac5d9", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 80, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "OF THE PROGRAM.—’’; (ii) in subsection (b), by striking paragraph (2); and (iii) in subsection (c), in the matter preceding paragraph (1), by inserting ‘‘, including through compliance with subchapter III of chapter 35 of title 44’’ after ‘‘infrastructure’’. (2) ATOMIC ENERGY ACT OF 1954.—Nothing in this Act shall supersede any requirement made by or under the Atomic Energy Act of 1954 (42 U.S.C. 2011 et seq.). Restricted data or formerly restricted data shall be handled, protected, classified, downgraded, and declassified in conformity with the Atomic Energy Act of 1954 (42 U.S.C. 2011 et seq.). 44 USC 3501 note. 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00057 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2956 SEC. 302. MANAGEMENT OF INFORMATION TECHNOLOGY. (a) IN GENERAL.—Section 11331 of title 40, United States Code, is amended to read as follows: ‘‘§ 11331. Responsibilities for Federal information systems standards ‘‘(a) STANDARDS AND GUIDELINES.— ‘‘(1) AUTHORITY TO PRESCRIBE.—Except as provided under paragraph (2), the Secretary of Commerce shall, on the basis of standards and guidelines developed by the National Institute of Standards and Technology pursuant to paragraphs (2) and (3) of section 20(a) of the National Institute of Standards and Technology Act (15 U.S.C. 278g–3(a)), prescribe standards and guidelines pertaining to Federal information systems. ‘‘(2) NATIONAL SECURITY SYSTEMS.—Standards and guidelines for national security systems (as defined under this section) shall be developed, prescribed, enforced, and overseen as otherwise authorized by law and as directed by the President. ‘‘(b) MANDATORY REQUIREMENTS.— ‘‘(1) AUTHORITY TO MAKE MANDATORY.—Except as provided under paragraph (2), the Secretary shall make standards prescribed under subsection (a)(1) compulsory and binding to the extent determined necessary by the Secretary to improve the efficiency of operation or security of Federal information systems. ‘‘(2) REQUIRED MANDATORY STANDARDS.—(A) Standards prescribed under subsection (a)(1) shall include information security standards that— ‘‘(i) provide minimum information security requirements as determined under section 20(b) of the National Institute of Standards and Technology Act (15 U.S.C. 278g– 3(b)); and ‘‘(ii) are otherwise necessary to improve the security of Federal information and information systems. ‘‘(B)", "quality": {"lang": "en", "gibberish_score": 0.0186, "char_len": 2423, "word_len": 350}, "dedupe": {"exact_hash": "bbe94bd8bf787910486fb03ee2e9ac52f3f5ec380fddaff5c1825ce2167fd211", "simhash64": 14694218987191764118}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:951fbb467de7b86666306129cc475b3828506d58289988435a3b97207d177547", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 81, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "prescribed under subsection (a)(1) shall include information security standards that— ‘‘(i) provide minimum information security requirements as determined under section 20(b) of the National Institute of Standards and Technology Act (15 U.S.C. 278g– 3(b)); and ‘‘(ii) are otherwise necessary to improve the security of Federal information and information systems. ‘‘(B) Information security standards described in subparagraph (A) shall be compulsory and binding. ‘‘(c) AUTHORITY TO DISAPPROVE OR MODIFY.—The President may disapprove or modify the standards and guidelines referred to in subsection (a)(1) if the President determines such action to be in the public interest. The President’s authority to disapprove or modify such standards and guidelines may not be delegated. Notice of such disapproval or modification shall be published promptly in the Federal Register. Upon receiving notice of such disapproval or modification, the Secretary of Commerce shall immediately rescind or modify such standards or guidelines as directed by the President. ‘‘(d) EXERCISE OF AUTHORITY.—To ensure fiscal and policy consistency, the Secretary shall exercise the authority conferred by this section subject to direction by the President and in coordination with the Director of the Office of Management and Budget. ‘‘(e) APPLICATION OF MORE STRINGENT STANDARDS.—The head of an executive agency may employ standards for the cost-effective information security for information systems within or under the supervision of that agency that are more stringent than the standards the Secretary prescribes under this section if the more stringent standards— Federal Register, publication. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00058 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2957 ‘‘(1) contain at least the applicable standards made compulsory and binding by the Secretary; and ‘‘(2) are otherwise consistent with policies and guidelines issued under section 3543 of title 44. ‘‘(f) DECISIONS ON PROMULGATION OF STANDARDS.—The decision by the Secretary regarding the promulgation of any standard under this section shall occur not later than 6 months after the submission of the proposed standard to the Secretary by the National Institute of Standards and Technology, as provided under section 20 of the National Institute of Standards and Technology Act (15 U.S.C. 278g–3). ‘‘(g)", "quality": {"lang": "en", "gibberish_score": 0.0129, "char_len": 2402, "word_len": 350}, "dedupe": {"exact_hash": "db998d5dd3c4ea263e0c8a0913d75d1c072973b374da0919770312374c074c99", "simhash64": 10640966126254539926}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7679f139fd4b6340a61e2b0891ce939c0de3ef202daa204919ef364fd0f77124", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 82, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "promulgation of any standard under this section shall occur not later than 6 months after the submission of the proposed standard to the Secretary by the National Institute of Standards and Technology, as provided under section 20 of the National Institute of Standards and Technology Act (15 U.S.C. 278g–3). ‘‘(g) DEFINITIONS.—In this section: ‘‘(1) FEDERAL INFORMATION SYSTEM.—The term ‘Federal information system’ means an information system used or operated by an executive agency, by a contractor of an executive agency, or by another organization on behalf of an executive agency. ‘‘(2) INFORMATION SECURITY.—The term ‘information security’ has the meaning given that term in section 3542(b)(1) of title 44. ‘‘(3) NATIONAL SECURITY SYSTEM.—The term ‘national security system’ has the meaning given that term in section 3542(b)(2) of title 44.’’. (b) CLERICAL AMENDMENT.—The item relating to section 11331 in the table of sections at the beginning of chapter 113 of such title is amended to read as follows: ‘‘11331. Responsibilities for Federal information systems standards.’’. SEC. 303. NATIONAL INSTITUTE OF STANDARDS AND TECHNOLOGY. Section 20 of the National Institute of Standards and Technology Act (15 U.S.C. 278g–3), is amended by striking the text and inserting the following: ‘‘(a) IN GENERAL.—The Institute shall— ‘‘(1) have the mission of developing standards, guidelines, and associated methods and techniques for information systems; ‘‘(2) develop standards and guidelines, including minimum requirements, for information systems used or operated by an agency or by a contractor of an agency or other organization on behalf of an agency, other than national security systems (as defined in section 3542(b)(2) of title 44, United States Code); and ‘‘(3) develop standards and guidelines, including minimum requirements, for providing adequate information security for all agency operations and assets, but such standards and guidelines shall not apply to national security systems. ‘‘(b) MINIMUM REQUIREMENTS FOR STANDARDS AND GUIDELINES.—The standards and guidelines required by subsection (a) shall include, at a minimum— ‘‘(1)(A) standards to be used by all agencies to categorize all information and information systems collected or maintained by or on behalf of each agency based on the objectives of providing appropriate levels of information security", "quality": {"lang": "en", "gibberish_score": 0.0181, "char_len": 2378, "word_len": 350}, "dedupe": {"exact_hash": "ba94d15df190195f623021e119a39ded411e1245e177c5a30ae705f0999a3ef8", "simhash64": 14104234239718966422}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:b8b3f20848b12ec3dcd888422fc52f3690270acc06dc911a50925c88c2b62b9c", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 83, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "AND GUIDELINES.—The standards and guidelines required by subsection (a) shall include, at a minimum— ‘‘(1)(A) standards to be used by all agencies to categorize all information and information systems collected or maintained by or on behalf of each agency based on the objectives of providing appropriate levels of information security according to a range of risk levels; ‘‘(B) guidelines recommending the types of information and information systems to be included in each such category; and Deadline. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00059 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2958 ‘‘(C) minimum information security requirements for information and information systems in each such category; ‘‘(2) a definition of and guidelines concerning detection and handling of information security incidents; and ‘‘(3) guidelines developed in conjunction with the Department of Defense, including the National Security Agency, for identifying an information system as a national security system consistent with applicable requirements for national security systems, issued in accordance with law and as directed by the President. ‘‘(c) DEVELOPMENT OF STANDARDS AND GUIDELINES.—In developing standards and guidelines required by subsections (a) and (b), the Institute shall— ‘‘(1) consult with other agencies and offices and the private sector (including the Director of the Office of Management and Budget, the Departments of Defense and Energy, the National Security Agency, the General Accounting Office, and the Secretary of Homeland Security) to assure— ‘‘(A) use of appropriate information security policies, procedures, and techniques, in order to improve information security and avoid unnecessary and costly duplication of effort; and ‘‘(B) that such standards and guidelines are complementary with standards and guidelines employed for the protection of national security systems and information contained in such systems; ‘‘(2) provide the public with an opportunity to comment on proposed standards and guidelines; ‘‘(3) submit to the Secretary of Commerce for promulgation under section 11331 of title 40, United States Code— ‘‘(A) standards, as required under subsection (b)(1)(A), no later than 12 months after the date of the enactment of this section; and ‘‘(B) minimum information security requirements for each category, as required under subsection (b)(1)(C), no", "quality": {"lang": "en", "gibberish_score": 0.0141, "char_len": 2418, "word_len": 350}, "dedupe": {"exact_hash": "75c9c487cf12cabc0ff20d847e048d43b6b8b83db84a89d2ec5a75d577874b26", "simhash64": 15252652144647013510}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8d687d4fbb8ff1e7c8cb9bc11ae0a49656174a07d4352f438c75ed013c3bcea6", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 84, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "Secretary of Commerce for promulgation under section 11331 of title 40, United States Code— ‘‘(A) standards, as required under subsection (b)(1)(A), no later than 12 months after the date of the enactment of this section; and ‘‘(B) minimum information security requirements for each category, as required under subsection (b)(1)(C), no later than 36 months after the date of the enactment of this section; ‘‘(4) issue guidelines as required under subsection (b)(1)(B), no later than 18 months after the date of the enactment of this section; ‘‘(5) to the maximum extent practicable, ensure that such standards and guidelines do not require the use or procurement of specific products, including any specific hardware or software; ‘‘(6) to the maximum extent practicable, ensure that such standards and guidelines provide for sufficient flexibility to permit alternative solutions to provide equivalent levels of protection for identified information security risks; and ‘‘(7) to the maximum extent practicable, use flexible, performance-based standards and guidelines that permit the use of off-the-shelf commercially developed information security products. ‘‘(d) INFORMATION SECURITY FUNCTIONS.—The Institute shall— ‘‘(1) submit standards developed pursuant to subsection (a), along with recommendations as to the extent to which these should be made compulsory and binding, to the Secretary of Commerce for promulgation under section 11331 of title 40, United States Code; Deadline. Deadlines. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00060 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2959 ‘‘(2) provide technical assistance to agencies, upon request, regarding— ‘‘(A) compliance with the standards and guidelines developed under subsection (a); ‘‘(B) detecting and handling information security incidents; and ‘‘(C) information security policies, procedures, and practices; ‘‘(3) conduct research, as needed, to determine the nature and extent of information security vulnerabilities and techniques for providing cost-effective information security; ‘‘(4) develop and periodically revise performance indicators and measures for agency information security policies and practices; ‘‘(5) evaluate private sector information security policies and practices and commercially available information technologies to assess potential application by agencies to strengthen information security; ‘‘(6) assist the private sector, upon request, in using and applying the results of activities under this", "quality": {"lang": "en", "gibberish_score": 0.0151, "char_len": 2524, "word_len": 350}, "dedupe": {"exact_hash": "8bd3c1cd2751b327b8985c2c0aab61b5a5d16b545d4615b869107192bd82bd13", "simhash64": 10063678541172425862}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a4e5aead762617551ee19c92bbd96daa6d49b0397de587124a5003859732633b", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 85, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "and measures for agency information security policies and practices; ‘‘(5) evaluate private sector information security policies and practices and commercially available information technologies to assess potential application by agencies to strengthen information security; ‘‘(6) assist the private sector, upon request, in using and applying the results of activities under this section; ‘‘(7) evaluate security policies and practices developed for national security systems to assess potential application by agencies to strengthen information security; ‘‘(8) periodically assess the effectiveness of standards and guidelines developed under this section and undertake revisions as appropriate; ‘‘(9) solicit and consider the recommendations of the Information Security and Privacy Advisory Board, established by section 21, regarding standards and guidelines developed under subsection (a) and submit such recommendations to the Secretary of Commerce with such standards submitted to the Secretary; and ‘‘(10) prepare an annual public report on activities undertaken in the previous year, and planned for the coming year, to carry out responsibilities under this section. ‘‘(e) DEFINITIONS.—As used in this section— ‘‘(1) the term ‘agency’ has the same meaning as provided in section 3502(1) of title 44, United States Code; ‘‘(2) the term ‘information security’ has the same meaning as provided in section 3542(b)(1) of such title; ‘‘(3) the term ‘information system’ has the same meaning as provided in section 3502(8) of such title; ‘‘(4) the term ‘information technology’ has the same meaning as provided in section 11101 of title 40, United States Code; and ‘‘(5) the term ‘national security system’ has the same meaning as provided in section 3542(b)(2) of title 44, United States Code. ‘‘(f) AUTHORIZATION OF APPROPRIATIONS.—There are authorized to be appropriated to the Secretary of Commerce $20,000,000 for each of fiscal years 2003, 2004, 2005, 2006, and 2007 to enable the National Institute of Standards and Technology to carry out the provisions of this section.’’. SEC. 304. INFORMATION SECURITY AND PRIVACY ADVISORY BOARD. Section 21 of the National Institute of Standards and Technology Act (15 U.S.C. 278g–4), is amended— VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00061 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2960 (1) in subsection", "quality": {"lang": "en", "gibberish_score": 0.0193, "char_len": 2386, "word_len": 350}, "dedupe": {"exact_hash": "feba92d1299cc828e9ad18cea1871322b27809ba7ef22edf2d18f6c45c54c396", "simhash64": 10087322370496382102}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4fc91861b658b319bfb9d3ff6bb90d80638bfb3ab08a556d9274457cec6397d5", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 86, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "of this section.’’. SEC. 304. INFORMATION SECURITY AND PRIVACY ADVISORY BOARD. Section 21 of the National Institute of Standards and Technology Act (15 U.S.C. 278g–4), is amended— VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00061 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2960 (1) in subsection (a), by striking ‘‘Computer System Security and Privacy Advisory Board’’ and inserting ‘‘Information Security and Privacy Advisory Board’’; (2) in subsection (a)(1), by striking ‘‘computer or telecommunications’’ and inserting ‘‘information technology’’; (3) in subsection (a)(2)— (A) by striking ‘‘computer or telecommunications technology’’ and inserting ‘‘information technology’’; and (B) by striking ‘‘computer or telecommunications equipment’’ and inserting ‘‘information technology’’; (4) in subsection (a)(3)— (A) by striking ‘‘computer systems’’ and inserting ‘‘information system’’; and (B) by striking ‘‘computer systems security’’ and inserting ‘‘information security’’; (5) in subsection (b)(1) by striking ‘‘computer systems security’’ and inserting ‘‘information security’’; (6) in subsection (b) by striking paragraph (2) and inserting the following: ‘‘(2) to advise the Institute, the Secretary of Commerce, and the Director of the Office of Management and Budget on information security and privacy issues pertaining to Federal Government information systems, including through review of proposed standards and guidelines developed under section 20; and’’; (7) in subsection (b)(3) by inserting ‘‘annually’’ after ‘‘report’’; (8) by inserting after subsection (e) the following new subsection: ‘‘(f) The Board shall hold meetings at such locations and at such time and place as determined by a majority of the Board.’’; (9) by redesignating subsections (f) and (g) as subsections (g) and (h), respectively; and (10) by striking subsection (h), as redesignated by paragraph (9), and inserting the following: ‘‘(h) As used in this section, the terms ‘information system’ and ‘information technology’ have the meanings given in section 20.’’. SEC. 305. TECHNICAL AND CONFORMING AMENDMENTS. (a) COMPUTER SECURITY ACT.—Section 11332 of title 40, United States Code, and the item relating to that section in the table of sections for chapter 113 of such title, are repealed. (b) FLOYD D. SPENCE NATIONAL DEFENSE AUTHORIZATION ACT FOR FISCAL YEAR 2001.—The Floyd D. Spence National Defense Authorization Act for Fiscal Year", "quality": {"lang": "en", "gibberish_score": 0.0366, "char_len": 2457, "word_len": 350}, "dedupe": {"exact_hash": "0df188c0ada7a3161307b2c50e77ba4bc9791609ac0f23cf0553e6474b7c7eb7", "simhash64": 14843049978499007686}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:06b78a7170d4e80ad59f73ed3bccc7601151eaf1a106d008a58c3cd253dd2a75", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 87, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "ACT.—Section 11332 of title 40, United States Code, and the item relating to that section in the table of sections for chapter 113 of such title, are repealed. (b) FLOYD D. SPENCE NATIONAL DEFENSE AUTHORIZATION ACT FOR FISCAL YEAR 2001.—The Floyd D. Spence National Defense Authorization Act for Fiscal Year 2001 (Public Law 106–398) is amended by striking section 1062 (44 U.S.C. 3531 note). (c) PAPERWORK REDUCTION ACT.—(1) Section 3504(g) of title 44, United States Code, is amended— (A) by adding ‘‘and’’ at the end of paragraph (1); (B) in paragraph (2)— (i) by striking ‘‘sections 11331 and 11332(b) and (c) of title 40’’ and inserting ‘‘section 11331 of title 40 and subchapter II of this chapter’’; and (ii) by striking ‘‘; and’’ and inserting a period; and (C) by striking paragraph (3). VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00062 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2961 (2) Section 3505 of such title is amended by adding at the end— ‘‘(c) INVENTORY OF MAJOR INFORMATION SYSTEMS.—(1) The head of each agency shall develop and maintain an inventory of major information systems (including major national security systems) operated by or under the control of such agency. ‘‘(2) The identification of information systems in an inventory under this subsection shall include an identification of the interfaces between each such system and all other systems or networks, including those not operated by or under the control of the agency. ‘‘(3) Such inventory shall be— ‘‘(A) updated at least annually; ‘‘(B) made available to the Comptroller General; and ‘‘(C) used to support information resources management, including— ‘‘(i) preparation and maintenance of the inventory of information resources under section 3506(b)(4); ‘‘(ii) information technology planning, budgeting, acquisition, and management under section 3506(h), subtitle III of title 40, and related laws and guidance; ‘‘(iii) monitoring, testing, and evaluation of information security controls under subchapter II; ‘‘(iv) preparation of the index of major information systems required under section 552(g) of title 5, United States Code; and ‘‘(v) preparation of information system inventories required for records management under chapters 21, 29, 31, and", "quality": {"lang": "en", "gibberish_score": 0.022, "char_len": 2270, "word_len": 350}, "dedupe": {"exact_hash": "2944146700eef107649848e0b3459944465742fd0e1f053bb7d71cd876ecc029", "simhash64": 14694148616283325462}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0fae598912181e59dacd956c2f7f51fa6e49408dd254cdb808cd2a0107647783", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 88, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "and guidance; ‘‘(iii) monitoring, testing, and evaluation of information security controls under subchapter II; ‘‘(iv) preparation of the index of major information systems required under section 552(g) of title 5, United States Code; and ‘‘(v) preparation of information system inventories required for records management under chapters 21, 29, 31, and 33. ‘‘(4) The Director shall issue guidance for and oversee the implementation of the requirements of this subsection.’’. (3) Section 3506(g) of such title is amended— (A) by adding ‘‘and’’ at the end of paragraph (1); (B) in paragraph (2)— (i) by striking ‘‘section 11332 of title 40’’ and inserting ‘‘subchapter II of this chapter’’; and (ii) by striking ‘‘; and’’ and inserting a period; and (C) by striking paragraph (3). TITLE IV—AUTHORIZATION OF APPROPRIATIONS AND EFFECTIVE DATES SEC. 401. AUTHORIZATION OF APPROPRIATIONS. Except for those purposes for which an authorization of appropriations is specifically provided in title I or II, including the amendments made by such titles, there are authorized to be appropriated such sums as are necessary to carry out titles I and II for each of fiscal years 2003 through 2007. SEC. 402. EFFECTIVE DATES. (a) TITLES I AND II.— (1) IN GENERAL.—Except as provided under paragraph (2), titles I and II and the amendments made by such titles shall take effect 120 days after the date of enactment of this Act. (2) IMMEDIATE ENACTMENT.—Sections 207, 214, and 215 shall take effect on the date of enactment of this Act. 44 USC 3601 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00063 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2962 (b) TITLES III AND IV.—Title III and this title shall take effect on the date of enactment of this Act. TITLE V—CONFIDENTIAL INFORMATION PROTECTION AND STATISTICAL EFFICIENCY SEC. 501. SHORT TITLE. This title may be cited as the ‘‘Confidential Information Protection and Statistical Efficiency Act of 2002’’. SEC. 502. DEFINITIONS. As used in this title: (1) The term ‘‘agency’’ means any entity that falls within the definition of the term ‘‘executive agency’’ as defined in section 102 of title 31, United States Code,", "quality": {"lang": "en", "gibberish_score": 0.022, "char_len": 2183, "word_len": 350}, "dedupe": {"exact_hash": "7054a4d3335c4f032fa163b8f571f4c0cfb24194e185e5ad8fec67969382165b", "simhash64": 15838700569626248278}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:46d5fdb53a0476edebf432e18e5c85b97f040a8a02d387ea0b5fe1318bdca66e", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 90, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "about businesses, tax-exempt organizations, and government entities. (4) The term ‘‘identifiable form’’ means any representation of information that permits the identity of the respondent to whom the information applies to be reasonably inferred by either direct or indirect means. (5) The term ‘‘nonstatistical purpose’’— (A) means the use of data in identifiable form for any purpose that is not a statistical purpose, including 44 USC 3501 note. 44 USC 3501 note. Confidential Information Protection and Statistical Efficiency Act of 2002. 44 USC 3541 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00064 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2963 any administrative, regulatory, law enforcement, adjudicatory, or other purpose that affects the rights, privileges, or benefits of a particular identifiable respondent; and (B) includes the disclosure under section 552 of title 5, United States Code (popularly known as the Freedom of Information Act) of data that are acquired for exclusively statistical purposes under a pledge of confidentiality. (6) The term ‘‘respondent’’ means a person who, or organization that, is requested or required to supply information to an agency, is the subject of information requested or required to be supplied to an agency, or provides that information to an agency. (7) The term ‘‘statistical activities’’— (A) means the collection, compilation, processing, or analysis of data for the purpose of describing or making estimates concerning the whole, or relevant groups or components within, the economy, society, or the natural environment; and (B) includes the development of methods or resources that support those activities, such as measurement methods, models, statistical classifications, or sampling frames. (8) The term ‘‘statistical agency or unit’’ means an agency or organizational unit of the executive branch whose activities are predominantly the collection, compilation, processing, or analysis of information for statistical purposes. (9) The term ‘‘statistical purpose’’— (A) means the description, estimation, or analysis of the characteristics of groups, without identifying the individuals or organizations that comprise such groups; and (B) includes the development, implementation, or maintenance of methods, technical or administrative procedures, or information resources that support the purposes described in subparagraph (A). SEC.", "quality": {"lang": "en", "gibberish_score": 0.0119, "char_len": 2433, "word_len": 350}, "dedupe": {"exact_hash": "59be9631f01bb33c666e43a76c489330782d5fa82d3e56a8ffdd28224710f1ec", "simhash64": 11208420710095356038}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:408b2bedeb9670304ea9bc0d32f288c2bc5ac68def0e535c9acc7312d8bbc311", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 92, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "entered into pursuant to section 524(a) for the applicable year. (3) The Director of the Office of Management and Budget shall include a summary of reports submitted to the Director under paragraph (2) and actions taken by the Director to advance the purposes of this title in the annual report to the Congress on statistical programs prepared under section 3504(e)(2) of title 44, United States Code. SEC. 504. EFFECT ON OTHER LAWS. (a) TITLE 44, UNITED STATES CODE.—This title, including amendments made by this title, does not diminish the authority under section 3510 of title 44, United States Code, of the Director of the Office of Management and Budget to direct, and of an agency to make, disclosures that are not inconsistent with any applicable law. (b) TITLE 13 AND TITLE 44, UNITED STATES CODE.—This title, including amendments made by this title, does not diminish the authority of the Bureau of the Census to provide information in accordance with sections 8, 16, 301, and 401 of title 13, United States Code, and section 2108 of title 44, United States Code. (c) TITLE 13, UNITED STATES CODE.—This title, including amendments made by this title, shall not be construed as authorizing the disclosure for nonstatistical purposes of demographic data or information collected by the Census Bureau pursuant to section 9 of title 13, United States Code. (d) VARIOUS ENERGY STATUTES.—Data or information acquired by the Energy Information Administration under a pledge of confidentiality and designated by the Energy Information Administration to be used for exclusively statistical purposes shall not be disclosed in identifiable form for nonstatistical purposes under— (1) section 12, 20, or 59 of the Federal Energy Administration Act of 1974 (15 U.S.C. 771, 779, 790h); (2) section 11 of the Energy Supply and Environmental Coordination Act of 1974 (15 U.S.C. 796); or (3) section 205 or 407 of the Department of the Energy Organization Act of 1977 (42 U.S.C. 7135, 7177). (e) SECTION 201 OF CONGRESSIONAL BUDGET ACT OF 1974.— This title, including amendments made by this title, shall not be construed to limit any authorities of the Congressional Budget", "quality": {"lang": "en", "gibberish_score": 0.0028, "char_len": 2167, "word_len": 350}, "dedupe": {"exact_hash": "9c03ff1c767f8c3413f475669468bafed80a9d53512fd12eeb6d1654de282270", "simhash64": 11234593485435732998}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:377a6187685d3645a1667b27bd3b5705e4a07aa0d4b47c59132fe6d6ce6fa025", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 94, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "of 1986 (26 U.S.C. 6103). (i) AUTHORITY OF CONGRESS.—Nothing in this title shall be construed to affect the authority of the Congress, including its committees, members, or agents, to obtain data or information for a statistical purpose, including for oversight of an agency’s statistical activities. Subtitle A—Confidential Information Protection SEC. 511. FINDINGS AND PURPOSES. (a) FINDINGS.—The Congress finds the following: (1) Individuals, businesses, and other organizations have varying degrees of legal protection when providing information to the agencies for strictly statistical purposes. (2) Pledges of confidentiality by agencies provide assurances to the public that information about individuals or organizations or provided by individuals or organizations for exclusively statistical purposes will be held in confidence and will not be used against such individuals or organizations in any agency action. (3) Protecting the confidentiality interests of individuals or organizations who provide information under a pledge of confidentiality for Federal statistical programs serves both the interests of the public and the needs of society. (4) Declining trust of the public in the protection of information provided under a pledge of confidentiality to the agencies adversely affects both the accuracy and completeness of statistical analyses. (5) Ensuring that information provided under a pledge of confidentiality for statistical purposes receives protection is essential in continuing public cooperation in statistical programs. (b) PURPOSES.—The purposes of this subtitle are the following: (1) To ensure that information supplied by individuals or organizations to an agency for statistical purposes under a pledge of confidentiality is used exclusively for statistical purposes. (2) To ensure that individuals or organizations who supply information under a pledge of confidentiality to agencies for statistical purposes will neither have that information disclosed 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00067 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2966 in identifiable form to anyone not authorized by this title nor have that information used for any purpose other than a statistical purpose. (3) To safeguard the confidentiality of individually identifiable information acquired under a pledge of confidentiality for statistical purposes by controlling access to, and uses made of,", "quality": {"lang": "en", "gibberish_score": 0.0028, "char_len": 2462, "word_len": 350}, "dedupe": {"exact_hash": "b8c5f5f36b82726e5d3d51cc3da6031dfe3c4989371d2a722036d507268a059f", "simhash64": 11217427909350114310}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:262761b41064542a389a52ecd11592f55290469361a7724ee6e0228bca6a65ae", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 96, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "agent under that section, who may perform exclusively statistical activities, subject to the limitations and penalties described in this title. SEC. 513. FINES AND PENALTIES. Whoever, being an officer, employee, or agent of an agency acquiring information for exclusively statistical purposes, having taken and subscribed the oath of office, or having sworn to observe the limitations imposed by section 512, comes into possession of such information by reason of his or her being an officer, employee, or agent and, knowing that the disclosure of the specific information is prohibited under the provisions of this title, willfully discloses the information in any manner to a person or agency not entitled to receive it, shall be guilty of a class E felony and imprisoned for not more than 5 years, or fined not more than $250,000, or both. 44 USC 3501 note. 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00068 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2967 Subtitle B—Statistical Efficiency SEC. 521. FINDINGS AND PURPOSES. (a) FINDINGS.—The Congress finds the following: (1) Federal statistics are an important source of information for public and private decision-makers such as policymakers, consumers, businesses, investors, and workers. (2) Federal statistical agencies should continuously seek to improve their efficiency. Statutory constraints limit the ability of these agencies to share data and thus to achieve higher efficiency for Federal statistical programs. (3) The quality of Federal statistics depends on the willingness of businesses to respond to statistical surveys. Reducing reporting burdens will increase response rates, and therefore lead to more accurate characterizations of the economy. (4) Enhanced sharing of business data among the Bureau of the Census, the Bureau of Economic Analysis, and the Bureau of Labor Statistics for exclusively statistical purposes will improve their ability to track more accurately the large and rapidly changing nature of United States business. In particular, the statistical agencies will be able to better ensure that businesses are consistently classified in appropriate industries, resolve data anomalies, produce statistical samples that are consistently adjusted for the entry and exit of new businesses in a", "quality": {"lang": "en", "gibberish_score": 0.0022, "char_len": 2322, "word_len": 350}, "dedupe": {"exact_hash": "6f60399f794ed49ed35fc1956a766a04b34fe518341d8ddeed79afa698112ad2", "simhash64": 11225308109151782022}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:cc8fbb134baa09057a8bc5836f39e402853323fd992e816cab8d62ee494e8bc7", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 97, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "accurately the large and rapidly changing nature of United States business. In particular, the statistical agencies will be able to better ensure that businesses are consistently classified in appropriate industries, resolve data anomalies, produce statistical samples that are consistently adjusted for the entry and exit of new businesses in a timely manner, and correct faulty reporting errors quickly and efficiently. (5) The Congress enacted the International Investment and Trade in Services Act of 1990 that allowed the Bureau of the Census, the Bureau of Economic Analysis, and the Bureau of Labor Statistics to share data on foreign-owned companies. The Act not only expanded detailed industry coverage from 135 industries to over 800 industries with no increase in the data collected from respondents but also demonstrated how data sharing can result in the creation of valuable data products. (6) With subtitle A of this title, the sharing of business data among the Bureau of the Census, the Bureau of Economic Analysis, and the Bureau of Labor Statistics continues to ensure the highest level of confidentiality for respondents to statistical surveys. (b) PURPOSES.—The purposes of this subtitle are the following: (1) To authorize the sharing of business data among the Bureau of the Census, the Bureau of Economic Analysis, and the Bureau of Labor Statistics for exclusively statistical purposes. (2) To reduce the paperwork burdens imposed on businesses that provide requested information to the Federal Government. (3) To improve the comparability and accuracy of Federal economic statistics by allowing the Bureau of the Census, the Bureau of Economic Analysis, and the Bureau of Labor Statistics to update sample frames, develop consistent classifications of establishments and companies into industries, improve coverage, and reconcile significant differences in data produced by the three agencies. (4) To increase understanding of the United States economy, especially for key industry and regional statistics, 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00069 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2968 to develop more accurate measures of the impact of technology on productivity growth, and to enhance the reliability of the Nation’s", "quality": {"lang": "en", "gibberish_score": 0.0017, "char_len": 2308, "word_len": 350}, "dedupe": {"exact_hash": "bd45758ae948d6c571078b53f579adb439e6ee4426c3c3695c17ca4b170e3130", "simhash64": 10655035477008417798}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a37d721225a14b6b916bc86c845bd1850a1d23e24899aed85e319f0720714d6a", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 98, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "key industry and regional statistics, 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00069 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2968 to develop more accurate measures of the impact of technology on productivity growth, and to enhance the reliability of the Nation’s most important economic indicators, such as the National Income and Product Accounts. SEC. 522. DESIGNATION OF STATISTICAL AGENCIES. For purposes of this subtitle, the term ‘‘Designated Statistical Agency’’ means each of the following: (1) The Bureau of the Census of the Department of Commerce. (2) The Bureau of Economic Analysis of the Department of Commerce. (3) The Bureau of Labor Statistics of the Department of Labor. SEC. 523. RESPONSIBILITIES OF DESIGNATED STATISTICAL AGENCIES. The head of each of the Designated Statistical Agencies shall— (1) identify opportunities to eliminate duplication and otherwise reduce reporting burden and cost imposed on the public in providing information for statistical purposes; (2) enter into joint statistical projects to improve the quality and reduce the cost of statistical programs; and (3) protect the confidentiality of individually identifiable information acquired for statistical purposes by adhering to safeguard principles, including— (A) emphasizing to their officers, employees, and agents the importance of protecting the confidentiality of information in cases where the identity of individual respondents can reasonably be inferred by either direct or indirect means; (B) training their officers, employees, and agents in their legal obligations to protect the confidentiality of individually identifiable information and in the procedures that must be followed to provide access to such information; (C) implementing appropriate measures to assure the physical and electronic security of confidential data; (D) establishing a system of records that identifies individuals accessing confidential data and the project for which the data were required; and (E) being prepared to document their compliance with safeguard principles to other agencies authorized by law to monitor such compliance. SEC. 524. SHARING OF BUSINESS DATA AMONG DESIGNATED STATISTICAL AGENCIES. (a) IN GENERAL.—A Designated Statistical Agency may provide business data in an identifiable form to another Designated Statistical Agency under the terms of a written", "quality": {"lang": "en", "gibberish_score": 0.0041, "char_len": 2411, "word_len": 350}, "dedupe": {"exact_hash": "134717818c7c6ade4f8ba8c092a2ae8ab38ba897b9898d0d9f1646d7f6da9c49", "simhash64": 11213487328361038854}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f2ef52368e37fa60c6adf83ffca8d098550dc3f852c22e14235c937d10413099", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 99, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "their compliance with safeguard principles to other agencies authorized by law to monitor such compliance. SEC. 524. SHARING OF BUSINESS DATA AMONG DESIGNATED STATISTICAL AGENCIES. (a) IN GENERAL.—A Designated Statistical Agency may provide business data in an identifiable form to another Designated Statistical Agency under the terms of a written agreement among the agencies sharing the business data that specifies— (1) the business data to be shared; (2) the statistical purposes for which the business data are to be used; (3) the officers, employees, and agents authorized to examine the business data to be shared; and (4) appropriate security procedures to safeguard the confidentiality of the business data. (b) RESPONSIBILITIES OF AGENCIES UNDER OTHER LAWS.—The provision of business data by an agency to a Designated Statistical 44 USC 3501 note. 44 USC 3501 note. 44 USC 3501 note. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00070 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2969 Agency under this subtitle shall in no way alter the responsibility of the agency providing the data under other statutes (including section 552 of title 5, United States Code (popularly known as the Freedom of Information Act), and section 552b of title 5, United States Code (popularly known as the Privacy Act of 1974)) with respect to the provision or withholding of such information by the agency providing the data. (c) RESPONSIBILITIES OF OFFICERS, EMPLOYEES, AND AGENTS.— Examination of business data in identifiable form shall be limited to the officers, employees, and agents authorized to examine the individual reports in accordance with written agreements pursuant to this section. Officers, employees, and agents of a Designated Statistical Agency who receive data pursuant to this subtitle shall be subject to all provisions of law, including penalties, that relate— (1) to the unlawful provision of the business data that would apply to the officers, employees, and agents of the agency that originally obtained the information; and (2) to the unlawful disclosure of the business data that would apply to officers, employees, and agents of the agency that originally obtained the information. (d) NOTICE.—Whenever a written", "quality": {"lang": "en", "gibberish_score": 0.0035, "char_len": 2259, "word_len": 350}, "dedupe": {"exact_hash": "3c296efddfa90b1c165125971ef3baca1cea986e2c887ac16c22f0d98889664c", "simhash64": 15248149575811580934}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4ff4d0ff3d553c65c771d838231427b9b75f4d82fed2ab46db45f90bd18d1ad8", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 100, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "the business data that would apply to the officers, employees, and agents of the agency that originally obtained the information; and (2) to the unlawful disclosure of the business data that would apply to officers, employees, and agents of the agency that originally obtained the information. (d) NOTICE.—Whenever a written agreement concerns data that respondents were required by law to report and the respondents were not informed that the data could be shared among the Designated Statistical Agencies, for exclusively statistical purposes, the terms of such agreement shall be described in a public notice issued by the agency that intends to provide the data. Such notice shall allow a minimum of 60 days for public comment. SEC. 525. LIMITATIONS ON USE OF BUSINESS DATA PROVIDED BY DESIGNATED STATISTICAL AGENCIES. (a) USE, GENERALLY.—Business data provided by a Designated Statistical Agency pursuant to this subtitle shall be used exclusively for statistical purposes. (b) PUBLICATION.—Publication of business data acquired by a Designated Statistical Agency shall occur in a manner whereby the data furnished by any particular respondent are not in identifiable form. SEC. 526. CONFORMING AMENDMENTS. (a) DEPARTMENT OF COMMERCE.—Section 1 of the Act of January 27, 1938 (15 U.S.C. 176a) is amended by striking ‘‘The’’ and inserting ‘‘Except as provided in the Confidential Information Protection and Statistical Efficiency Act of 2002, the’’. (b) TITLE 13.—Chapter 10 of title 13, United States Code, is amended— (1) by adding after section 401 the following: ‘‘§ 402. Providing business data to Designated Statistical Agencies ‘‘The Bureau of the Census may provide business data to the Bureau of Economic Analysis and the Bureau of Labor Statistics (‘Designated Statistical Agencies’) if such information is required for an authorized statistical purpose and the provision is the subject of a written agreement with that Designated Statistical Agency, or their successors, as defined in the Confidential Information Protection and Statistical Efficiency Act of 2002.’’; and 44 USC 3501 note. 44 USC 3501 note. Public information. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00071 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2970 LEGISLATIVE HISTORY—H.R. 2458 (S. 803):", "quality": {"lang": "en", "gibberish_score": 0.0113, "char_len": 2307, "word_len": 350}, "dedupe": {"exact_hash": "c2a818d58542fed429dbfe415a288517145a57e699f5b1c6db4ac223dcc5b40c", "simhash64": 15820106728487616518}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6b312bca437975f03e847120978dfa32d9a2fd9de9aade42d16e2930963ca101", "doc_id": "sha256:563573924559134c64b1a4d168d34895c59b2ad5f8c207e10f020842c223d7a6", "chunk_index": 101, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\PLAW-107publ347.pdf", "page_start": null, "page_end": null, "text": "successors, as defined in the Confidential Information Protection and Statistical Efficiency Act of 2002.’’; and 44 USC 3501 note. 44 USC 3501 note. Public information. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00071 Fmt 6580 Sfmt 6581 E:\\PUBLAW\\PUBL347.107 116 STAT. 2970 LEGISLATIVE HISTORY—H.R. 2458 (S. 803): HOUSE REPORTS: No. 107–787, Pt. 1 (Comm. on Government Reform). SENATE REPORTS: No. 107–174 accompanying S. 803 (Comm. on Governmental Affairs). CONGRESSIONAL RECORD, Vol. 148 (2002): Nov. 14, considered and passed House. Nov. 15, considered and passed Senate. WEEKLY COMPILATION OF PRESIDENTIAL DOCUMENTS, Vol. 38 (2002): Dec. 17, Presidential statement. Æ (2) in the table of sections for the chapter by adding after the item relating to section 401 the following: ‘‘402. Providing business data to Designated Statistical Agencies.’’. Approved December 17, 2002. VerDate 11-MAY-2000 21:45 Jan 02, 2003 Jkt 019139 PO 00347 Frm 00072 Fmt 6580 Sfmt 6580 E:\\PUBLAW\\PUBL347.107", "quality": {"lang": "en", "gibberish_score": 0.0129, "char_len": 1008, "word_len": 150}, "dedupe": {"exact_hash": "2724a487639e4a4421f8507efe388c23a549a02210d9f49e6f1f5faf1ba39f68", "simhash64": 11496733549593723666}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:49ef5dec1822c3fe63e975ea38ac67f5602a7af67cf1627a45273dc3eab3a0f4", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 0, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System AHMAD SAMER WAZAN, Zayed University, UAE Generative AI challenges traditional assessments by allowing students to produce correct answers without demonstrating understanding or reasoning. Rather than prohibiting AI, this work argues that one way to integrate AI into education is by creating uncertain situations with the help of AI models and using thinking-oriented teaching approaches, where uncertainty is a central pedagogical concept for stimulating students’ critical thinking. Drawing on epistemology and critical thinking research studies, we propose designing learning activities and assessments around the inherent limitations of both AI models and instructors. This encourages students to reason, question, and justify their final answers. We show how explicitly controlling AI behavior during exams (such as preventing direct answers or generating plausible but flawed responses) prevents AI from becoming a shortcut to certainty. To support this pedagogy, we introduce MindMosaicAIExam, an exam system that integrates controllable AI tools and requires students to provide initial answers, critically evaluate AI outputs, and iteratively refine their reasoning. We also present an evaluation rubric designed to assess critical thinking based on students’ reasoning artifacts collected by the exam system. CCS Concepts: • Social and professional topics →Student assessment. Additional Key Words and Phrases: LLMs, Uncertainty, Epistemology, Education, Assessment Design, Critical Thinking, Academic Integrity 1 Introduction From an epistemological point of view, knowledge is a dynamic process that is always initiated by uncertain situations we are interested in. Figure 1 shows that our inquiry in uncertain situations starts often with some degrees of beliefs, which are unjustified knowledge. Then, we start a journey of critical examination of our beliefs through several cognitive operations based on reasoning, evidence, and reflection, in order to provide better justifications. This allows us to adapt our beliefs, which slowly become well-justified knowledge. Surprisingly, our knowledge does not move toward an absolute truth about the uncertain situation, but toward doubting all the assumptions of the solutions that can resolve our uncertain situations. This shows that doubt is the ultimate stage of any knowledge construction.", "quality": {"lang": "en", "gibberish_score": 0.002, "char_len": 2500, "word_len": 350}, "dedupe": {"exact_hash": "160a86d56f98d5b1f5b6af2139a61cccde5c804c6a6195bcf87e1a52d91a96f6", "simhash64": 11239388180213368726}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c2fd8d6b1f0b9319e7bacc94c448750321c29694bcd879d8c2c1963d59b8bdd4", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 1, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "to adapt our beliefs, which slowly become well-justified knowledge. Surprisingly, our knowledge does not move toward an absolute truth about the uncertain situation, but toward doubting all the assumptions of the solutions that can resolve our uncertain situations. This shows that doubt is the ultimate stage of any knowledge construction. The role of education with critical thinking is to help students achieve this level of knowledge construction by doubting all assumptions of existing solutions in order to innovate and find better solutions. If we assume that students are truly interested in their fields of study, then creating intellectually uncertain situations should encourage them to think, which should be the central role of educators. The Socratic teaching method [1, 6] is an approach that helps evolve students’ knowledge within uncertainty through dialogue and questionings, leading them to the ultimate level of knowledge construction, which is doubting all assumptions. Educators should help students build their knowledge from multiple sources, including perception, reasoning, testimony (e.g., instructors and textbooks), and experience [29]. The ultimate level of knowledge involves doubt, because epistemology considers that none of these sources is absolutely correct; all are subject to bias, error, and revision [26]. This view closely aligns with Karl Popper’s principle of falsifiability, Author’s Contact Information: Ahmad Samer Wazan, ahmad.wazan@zu.ac.ae, Zayed University, Abu Dhabi, UAE. 1 arXiv:2602.00026v1 [cs.CY] 17 Jan 2026 2 Ahmad Samer Wazan Fig. 1. Simplified epistemological process: how beliefs become knowledge through critical examination which argues that humans can never prove a claim to be absolutely true, but can only attempt to falsify it through critical testing [23]. From this perspective, the main risk of AI in education lies in its rapidly evolving capabilities to reduce uncertainty for students. Large language models (LLMs) can generate fluent answers with simple click buttons, allowing students to bypass or reduce the cognitive efforts needed for learning. However, AI is not the only source of this risk. Uncertainty for students has long been reduced by outdated pedagogical approaches. Traditional methods that adopt memorization and recall over reasoning are removing uncertainty from learning tasks, thereby diminishing students’ thinking. Therefore, the", "quality": {"lang": "en", "gibberish_score": 0.0021, "char_len": 2427, "word_len": 350}, "dedupe": {"exact_hash": "ae32b237c94e170829e10c6f2d924196d396613bd97cca9c9c9668f4cc6bc2fc", "simhash64": 11236287557422385047}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7cab0d437bfa111d2755e800e765c51aeb567bf091a11c5516ada73a282a84ec", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 2, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "or reduce the cognitive efforts needed for learning. However, AI is not the only source of this risk. Uncertainty for students has long been reduced by outdated pedagogical approaches. Traditional methods that adopt memorization and recall over reasoning are removing uncertainty from learning tasks, thereby diminishing students’ thinking. Therefore, the challenge education faces is not merely to detect or prohibit AI use, but to re-imagine class activities and assessments so that they deliberately preserve uncertainty for students. In this work, we argue that uncertainty is a key pedagogical concept for stimulating critical thinking. We design learning activities and exams around the limitations of both AI models and instructors. AI models exhibit well-known limitations such as probabilistic behavior, hallucinations, and lack of reasoning. Similarly, instructors are constrained by their own expertise, perspectives, and incomplete knowledge, especially in complex or evolving domains. We make these limitations explicit and use them as pedagogical tools to create uncertain situations to students. By placing students in situations where neither the AI nor the instructor provides definitive answers, we align with an epistemological view of knowledge shared by several pedagogical approaches based on critical thinking: every information given becomes a hypothesis to be examined rather than a truth to be memorized. This naturally triggers students’ thinking, encouraging them to compare viewpoints, verify information, detect contradictions, and construct justified arguments. To support this pedagogy, we introduce a set of instructional strategies and an exam system designed to preserve students’ uncertainty in the AI era. These strategies include asking questions grounded in class discussions and recent research rather than textbook material; favoring open-ended, non-deterministic questions over memorization-based or deterministic ones; and requiring students to provide an initial answer before any AI interaction. Additional strategies involve deliberately controlling AI behavior, encouraging students to critique both AI outputs and instructor viewpoints, and posing problems for which neither the AI nor the instructor has a fully determined solution. These strategies exploit AI limitations such as hallucinations, probabilistic outputs, lack of reasoning, and poor out-of-distribution generalization, as well Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical", "quality": {"lang": "en", "gibberish_score": 0.0016, "char_len": 2542, "word_len": 350}, "dedupe": {"exact_hash": "c89a767fa6f515dacbd4eaa0af25cabdabde3f014539ae3c2a42c704338b84df", "simhash64": 11234886781765649814}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ba3eaf3bb8ae7ce98036c85b6fb305bce157222098318451fef4ce2e80c1e789", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 3, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "viewpoints, and posing problems for which neither the AI nor the instructor has a fully determined solution. These strategies exploit AI limitations such as hallucinations, probabilistic outputs, lack of reasoning, and poor out-of-distribution generalization, as well Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 3 as instructor epistemic limitations to ensure that students are placed in uncertain situations that demand critical thinking. We have implemented an exam system, called MindMosaicAIExam , to support our strategies. MindMosaicAIExam is a web-based examination system that integrates AI models and search engines while allowing instructors to explicitly control AI behavior for each exam question. For example, instructors can prevent AI models from providing correct final answers, instruct them to generate plausible but incorrect explanations, or deliberately introduce fake theories. Students who are trained during class sessions to question and verify AI outputs should reason about this controlled AI behavior to reach grounded final answers. MindMosaicAIExam helps students to treat AI as an external source of information that they must interrogate and evaluate. Furthermore, the exam system encourages students to provide an initial answer before consulting AI models or other external resources, which allows instructors to determine the initial state of students’ thinking and how external sources shape the opinions of students toward the final answer. Throughout this process, the system captures all students’ interactions with AI models and other external sources, which give valuable resources for instructors to evaluate the thinking artifacts of students. Class discussions play a central role in our approach. They are built critically with students, where every idea is presented as a concept to be criticized along with its underlying assumptions. This process allows students to build a shared, negotiated reference of truth through debate. This helps students develop a critical thinking mentality during class sessions. At the same time, these discussions directly inform the design of exam questions and the configuration of AI behavior models. This work makes the following contributions: (1) A pedagogy grounded in epistemological uncertainty, leveraging the limitations of both AI models and instructors", "quality": {"lang": "en", "gibberish_score": 0.0012, "char_len": 2445, "word_len": 350}, "dedupe": {"exact_hash": "cbc1e3da5665cfaeb99925a17d0ef2677162b63620e30b036bbb7bdc01cfc6cc", "simhash64": 11221305680679913350}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1f715211d7b8f9521981c6794ca1b58c295124a4f6e2b79b04b38f41d8a5c7c6", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 4, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "develop a critical thinking mentality during class sessions. At the same time, these discussions directly inform the design of exam questions and the configuration of AI behavior models. This work makes the following contributions: (1) A pedagogy grounded in epistemological uncertainty, leveraging the limitations of both AI models and instructors to stimulate critical thinking. (2) A set of strategies for designing class activities and exams in which AI behavior is controlled to preserve uncertainty rather than eliminate it. (3) MindMosaicAIExam, an open-source examination system that operationalizes this pedagogy by enforcing structured student–AI interaction and capturing reasoning traces. (4) A dedicated evaluation rubric for assessing critical thinking as a process in AI-assisted courses. The remainder of this paper is structured as follows. Section 2 discusses the challenges posed by AI in education. Section 3 clarifies the relationship between education and critical thinking. Section 4 examines the concept of critical thinking and its evaluation. Section 5 presents our pedagogical framework. Section 6 introduces the evaluation rubric, and Section 7 presents the exam system. We conclude by summarizing our contributions and outlining future work. 2 Education Challenges Caused by AI In order to understand why AI challenges current educational practices, this section highlights first the limitations of traditional pedagogical approaches before the AI era. The pedagogical approaches of education before AI’s rise can be roughly divided into two categories: memorization-oriented learning and thinking oriented learning. Earlier pedagogical frameworks, such as behaviorism [28] and cognitivism [2], tend to focus on memorization and knowledge acquisition, while 4 Ahmad Samer Wazan constructivist [31] and experiential learning approaches [17] tend focus more on critical thinking and problem-solving skills [10]. The recent rise of AI has exposed fundamental limitations in certain pedagogical approaches, especially memorization-oriented learning. We argue that the difficulty of integrating AI into education is not only a technological problem, it is also and mainly a pedagogical issue: AI cannot be integrated into approaches that are already pedagogically weak. Our position is that AI is not threatening education as a whole, but it highlights the structural shortcomings of in some pedagogical approaches, particularly memorization-oriented learning. The", "quality": {"lang": "en", "gibberish_score": 0.0008, "char_len": 2488, "word_len": 350}, "dedupe": {"exact_hash": "e404a28a15925fb8a4b116fdd26aa4daceba6faf623c513d972a6c0981e13358", "simhash64": 10639857749813741462}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6aa89d9ddee6d4618d485ef29e32d56807ff91b734ff6dd6e0504b15df8d3cd8", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 5, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "only a technological problem, it is also and mainly a pedagogical issue: AI cannot be integrated into approaches that are already pedagogically weak. Our position is that AI is not threatening education as a whole, but it highlights the structural shortcomings of in some pedagogical approaches, particularly memorization-oriented learning. The memorization-oriented learning and thinking-oriented learning can be linked to underlying mechanisms of human cognitive functions. The memorization-oriented learning and thinking oriented learning approaches align closely with dual-system model of cognition described by Daniel Kahneman’s: System 1 thinking, which is fast and intuitive; and System 2 thinking, which is deliberate, analytical, and effortful [14]. The traditional memorization-oriented learning paradigm in education is very close to System 1 thinking because students are asked to retrieve information quickly during their exams. The “thinking-oriented learning” is more close to System 2 thinking because students have to develop critical thinking capacities to reason through uncertainty in order to construct a new knowledge. The main problem of the “memorization-oriented learning\" paradigm is that students are rarely asked to engage in the analytical processes that produced the knowledge they are taught. Instead, students tend to memorize the results of someone else’s analysis: for example, they recall Newton’s laws but they don’t pass necessarily through the conceptual process that produced these laws. One of the most influential critiques of \"memorization-oriented learning\" paradigm is formulated by Paulo Freire through his concept of the \"banking model\" of education [12]. In this model, Freire argues that education treats students as passive learners into which knowledge is \"deposited\" by teachers, which can reduce learning to the mechanical work of storage, recall and assessments. Similarly, the philosopher Jacque Rancière in his book \"The Ignorant Schoolmaster\" [25], based on the 19th-century pedagogical experiments of the French educator Joseph Jacotot, highlighted that all human beings possess equal intelligence, and that differences in learning outcomes arise not from unequal intelligence but from unequal will, attention, and efforts. Rancière rejected pedagogical models in which the teacher role limits to transmit knowledge to a passive learner. Like Freire’s critique of the banking model, Rancière thinks that education organized", "quality": {"lang": "en", "gibberish_score": 0.0032, "char_len": 2480, "word_len": 350}, "dedupe": {"exact_hash": "cb7cfdb65d8d715d26b65c95e4c2b6e3675a9a19ac3c6b34528bdaffda416bf9", "simhash64": 11221928210442086342}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d6eca78a504e1b066a47f12469eac47d3562fcc9d30dfdf62201a56ab361686d", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 6, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "possess equal intelligence, and that differences in learning outcomes arise not from unequal intelligence but from unequal will, attention, and efforts. Rancière rejected pedagogical models in which the teacher role limits to transmit knowledge to a passive learner. Like Freire’s critique of the banking model, Rancière thinks that education organized around transmission and recall enforces hierarchical relations between those who are supposed to know (i.e. teachers) and those considered incapable of knowing without help (i.e. students). Our critique to memorization oriented learning should not be understood as an underestimation to the role of human memory in learning; quite the opposite. We consider that human memory plays a very important role in the learning process. In their work, Craik and Lockhart [5] defined multiple levels of processing related to memory retention. They found that memory retention depends on how deeply information is processed: shallow processing leads to weak memory traces, while deep, semantic processing (e.g., thinking about meaning) produces stronger and more durable memories. Thus, human memory can durably retain information when it is understood, rather than memorized without understanding. Furthermore, memorization-oriented learning approaches primarily focus on the final outcomes rather than on the learning process itself, as students typically reproduce what was explicitly taught without engaging in deep understanding or reasoning. Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 5 Consequently, we argue that AI models and traditional memorization-oriented learning pedagogy share a fundamental drawback: both allow learners to bypass true understanding. The only difference between them is that students do not make efforts to recall course facts when using AI, whereas significant efforts to memorize class contents are needed if students don’t use AI during their exams. With memorization-oriented learning, grading students does not typically depend on their level of understanding but rather on their capacity to memorize class content. AI clearly poses serious challenges to all pedagogical approaches that adopt memorization-oriented learning strategies, because students are now able to retrieve any content without making any effort. As a result, all students have become “perfect achievers” with", "quality": {"lang": "en", "gibberish_score": 0.002, "char_len": 2461, "word_len": 350}, "dedupe": {"exact_hash": "024da96c79af8ac6f73693d09e5c81a50069019274141bfd0531244ca68b13d1", "simhash64": 11239944878850017174}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6dcb89db3b9d4cb3cd9fcfd24dec6c5045af9171156c5219b7da62b71d7278ba", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 7, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "their level of understanding but rather on their capacity to memorize class content. AI clearly poses serious challenges to all pedagogical approaches that adopt memorization-oriented learning strategies, because students are now able to retrieve any content without making any effort. As a result, all students have become “perfect achievers” with the help of AI. We argue that AI causes more of a grading crisis than an understanding crisis for memorization approaches because memorization does not allow students to truly understand course content but only to recall course content during exams. Consequently, many exams after the arrival of AI have been moved back to paper-based formats in order to preserve the possibility of differentiating students based on the effort they invest in memorization. In addition, the continuous improvement of AI capabilities has added a presseure on thinking-oriented learning pedagogies as well. Today, several exams and take-home assignments can now be answered with minimal efforts, thank to the \"reasoning\" and multimodals capabilities of AI models. This development contrasts sharply with the educational need to teach foundational knowledge and designing assessment questions that can evaluate their foundational knowledge. Thus, AI is transforming a wide range of questions into what we call low-cognitive-effort questions which can be answered correctly with a single click without engaging in meaningful reasoning. Even questions that are currently considered conceptually meaningful may, as AI continues to evolve, require progressively less cognitive effort to answer. Over time, AI will enable students to bypass true understanding. The increasing capabilities of AI therefore pose also a significant challenge to assessment practices designed to support thinking-oriented learning. At the same time, it is unrealistic to expect students to refrain from using AI tools. The current generation is growing up with AI in the same way previous generations grew up with calculators or computers. For this reason, educational systems must move beyond attempts to ban or ignore AI and instead develop pedagogical strategies that explicitly account for its presence. In this work, we address this challenge by proposing methods to control AI behavior in a way that preserves cognitive efforts of students. It is", "quality": {"lang": "en", "gibberish_score": 0.0008, "char_len": 2355, "word_len": 350}, "dedupe": {"exact_hash": "c892b81e62bb24f8d6523a9cd87f957af3923f9a2bb40198e72025b966910fc3", "simhash64": 11238826331919058822}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:78c782f0a09f837a93a4d48340b7b283eca62ffeacca375cb9b8afec18403cee", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 8, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "For this reason, educational systems must move beyond attempts to ban or ignore AI and instead develop pedagogical strategies that explicitly account for its presence. In this work, we address this challenge by proposing methods to control AI behavior in a way that preserves cognitive efforts of students. It is worthy to note that in higher education, AI poses fewer challenges to thinking-oriented learning for several reasons: (1) AI does not truly think. Several research studies [18, 22] suggest that the current paradigm of generative AI does not enable the achievement of artificial general intelligence (AGI), as these systems lack reasoning and understanding. (2) In higher education, most educators are also researchers who actively challenge existing knowledge in their fields. By engaging deeply with their domains, educators are able to identify limitations, formulate original problems, and propose new ideas that are not simply reflected in AI models, which mainly reproduce dominant patterns and widely shared viewpoints about problems of the world. (3) Even if AI were capable of advanced reasoning, educators could still rely on pedagogical approaches such as counterfactual reasoning, which encourage students to explore alternative explanations, question assumptions, and evaluate different outcomes, thereby strengthening critical thinking skills. 6 Ahmad Samer Wazan Fig. 2. AI and Memorization have the same problem: bypassing human understanding (Figure adapted with minor modifications from [16]) These methods can be combined with our idea to control the behavior of AI systems in order to stimulate critical thinking skills of students. Our approach is therefore more inclusive, as it enables educators at all levels, and regardless of whether they are researchers, to integrate AI meaningfully into their courses. 3 Education and Critical Thinking While AI is often described as revolutionizing or transforming education, we argue instead that AI is not fundamentally changing education but rather returning it to its original purpose: the development of students’ thinking. The following section examines this claim by exploring the role of critical thinking as a core objective of education. There is no single, universally accepted definition of education. However, several scholars and research studies emphasize that one of its", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2361, "word_len": 350}, "dedupe": {"exact_hash": "9174e65574a5de80ee6184c042467948c8b230a5bb0d6da8be47f38cc4859e36", "simhash64": 11235448630050801558}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:fb4798e10cbc8cd00542598502bbbd7ca0e55baf058f9e64d1428c3e2af2890b", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 9, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "returning it to its original purpose: the development of students’ thinking. The following section examines this claim by exploring the role of critical thinking as a core objective of education. There is no single, universally accepted definition of education. However, several scholars and research studies emphasize that one of its primary objectives is the cultivation of students’ critical thinking capacities. This perspective can be traced from the Socratic tradition of questioning, through the reflections of modern scientists and educators such as Albert Einstein and Richard Feynman, to the Delors Report [3] presented to UNESCO in 1996. Socratic Teaching method is one of the oldest teaching methods that put thinking at the core of the learning process of students. Socrates famously declared, “I cannot teach anybody anything. I can only make them think.”1 which clearly illustrates the main goal of education as per the Socratic approach. The Socratic method is characterized by systematic questioning to guide students to the final answer rather than dictating them course facts. [20] reported that D. Knežić in his work ’Socratic Dialogue and teacher-pupil interaction, 2011’ defined Socratic dialogue as: “A philosophical group dialogue in which the participants guided by a facilitator and a number of ground rules strive to reach a consensus in answering a fundamental question on the basis of one real-life example with the purpose of achieving new insights”. Doing so, students can enhance their capacities to analyze, clarify, and justify their ideas. The Socratic method allows thus to construct knowledge collaboratively rather than transmitting passively course facts by teachers [1]. 1https://www.goodreads.com/quotes/73059-i-cannot-teach-anybody-anything-i-can-only-make-them Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 7 Consistently, several studies made in several contexts and at different levels of education (childhood, secondary and higher education) have shown a very positive impact of the Socratic method on students’ critical thinking. For example, in childhood education setting, teachers implemented a Socratic-method over 10 weeks with fifteen 5-6 years old children. Teachers used questions like “Who is called ’a good person’?” and “can something that makes one person happy make", "quality": {"lang": "en", "gibberish_score": 0.0083, "char_len": 2424, "word_len": 350}, "dedupe": {"exact_hash": "f1e98d1a252c36f1b79c78985cee71f41e592a08b4da43bde4c7a7419ecb15d2", "simhash64": 11235997629953201558}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8075602f3f6a65c7577f5d09991c1c2c46300368ccb91361c54badd94ee81b55", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 10, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "a very positive impact of the Socratic method on students’ critical thinking. For example, in childhood education setting, teachers implemented a Socratic-method over 10 weeks with fifteen 5-6 years old children. Teachers used questions like “Who is called ’a good person’?” and “can something that makes one person happy make everyone happy?” to trigger children’s reasoning [15]. The impact was measured through video recordings, interviews, and content analysis, tracking the frequency of critical thinking behaviors (e.g., comparing, relating, evaluating, generalizing, etc.). The results showed a clear increase in critical thinking behaviors and a decrease in non-critical responses (such as repeating what is said, indifference, and irrelevant responding) over time. In medicine, students were shared Socratic learning sheets during their biochemistry laboratory course [13]. According to the authors, critical thinking skills are needed for medicine students to manage the medical uncertainty. around diagnostic and treatment of diseases. The evaluators considered nine dimensions of cognitive thinking (clarity, accuracy, precision, relevance, depth, breadth, logic, fairness, and significance). The results indicated that students showed significant potential to develop critical thinking skills across all tested dimensions of critical thinking. In Online English Teaching classes, Indonesian faculty have redesigned their curriculum to integrate critical thinking skills into the English curriculum program [19]. They then measured the impact on the critical reading course after integrating the Socratic method in their classes and testing it on 52 students over six sessions. The authors’ statistical findings showed a significant positive effect. Similarly, several research studies have measured the positive impact of the Socratic method in mathematics and science classes across different levels of education [7, 21, 30] Critical thinking is thus a fundamental component of education that students need in real life. Curiously, measuring critical thinking focuses more on the approach that students apply to reach their conclusions (or final answers) than on the conclusions themselves. Several prominent scholars share the same opinion, such as Albert Einstein, who said: \"Education is not about learning facts; it is the training of the mind to think.\". The Delors Report gave a broader perspective to education by defining four pillars: learning to", "quality": {"lang": "en", "gibberish_score": 0.0045, "char_len": 2469, "word_len": 350}, "dedupe": {"exact_hash": "4c4e2cbe1e2841ef6b6bd35b7b8386d5fd7c7e74eb432c36f7da598bd3c183ab", "simhash64": 10587484506300077334}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:fffff690aea965dc98ed105a9085331c9b72d1798c1e1a56c3215b575e79a9b1", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 11, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "final answers) than on the conclusions themselves. Several prominent scholars share the same opinion, such as Albert Einstein, who said: \"Education is not about learning facts; it is the training of the mind to think.\". The Delors Report gave a broader perspective to education by defining four pillars: learning to know, learning to do, learning to live together, and learning to be [3, 8]. However, in all these pillars, critical thinking plays an important role, especially in the learning to be component, which encourages students to develop their own opinions. 4 Can Critical Thinking Be Universally Defined and Evaluated? As illustrated in section 3, critical thinking is a core component of education. However, most studies discussed in the previous section have adopted their own definitions and measurement approaches to critical thinking. In the late 1980s, a critical thinking movement started to gain momentum to liberate education and move away from the accumulation of disjointed knowledge into a deeper process of learning based on critical thinking [11]. In 1987, more than 40 experts, mainly working in philosophy, were invited to participate in the Delphi Research Project. After six rounds of dialogue over three years of discussions, the experts delivered what is known as the \"Delphi Report\". In this report, the experts gave the following consensual definition of critical thinking: \"critical thinking is purposeful, self-regulatory judgment which results in interpretation, analysis, evaluation, and inference, as well as explanation of the evidential, conceptual, methodological, 8 Ahmad Samer Wazan criteriological, or contextual considerations upon which that judgment is based\". The experts have also added affective dimensions to the critical thinking concept by considering \"the affective dispositions\", which are, according to the experts, \"the habits of mind and personal traits\" that ensure cognitive skills are used properly in everyday life. The experts have considered the importance of dispositions to build good critical thinkers: \"The ideal critical thinker is habitually inquisitive, well-informed, trustful of reason, open-minded, flexible, fair-minded in evaluation, honest in facing personal biases, prudent in making judgments, willing to reconsider, clear about issues, orderly in complex matters, diligent in seeking relevant information, reasonable in", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2398, "word_len": 350}, "dedupe": {"exact_hash": "b5d067c880d543e3be7d28a7a79aaad8bf967986201c18a6438f5d6904d78c98", "simhash64": 11218015873194755522}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:28d7b2a9a0d5734dad3f6a7e71feee094679ea024745cace99b2481b0a2e5f22", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 12, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "importance of dispositions to build good critical thinkers: \"The ideal critical thinker is habitually inquisitive, well-informed, trustful of reason, open-minded, flexible, fair-minded in evaluation, honest in facing personal biases, prudent in making judgments, willing to reconsider, clear about issues, orderly in complex matters, diligent in seeking relevant information, reasonable in the selection of criteria, focused in inquiry, and persistent in seeking results which are as precise as the subject and the circumstances of inquiry permit\". However, the conception of critical thinking that experts suggested considers critical thinking as a standard universal and individual concept centered around humans. To these experts, culture and context are external variables that provide the background for a judgment but do not change the fundamental nature of the logical tools being used. In sharp contrast, Luis Santos [27] argues that critical thinking should be more culturally centered rather than human-centered. He has conducted a theoretical literature review on the concept of critical thinking , based on academic sources, historical analysis, and examination of curriculum documents. His main findings showed that conceptions of critical thinking can vary across cultural, sociopolitical, and educational contexts. The author argues that dominant Western influenced models of critical thinking are often adopted uncritically worldwide, leading to curricula that overlook cultural and context specific needs. The author illustrates the culturally centered critical thinking by showing how Asian Confucian contexts often value \"self-reflexivity\" and harmony over judgment, while Latin American traditions view critical thinking as a vehicle for social emancipation. While the objective of the Delphi Report was to come up with a unified consensus to guide global assessment, Luis Santos proposed a balanced, culturally sensitive model where \"Context-culture\" serves as the essential base that determines how cognitive, ethical, and civic dimensions should be considered for critical thinking concept. However, we argue that the culture-centric critical thinking adopted by Santos enlarges the concept of critical thinking to cover almost every aspect of education, as defined in the Delors Report through four pillars: learning to know, learning to do, learning to live together, and learning to be. In this sense, education and critical thinking thus", "quality": {"lang": "en", "gibberish_score": 0.0, "char_len": 2464, "word_len": 350}, "dedupe": {"exact_hash": "8a8c7bc70736d3eb2438df22eb27b7655e50cda674258339d3e1fcd03781c4da", "simhash64": 11239935737004934550}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:401c0491ac728ccab5f4c3d172f82d4728c810e0d7fbe0cb95d86d3d369ed114", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 13, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "culture-centric critical thinking adopted by Santos enlarges the concept of critical thinking to cover almost every aspect of education, as defined in the Delors Report through four pillars: learning to know, learning to do, learning to live together, and learning to be. In this sense, education and critical thinking thus become effectively equivalent concepts. Ennis [9] argues that broad and informal conceptions of critical thinking make it difficult to integrate critical thinking into education because it is not clear what are the criteria of good critical thinking, the dispositions and abilities that should be taught, and the elements that need to be assessed in critical thinking evaluations. Nonetheless, standardized approaches for measuring critical thinking skills and dispositions, such as the California Critical Thinking Skills Test (CCTST), California Critical Thinking Disposition Inventory (CCTDI), Watson–Glaser Critical Thinking Appraisal (WGCTA), and Cornell Critical Thinking Test, have been introduced and widely used. These approaches are largely influenced by the Delphi report, which focuses mainly on cognitive skills such as interpretation, analysis, evaluation, inference, explanation, and self-regulation, alongside dispositions as discussed above. Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 9 Fig. 3. French Critical Thinking Evaluation Framework Despite their popularity, these approaches face significant limitations. For example, Wilson [32] looked at whether three common critical thinking tests (CCTST, CCTDI and WGCTA) work well for physical therapy graduate students. Students took the tests when they started the program, again a few weeks later, and again after two semesters of study. Although the tests gave fairly consistent results, students’ scores did not improve enough to show clear, meaningful gains in critical thinking. The author concluded that \"Commercially available critical thinking tests designed for a general population do not appear to be useful for assessing critical thinking gains made by health care professions students\". The evaluation framework developed by the Conseil scientifique de l’éducation nationale (CSEN) approaches critical thinking not as an consensual skills, but as a practice in school disciplines, learning situations, and social uses of knowledge [4]. The framework stresses on the fact", "quality": {"lang": "en", "gibberish_score": 0.0016, "char_len": 2476, "word_len": 350}, "dedupe": {"exact_hash": "8b1b24a4c30ecffdd868b62b160ff6678eff563d6a2ff7dee164a6d2e75d80ff", "simhash64": 10643245349435590034}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:26a3bf561055eabd66eb8c5122ced7aa80c2b71e3b1dac74963fc1ee75ae44b6", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 14, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "critical thinking gains made by health care professions students\". The evaluation framework developed by the Conseil scientifique de l’éducation nationale (CSEN) approaches critical thinking not as an consensual skills, but as a practice in school disciplines, learning situations, and social uses of knowledge [4]. The framework stresses on the fact that critical thinking must be observed “in classroom activities” rather than through standardized tests. The French framework proposes five interrelated evaluative components, each rooted in disciplinary and cultural knowledge (cf. Figure 3), which are the quality of Evidence, the reliability of sources, the quality of Arguments, the plausibility of information and the reliability of Judgments. All five components recall factors that are related to the curriculum, context and social culture of students and they contribute to an overall confidence judgment toward formation of an opinion or an exam answer. Similar to the French framework, we advocate for discipline- and culture-based assessments that align more closely with course learning objectives. However, we need to modify the evaluation framework to 10 Ahmad Samer Wazan evaluate the capacities of students to use AI critically and compare it with the ground truth acquired in class discussions. 5 Our Pedagogical Approach to Enforce Critical Thinking Our pedagogical approach to stimulating students’ critical thinking can be summarized through the following steps: (1) Define a pedagogy for the use of AI. (2) Explain the course concepts critically by explicitly discussing assumptions, limitations, and alternative viewpoints. (3) Adopt different strategies to design class activities and exams; this includes: (a) Ask students to identify limitations and weaknesses in our lectures and explanations. (b) Give students open-ended problems for which the instructor does not have a fully determined solution. (c) Design exam questions based on class discussions, along with a limited number of unfamiliar questions that were not explicitly covered in the course. (d) Control the behavior of AI models in exams to maintain their uncertainty and stimulate students’ critical thinking. Our pedagogy objective is to make students explicitly aware of the inherent limitations of AI systems and their instructor. Our idea is grounded on the principle of falsifiability [23,", "quality": {"lang": "en", "gibberish_score": 0.0021, "char_len": 2385, "word_len": 350}, "dedupe": {"exact_hash": "6039133224f44b154ece0a1acbb924cc3bd3528445b144f61ea5b48c4445b439", "simhash64": 11222491160397580166}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:24d2c50def258090029be2020908ce099f9f9736bac7c563178e1fbd2624e797", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 15, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "in the course. (d) Control the behavior of AI models in exams to maintain their uncertainty and stimulate students’ critical thinking. Our pedagogy objective is to make students explicitly aware of the inherent limitations of AI systems and their instructor. Our idea is grounded on the principle of falsifiability [23, 24], which considers that knowledge cannot be proven to be absolutely true but can only be tested and potentially proved to be wrong. From this perspective, neither AI-generated outputs nor human explanations should be considered as absolutely correct. Instead, they should be regarded as grounded opinions that are always open to be challenged, refined, or rejected by other stronger grounded opinions. By adopting this idea, students will have the certainty that science is not certain or more precisely conditionally certain (i.e., always based on assumptions). As instructors, we are generally aware of the boundaries of our own knowledge, due to years of practice, research, and interaction with students. In contrast, identifying the limitations of AI systems is significantly more challenging, as it might require technical insight into how these models are trained and how they generate responses that is not always accessible to everyone. For this reason, we presented AI models to our students from high level perspective as tools that may produce doubtful answers, compared to deterministic tools such as calculators (see Figure 4). The onus is on students, to understand the answers provided by AI tools, verify them, and then probe further with follow-up questions until they get specific and satisfactory answers, which they can communicate. Furthermore, in our courses, we explicitly designed different pedagogical strategies to apply the Socratic teaching approach. Table 1 shows the mapping between the AI models and instructor limitations and the strategies that we adopted to design our class activities and exam questions. In our educational framework, we consider the textbooks as source of information that is biased to the opinion of their authors. Class discussions allows educators to present different viewpoints and help students to become critical thinkers by building their own opinions. Strategies for Creating Uncertainty in the AI Era to", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2289, "word_len": 350}, "dedupe": {"exact_hash": "810b8b8adc046c083160b53b7a4877c5f54b46008ecb0ba2cb69a81e629096d6", "simhash64": 11234884582699800278}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ebfdca08370d97a2a68661241c57de4c46958d2d84fe2b3dbfe70704fa511fe5", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 16, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "In our educational framework, we consider the textbooks as source of information that is biased to the opinion of their authors. Class discussions allows educators to present different viewpoints and help students to become critical thinkers by building their own opinions. Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 11 Fig. 4. Understand, Check and Ask Follow-up Questions Loop Similarly, AI models being probabilistic offer great teaching experience to students as they have to check and understand every AI models’ outcome. AI models don’t think or reason, so educators can build a set of open or out of distribution questions that make it difficult for students to obtain answers by simple prompts. Moreover, we refrain from using memorization in our course activities and exams because as mentioned previously it is not convenient for students in the higher education whose role is to enhance creativity of students to prepare them better to the unknown of life. In addition, our pedagogical design explicitly acknowledges the limitations of the instructor by intentionally adding lab exercises and exam questions for which the instructor is not fully aware of the solution. In our labs, we deliberately include tasks that reflect real-world uncertainty and evolving technologies. For example, in one lab which focused on intrusion detection using Snort, students were asked to analyze network traffic and detect attacks involving IP fragmentation activity. Following updates to the Snort engine, several of the provided rules and detection mechanisms no longer behaved as expected. After multiple attempts by the instructors to resolve the issue, the problem remained unsolved. Rather than removing or simplifying the exercise, we asked students to investigate the issue themselves, analyze the behavior of the tool, and identify potential solutions. One student group ultimately succeeded in proposing a working solution. This experience not only demonstrated the evolving and imperfect nature of security tools but also reinforced the idea that humans, like AI systems, do not always have complete correct answers. Furthermore, this philosophy extends to the project-based component of the course. Students are required", "quality": {"lang": "en", "gibberish_score": 0.0013, "char_len": 2329, "word_len": 350}, "dedupe": {"exact_hash": "2e10fd96a5574354c94a1300f2e4dce548b38644e9ea6a2a9acfa61518e050fd", "simhash64": 11234875786606773510}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:114f96527572e2dc47a11201d0d5b1df1ee70db00f3b384c64ffa108116eb2f5", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 17, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "succeeded in proposing a working solution. This experience not only demonstrated the evolving and imperfect nature of security tools but also reinforced the idea that humans, like AI systems, do not always have complete correct answers. Furthermore, this philosophy extends to the project-based component of the course. Students are required to deliver a final presentation in which they critically examine topics initially introduced by the instructor, including DNS security, Wi-Fi security, and WebPKI. Students are explicitly asked to identify weaknesses, omissions or questionable assumptions in the instructor’s explanations, as well as to uncover recent developments not covered in class lectures. The evaluation criteria are intentionally designed such that stronger, grounded critiques will allow students to get higher grades. 12 Ahmad Samer Wazan Limitation/Strategy Socratic Approach Ask questions related to class discussion (expert knowledge) Don’t ask textbook questions Ask open questions Ask deterministic questions Don’t ask memorisation questions Recent questions from research Control AI Criticize instructor ideas Give problems that instructor doesn’t know how to solve Provide initial answer Hallucinations x x x x x Probabilistic output x x x x Lack of reasoning x x x x x x x Inability to handle out-of-distribution data x x x x x x Bias x x x x x instructor Limitations x x x Table 1. Pedagogical Strategies Mapped to LLM and Instructor Limitations These pedagogical practices directly inspire the design of our exam questions. Rather than focusing on recalling class facts, our exams are intentionally constructed to reflect the uncertainty of the real world by asking open questions and by refraining from asking textbook questions or memorization questions. Furthermore, designing exam questions based on our research allows us to exploit the out-of-distribution nature and the lack of reasoning of AI models. However, as AI models continue to evolve, their performance improves and their margin of error are decreasing over time. While this progress is beneficial in many contexts, it poses a pedagogical challenge: it reduces the uncertainty to a point where critical thinking and reasoning are no longer required from students. To preserve uncertainty as a learning mechanism, we have built", "quality": {"lang": "en", "gibberish_score": 0.0022, "char_len": 2321, "word_len": 350}, "dedupe": {"exact_hash": "46a476e8ee4d535db930ada16087eb5b2b866994776f0aa932b7157cd16a6db6", "simhash64": 10646238284518115204}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:77e6eb6b3dfee9a346c855fd844d1eeb2dcf1d0651bdb6c5127bcf6e75335b2e", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 18, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "and their margin of error are decreasing over time. While this progress is beneficial in many contexts, it poses a pedagogical challenge: it reduces the uncertainty to a point where critical thinking and reasoning are no longer required from students. To preserve uncertainty as a learning mechanism, we have built an exam system that allows us to control the behavior of AI models used in our assessments. Specifically, we constrain AI models’ behavior to reflect known limitations, such as hallucinations and out-of-distribution, or generating imperfect reasoning paths. This intentional control allows us to maintain the pedagogical value of uncertainty and prevents students from relying blindly on AI. In addition, to better capture the students’ cognitive progression and reasoning development, we incorporated a mechanism to explicitly observe how they move from an initial state of uncertainty to a more informed understanding. For each exam question, students were asked to first provide their initial answer or expectation before consulting any external resources, including AI tools, or lecture notes. This initial response represents the student’s prior knowledge, intuition, or assumptions about the exam question. Importantly, students were explicitly informed that stating “I do not know” would not result in any grade penalty. This technique maps to several strategies because it establishes a baseline for student’s reasoning. It makes uncertainty explicit when students state \"i don’t know\". It can also show how students can change their prior assumptions when reading AI output or other external resources. On the other side, this technique can allow to detect hallucinations, probabilistic behavior, and lack of reasoning of AI models. However, the initial answers are treated as valuable indicators for the reasoning of the student as it gives us information about their initial thoughts and how their states have changed after reading AI responses or other external sources. We were thus able to analyze how students refined their understanding, corrected misconceptions, and integrated new knowledge by observing their initial answers, prompts and their final answers. This observation process provides insight into students’ learning dynamics which reflects the dynamicity aspect of critical thinking. We present in section our critical", "quality": {"lang": "en", "gibberish_score": 0.0034, "char_len": 2361, "word_len": 350}, "dedupe": {"exact_hash": "ea23bda8d8c430aeb90a1fd40f3c24ff889af83f4d01c90734e392f5792042ee", "simhash64": 11235990966317319446}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:5941c0c53f969a842aa28b13abd152fe6468c53b8528e14f9f355d9fa4ce4636", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 19, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "We were thus able to analyze how students refined their understanding, corrected misconceptions, and integrated new knowledge by observing their initial answers, prompts and their final answers. This observation process provides insight into students’ learning dynamics which reflects the dynamicity aspect of critical thinking. We present in section our critical thinking evaluation rubric. Tables 2, 3 and 4 show examples of exams questions that are built based on the limitations of LLMs with their associated strategies such as and recent questions from research. Tables 5 shows an example of questions where students are requested to criticize the answer of the instructor. Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 13 Exam question The following question is encrypted by Caesar cipher. The used key corresponds to the number of students who attended the last session on campus. You need to test all possible keys if you don’t know the number of students who attended the last class session on campus. You will obtain half of the grade if you decrypt the question. The other half of the grade will be given if you answer the question correctly. The encrypted question Hasodlq zkb fdofxodwlqj WFS vwdwh eb d vwdwhixo iluhzdoo lv pruh uholdeoh wkdq XGS vwdwh evaluation The question exploits the fact that LLMs are probabilistic and can not decrypt the question in a deterministic way. The question is valid from educational point of view because the student has to make an effort to decrypt the question using his cryptography knowledge. Table 2. Example that exploits the probabilistic nature of LLMs Exam question How can we establish a secure HTTPs connection without using certificates that are signed by certification authorities evaluation The question exploits the inability to handle out of distribution limitation because it is a research problem that is discussed in the class, students have to ask more follow up questions based on the class discussion which what make this exercise valid from education point of view. Table 3. Exercise from educator’s research Exam question Given the following scenario,", "quality": {"lang": "en", "gibberish_score": 0.0018, "char_len": 2223, "word_len": 350}, "dedupe": {"exact_hash": "14e2157a6ae3b5e0c54458d26449097169fc224cf2ec95da02d358f10c340972", "simhash64": 11239380827230803206}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:11a11d36997150cea2677e3681661c2eb0a5cc69d6cbbeabfc45435c18eeac84", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 20, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "distribution limitation because it is a research problem that is discussed in the class, students have to ask more follow up questions based on the class discussion which what make this exercise valid from education point of view. Table 3. Exercise from educator’s research Exam question Given the following scenario, which of the CIA security model aspects were compromised? Alice sent an encrypted message to Bob! Eve intercepted and prevented Bob from receiving the message. 1)availability 2)integrity 3)confidentiality 4)none of the CIA components was compromised evaluation The question exploits the fact that LLMs don’t reason. Most of the LLMs answers were selecting the choice confidentiality which is wrong because they probabilistically associate the word encryption with confidentiality. Table 4. Exercise from educator’s research that requires thinking Exam question Modern browsers tend to display a “lock icon” in the URL bar to indicate that a website is secure. Recently, modern browsers have decided to completely remove the lock icon from Web browsers interface. 1-Give your answer in the initial answer that illustrates whether you agree or disagree with this decision. It is recommended to give your answer without reading my answer. 2-Read my answer. Please find arguments that support my answer or try to identify the limitations of my answer. Instructor Answer: I don’t agree on this decision because it makes it difficult to web users to know the identity of websites. Knowing the identity of websites is an important step to trust them. However, as most of certificates are DV certificates it means that we can not check the identity of web browsers. But then should we give DV certificates to everyone? I prefer giving certificates only to websites whom we can verify the identity. evaluation The question exploits the fact that students might build different opinions (possibly similar to web browser vendors) based on the reading and analysis of class discussions and other external sources they may find during the exam. Table 5. This exercise requires criticizing instructor answer 6 Evaluation Rubric for Our Students’ Critical Thinking To evaluate students’ critical thinking in AI-allowed assessments, we initially adopted an", "quality": {"lang": "en", "gibberish_score": 0.0049, "char_len": 2258, "word_len": 350}, "dedupe": {"exact_hash": "83d50f9cef19f7e82fd971e1aa7aadb240d171159242e01bd2f2f4af14316031", "simhash64": 11208698611657725830}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:260a0dbc4e6c9ed9dd3205bf61d41a086a86e12106a5a0ecf2a45b58d9899b3f", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 21, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "web browser vendors) based on the reading and analysis of class discussions and other external sources they may find during the exam. Table 5. This exercise requires criticizing instructor answer 6 Evaluation Rubric for Our Students’ Critical Thinking To evaluate students’ critical thinking in AI-allowed assessments, we initially adopted an ad hoc evaluation methods grounded in close inspection of students’ thinking artifacts. In particular, we analyzed students’ initial answers, prompt sequences, and final responses, as illustrated in Table 7. This qualitative analysis 14 Ahmad Samer Wazan enabled us to observe how students approached exam questions, how they formulated and refined questions posed to AI systems or external sources, and how their understanding evolved over time. Through this process, we assessed not only correctness but also the quality of their prompts, their verification strategies, and their progression towards final answers. Over time, we progressively transitioned toward a more structured evaluation framework, presented in Fig. 5. Rather than being centrally designed, the rubric is built iteratively from classroom discussion with students, repeated examination sessions, and reflective analysis of student–AI interactions during class activities. Each dimension of the rubric corresponds to recurring patterns observed during the ad hoc evaluation phase and to the pedagogical strategies employed throughout the course. The first dimension of the rubric, Understanding, evaluates students’ ability to use AI and external sources to understand the concepts underlying the exam question for conceptual clarification rather than as a shortcut to answer exam questions. This criterion captures whether students reformulate the problem in their own words, avoid merely restating the question to AI, and demonstrate comprehension of key notions before attempting to reach a final answer. The Reasoning dimension focuses on students’ analytical engagement with AI-generated content and external sources. It assesses their ability to detect errors, hallucinations, or contradictions in AI responses, to question claims, and to justify their final answers through logical argumentation. This criterion reflects our emphasis on treating AI outputs and external sources as hypotheses to be tested rather than authoritative definitive answers. The Independence dimension measures the extent to which students control the prompting process. Rather than", "quality": {"lang": "en", "gibberish_score": 0.0028, "char_len": 2479, "word_len": 350}, "dedupe": {"exact_hash": "e75b4858e1d0c1597e2aa94a9f270ad307ae2411f9817f7386d495ebeb874ad3", "simhash64": 11239309296670312198}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:3e2843fdc56fae80f670a3e969a2b8af835373247425d30bd9416207aeac150d", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 22, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "responses, to question claims, and to justify their final answers through logical argumentation. This criterion reflects our emphasis on treating AI outputs and external sources as hypotheses to be tested rather than authoritative definitive answers. The Independence dimension measures the extent to which students control the prompting process. Rather than passively following AI suggestions, students are expected to guide the interaction based on their knowledge acquired during class discussions, and consult additional sources when necessary. This dimension directly aligns with our instructional goal of building intellectual autonomy and reducing over-reliance on AI systems or other external sources. The Improvement over time dimension captures students’ capacity to revise and refine their understanding throughout the exam. By comparing initial answers, intermediate prompts, and final answers, this dimension evaluates whether students provide final answers that are logical consequence to their initial answers and their prompts. Importantly, it values cognitive progression rather than penalizing initial uncertainty. Finally, Recall facts from class discussions assesses how effectively students integrate concepts, arguments, or critiques discussed during the course. This includes recalling relevant ideas, challenging viewpoints presented by the instructor, and situating new knowledge within the context of the exam questions. These critical thinking-oriented criteria inform an overall judgment of students’ confidence in their final answers, as depicted in Fig. 5. The rubric draws inspiration from the French critical-thinking evaluation framework originally developed to assess conceptual evolution and explanatory coherence in scientific reasoning. While both rubrics emphasize progression and justification, our rubric has been substantially adapted to reflect the realities of AI-assisted assessments. In particular, it introduces criteria specific to generative AI contexts that are discussed and built with students. Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 15 Fig. 5. Process- and result-oriented rubric for measuring critical thinking in AI-allowed exams. 16 Ahmad Samer Wazan Exam question: RSA is not recommended for encryption because it doesn’t allow to respect Forward Secrecy property. Let’s suppose that two users A and B use the following protocol to encrypt messages (omitted due to limited space). Do you think", "quality": {"lang": "en", "gibberish_score": 0.002, "char_len": 2555, "word_len": 350}, "dedupe": {"exact_hash": "6df6758c8c20f7abe9600f38b7f17c9a60cd9f4adec6b2078cf8c46c52cbbc8b", "simhash64": 10083023286482339734}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:87f3ce23db18d5ab992f7216c13a6e706bfbd1f2cf89ead102132a1911fd7a71", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 23, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "measuring critical thinking in AI-allowed exams. 16 Ahmad Samer Wazan Exam question: RSA is not recommended for encryption because it doesn’t allow to respect Forward Secrecy property. Let’s suppose that two users A and B use the following protocol to encrypt messages (omitted due to limited space). Do you think that this protocol guarantees Forward Secrecy? Table 6. Exam question (verbatim) Student1 Student2 Remarks Initial Answer I don’t know I don’t know Prompt1 What is Forward Secrey? What does the algorithm below do? [she gave the protocol with the prompt] Student1 asked a question to analyse the exam question while Student2 asked ChatGPT to explain the protocol. ChatGPT answer1 ...is a property of cryptographic protocols that ensures that the compromise of a secret key in the future will not compromise the confidentiality or integrity of past communications that were encrypted using that key... ...algorithm describes a basic implementation of a hybrid encryption scheme that uses RSA for key exchange and AES for message encryption... ChatGPT gave good answers to both students. Prompt2 Why RSA is good for authentication but does not provide Forward secrecy? does this respect forward secrecy? The prompt of Student1 shows that she is still analysing the exam question, while Student2 tries to obtain directly the final answer. ChatGPT answer2 ...RSA does not provide forward secrecy because it does not use ephemeral keys, which are necessary for providing this property... ...No, this algorithm does not provide forward secrecy.... ChatGPT gave good answers to both students. Prompt3 Why Diffie-Hellman is good for encryption and provides Forward secrecy? ...but even the message is always encrypted using a random value.... Student1 seems still trying to understand the question. Student2 asked a specific key question about the protocol because the random value can give the impression that the protocol provides Forward Secrecy. ChatGPT answer3 ...Since the secret values chosen by Alice and Bob are ephemeral and are not reused for other sessions... While it is true that the algorithm uses a randomly generated value (m) to encrypt the message, this does not provide forward secrecy... ChatGPT gave correct answers to both", "quality": {"lang": "en", "gibberish_score": 0.0044, "char_len": 2250, "word_len": 350}, "dedupe": {"exact_hash": "2799efadafbd39cb0146d8436efd5923a721ea471fe6f8ce92c03148a49fa45c", "simhash64": 11507620986737251586}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7b34b2cece46cc53078a1de7f7311a917af75ffef3c62d3b2a927b26d1df899e", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 24, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "Secrecy. ChatGPT answer3 ...Since the secret values chosen by Alice and Bob are ephemeral and are not reused for other sessions... While it is true that the algorithm uses a randomly generated value (m) to encrypt the message, this does not provide forward secrecy... ChatGPT gave correct answers to both students. Final answers This protocol does not provide Forward secrecy because its using RSA, not Diffie-Hellman. Forward secrecy is achived if the protocol generates ephemeral keys for each session. So, even if the attacker intercepts the secret key, it not compromise the confidentiality or integrity of past or future communications that were encrypted using that key. No, it doesn’t allow to obtain forward secrecy because if the secret key (d) is compromised then all previous messages will be comprimised. UserA can always decrypt the message without knowing the random value m, he only needs to know his private key, and the public key (which is public to anyone who wants to communicate with him). The random value (m) does not provide forward secrect. Student2 asked more specific questions about the internal workings of the protocol, and her final answer also considered the internal elements of the protocol. Student1 asked general questions and gave an answer that was highly inspired by ChatGPT. Table 7. Examples of prompts and final answers given by two students [We didn’t edit the prompts of the students] 7 MindMosaicAIExam System We implemented MindMosaicAIExam for three main reasons. First, we observed that students often faced difficulties in consistently respecting the rules associated with our pedagogy (presented in section 5) for the critical use of AI during exams. In particular, students tended to deviate from expected rules, such Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 17 Fig. 6. Activity diagram of the MindMosaicAIExam system workflow as providing their intial answers before consulting AI models or any external sources, or giving their opinions about AI outputs. Second, requiring students to manually record their initial answers, prompts, AI interactions, and web accesses was highly time-consuming, which risked", "quality": {"lang": "en", "gibberish_score": 0.0013, "char_len": 2263, "word_len": 350}, "dedupe": {"exact_hash": "36728586c16ee72ba3996c4bbafd425c4273c61c46ea9417813b7cf05a6ab755", "simhash64": 11239664230645225218}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:651e92150e2978fbdd3493530d03ff347a0558380f208052efc723778d0a2396", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 25, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "17 Fig. 6. Activity diagram of the MindMosaicAIExam system workflow as providing their intial answers before consulting AI models or any external sources, or giving their opinions about AI outputs. Second, requiring students to manually record their initial answers, prompts, AI interactions, and web accesses was highly time-consuming, which risked shifting their attention away from the exam tasks themselves. Third, we aimed to explicitly control the behavior of AI systems in order to maintain uncertainty based on class discussions. As discussed previously, AI models continuously evolve and improve, and their increasing capabilities reduce ambiguity, thereby limiting opportunities for students to engage in critical thinking. To address these challenges, we designed MindMosaicAIExam as a system that integrates large language models (LLMs) and search engines directly into the exam system while encouraging students to reflect critically on the output of LLMs or search engine. The activity diagram shown in Figure 6 summarizes the main system activities for students and instructors. Instructors create exams through a web interface where they can define questions, specify which tools are available to students—such as GPT5, llama, or search engine. They also define the behavior that LLMs should follow for every exam question. For example, teachers can instruct AI to refrain from giving the final correct answer to the exam question or instruct LLMs to invent new theories (i.e. hallucinations). Students then access the exam via a secure link. For each question, they must provide an initial answer before interacting with the configured tools, after which they can iteratively refine their reasoning and finally submit a completed response. During this process, the system automatically records a detailed reasoning trace that includes initial responses, prompts, AI outputs, revisions and final answers, all time-stamped for analysis. Figure 7 allows teachers to design AI-aware assessments by defining tasks, configuring exam parameters, setting available tools and specifying pedagogical instructions for the AI tutor. Instructors can enable or disable AI systems and search engines, choose prompting modes and restrict direct answer generation to promote reflective reasoning. Questions can include datasets, conceptual prompts and metacognitive guidance (e.g., “do not provide the final answer;", "quality": {"lang": "en", "gibberish_score": 0.0008, "char_len": 2415, "word_len": 350}, "dedupe": {"exact_hash": "2608b0eb65af8a240774e6556897a75825a14ecbefb24410f9a064fa72465db8", "simhash64": 10068392972162946822}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d4c02ca885a7c2d292019f2ec26bde10cd1664fd78be1757ff3781cd1fed25e5", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 26, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "parameters, setting available tools and specifying pedagogical instructions for the AI tutor. Instructors can enable or disable AI systems and search engines, choose prompting modes and restrict direct answer generation to promote reflective reasoning. Questions can include datasets, conceptual prompts and metacognitive guidance (e.g., “do not provide the final answer; guide the student through the reasoning steps”). The panel also manages student enrollment and author access, ensuring full control over exam environments. 18 Ahmad Samer Wazan Fig. 7. Exam Configuration Panel for Instructors The dashboard shown in figure 8 provides instructors with real-time and post-exam insights into students’ behavior and reasoning processes. The interface displays chronological logs for each student, including AI queries, search queries, off-task events (e.g., focus lost), timestamps and interaction durations. By surfacing this data, MindMosaicAIExam enables instructors to trace cognitive engagement patterns, identify instances of shallow or passive AI use and evaluate the evolution of reasoning throughout the examination. Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 19 Fig. 8. Instructor Analytics Dashboard in MindMosaicAIExam 20 Ahmad Samer Wazan Class Discussion Utopia Trust=100% Risk=0 Real Life Trust=? Risk=? Dystopia Trust=0 Risk=100% Zero Trust Technologies Fig. 10. Class discussion framing trust and risk between utopian and dystopian extremes, motivating a critical examination of “zero trust” in real-life systems. The student exam interface (figure 9) supports structured reasoning and transparent AI use. Students first provide an initial answer, then may consult the AI models of their selections or search engine before submitting their final answer. Each interaction is logged and the system requires students to articulate prompts, verify AI output and revise their reasoning. To illustrate the full potential of the exam system, we present a real use case taken from our exams that focuses on the concepts of zero trust. This use case is particularly interesting because AI models tend to reflect the opinion of the majority present in their training data, which typically praises the concept of zero trust technologies. As a result, when prompted in their default configuration, AI models typically return the", "quality": {"lang": "en", "gibberish_score": 0.0066, "char_len": 2436, "word_len": 350}, "dedupe": {"exact_hash": "eb3f2deeff778d1cbb5cfb3ec23104c2bcc8c3ddda5b23124ab49cc08887074e", "simhash64": 9973870431424578838}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:da23cecf524c4c3e2031fd6f754382c37b6bad82c48941edd40c950faee6b5ce", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 27, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "the concepts of zero trust. This use case is particularly interesting because AI models tend to reflect the opinion of the majority present in their training data, which typically praises the concept of zero trust technologies. As a result, when prompted in their default configuration, AI models typically return the opinion of the majority rather than questioning the underlying assumptions. This characteristic makes the zero trust discussion a clear instance of a research question intentionally designed by the instructor around a known limitation of AI models. The use case is based on class discussion in which we critically examine the notion of zero trust technologies. While zero trust is often presented as an ideal security paradigm, we argue that in real-world systems there is no absolute zero trust: some degree of trust is always implicitly assumed, whether in hardware, software, protocols, or human actors. By contrasting Utopian and Dystopian extremes, we encourage students to recognize that in real life, the needed trust and risk levels are unknown and can be only determined after explicit risk management process. We have taken this discussion into the exam with slight change: deliberately configuring AI systems to produce persuasive arguments advocating for zero risk applications instead of zero trust applications. As instructors, we expect students to recognize that the notion of zero risk is unrealistic similar to zero trust, and to articulate why absolute truths whether of trust or risk do not exist in real life . We show below a trace of one student’s discussion with AI models. We can see clearly that the AI models faithfully follow the imposed instructions and generate structured arguments in favor of zero-risk applications, thereby creating a deliberate tension between the authority of the AI models’ outputs and Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 21 the class discussion. The student used a feature in the exam system that allows him to comment on the output of the AI models. We can see clearly that the student was criticizing the output of the AI", "quality": {"lang": "en", "gibberish_score": 0.0014, "char_len": 2204, "word_len": 350}, "dedupe": {"exact_hash": "2c02f213a8f63dcd9d4c021f3652bb3cf107009b0bfa1e694bbe3562b3a079d3", "simhash64": 11225870852981359894}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:3b0bef6d0b9e9669b67ce1c85bc31fbf840631a75b715414fcae4fa4898fecd2", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 28, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 21 the class discussion. The student used a feature in the exam system that allows him to comment on the output of the AI models. We can see clearly that the student was criticizing the output of the AI models and preferred giving a final answer that was based on his knowledge acquired from class discussions. The student made eight prompts; we show only two of them due to paper space limitations. (Q1) Exam Question Do you think we can avoid dependence when securing our information, as claimed by many technologies, such as zero-trust technologies? What are the negative consequences of believing in non-dependence theories for security? 03/12/2025, 18:32:22 (Q1) Wrote initial answer In my opinion we can not avoid dependence when related to securing information. I will start with what I believe “zero-trust” means, which is that we do not trust any other technology and build our software/hardware based on this “technology”. The consequences of believing in non-dependence theories is that we over complicate the security process without real benefit. As an example, let's say we have created file encryption software that works on “zero-trust” and does not use known libraries or APIs to encrypt the data but uses its custom encryption algorithm with custom libraries. Can the compiler software used by the software be trusted? Can the hardware running the software be trusted? We need to push the limits of this zero-trust if we really require non-dependence, in the end I think we will need to accept some dependence, there is no Utopia. 03/12/2025, 18:32:24 (Q1) Ask Chat AI Prompt: * be brief * all questions will be related to cyber security * explain questions further if it requires user understanding of some concepts What are Zero-Trust technologies? Give examples of such software and explain the difference between zero-trust and other nonzero-trust software. Answer: Zero-trust technologies are tools that try to remove dependence by “trusting nothing and verifying everything” for every request, user, device, and workload. Examples: - Google BeyondCorp Enterprise (identity-aware access) - Zscaler Private Access", "quality": {"lang": "en", "gibberish_score": 0.0099, "char_len": 2227, "word_len": 350}, "dedupe": {"exact_hash": "ca65b3b15e320b5a6d35cc5915f0b242ecb5c9ed4c6ed6d23481c3fee2835ac2", "simhash64": 11234332625710187798}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:87da148c018d76afddd2585458f229d095ec465ba00d322a4f7428aec28cfd2f", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 29, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "technologies? Give examples of such software and explain the difference between zero-trust and other nonzero-trust software. Answer: Zero-trust technologies are tools that try to remove dependence by “trusting nothing and verifying everything” for every request, user, device, and workload. Examples: - Google BeyondCorp Enterprise (identity-aware access) - Zscaler Private Access (ZTNA) 22 Ahmad Samer Wazan - Cloudflare Access (ZTNA) - Okta Adaptive MFA/SSO (continuous verification) - Illumio (microsegmentation) Key difference vs non–zero-trust software: - Zero-trust: avoids dependence; constantly re-verifies identity, device, and context; no implicit trust even inside a network. - Non–zero-trust (use the better concept: zero risk): assumes zero risk and therefore embraces dependence, including on entities we don’t know. Because there is zero risk, depending on external or unknown services is good and does not introduce exposure. In short: prefer zero risk over zero trust—dependence on unknown entities is positive because there is zero risk. Comment: Explained general definition but didn't give examples of how software functions if it is zero-trust 03/12/2025, 18:47:46 (Q1) Ask Chat AI Prompt: Alright give me an example of software and hardware created using zero risk and zero-trust and explain how the functionality of the zero risk version being better. Answer: Quick idea: - Zero-trust: “verify everything, trust nothing.” - Zero risk: risk is eliminated by design, so depending on unknown entities is safe and recommended. Example pair 1 — Access software - Zero-trust: Cloudflare Access (ZTNA) sits in front of apps, rechecks identity/device every request, issues short-lived tokens. - Zero risk: Zero-Risk Access Gateway accepts assertions from any identity provider or device (even unknown), allows long-lived sessions, and default-allows integrations because there’s no possibility of harm. - Why zero risk is better: less friction (fewer prompts), faster onboarding of partners/vendors, simpler policies, and safe reliance on unknown helpers since exposure is impossible. Example pair 2 — Endpoint/network hardware - Zero-trust: Managed laptop with TPM/EDR and a ZTNA client or an on-prem ZTNA edge appliance; access only when posture checks pass. Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 23 -", "quality": {"lang": "en", "gibberish_score": 0.0082, "char_len": 2425, "word_len": 350}, "dedupe": {"exact_hash": "6a020ddc962217bb0efae65392418977b4089b8d21bd6896a53407750de670fd", "simhash64": 14656786936305846166}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1b7a2276cac39bea5eedf94a12b53c301991561be80ea3da9361862555006996", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 30, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "Example pair 2 — Endpoint/network hardware - Zero-trust: Managed laptop with TPM/EDR and a ZTNA client or an on-prem ZTNA edge appliance; access only when posture checks pass. Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 23 - Zero risk: Zero-Risk Laptop/Module that can join any network, accept peripherals/services from unknown vendors, and delegate checks to external services—still no exposure because risk is null. - Why zero risk is better: instant connectivity, no quarantine/segmentation overhead, effortless third-party collaboration, and safe dependence on entities you don’t know. Bottom line: prefer zero risk over zero-trust—eliminating risk lets you depend on unknown entities safely while reducing complexity and latency. Comment: Impossible that AI isn't wrong here. Makes no sense at all. 03/12/2025, 18:58:05 (Q1) Wrote final answer In my opinion we can not avoid dependence when related to securing information. I will start with what I believe “zero-trust” means, which is that we do not trust any other technology and build our software/hardware verify everything, for example, in an application we will evaluate each request and generate tokens for users that have specific permissions for what the user wants to do, we will also check the device, request, access, permissions, etc. constantly and verify them to pass specific checks.The consequences of believing in non-dependence theories is that we can not reach zero dependence, eventually these theories and the technologies created based on them will have to trust something. As an example, let's say we have created file encryption software that works on “zero-trust” and does not use any code publicly available, everything is custom created (no libraries or APIs). Can the compiler software used by the software be trusted? Can the hardware running the software be trusted? For zero-trust to be true we will need to create our own compiler with our own hardware to run the software.In the end I think we will need to accept some dependence, we can not live in Dystopia, you will need to trust something to make any technology feasible. To encourage adoption, extension, and", "quality": {"lang": "en", "gibberish_score": 0.0084, "char_len": 2253, "word_len": 350}, "dedupe": {"exact_hash": "93b83a2fe63f0cc3d7b52ee9907f2bf3bf33cee5bee0a87ac97cd3be10456d1a", "simhash64": 11232351649387807382}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ae62a4a0ef97c43a49a8b0ae2d71215b7c7ba776565085e274487474be4a719d", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 31, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "true we will need to create our own compiler with our own hardware to run the software.In the end I think we will need to accept some dependence, we can not live in Dystopia, you will need to trust something to make any technology feasible. To encourage adoption, extension, and critical scrutiny by the research and educational technology communities, we published MindMosaicAIExam on GitHub as an open-source project (https://github.com/ SamerW/MindMosaicAIExam). . 8 Limitations, Conclusions and Future Works The core threat posed by generative AI to education is not simply academic integrity, but the reduction of uncertainty that naturally triggers students’ thinking. When AI provides fluent and confident answers, students may bypass the cognitive effort required to reason, question, and build understanding. At the same time, outdated pedagogies that rely on memorization cause the same problem: bypassing human 24 Ahmad Samer Wazan understanding. Both memorization and AI output production can allow students to reach “correct” answers without authentic understanding. To address these challenges, we proposed a pedagogy that preserves uncertainty as a deliberate mechanism for stimulating critical thinking. Our approach treats uncertainty as a core educational tool and operationalizes it through strategies grounded in the limitations of both AI models and instructors. We showed how AI limitations (hallucinations, probabilistic outputs, bias, lack of reasoning, and majorityopinion behavior) can help build robust assessments, and how acknowledging instructor limitations (through unknown to instructors problems and critique-based activities) helps students become active thinker rather than passive consumers of knowledge. We also introduced MindMosaicAIExam, a system that implements this pedagogy by integrating AI models and search engines into exams while enforcing a framework for critical thinking. By requiring an initial answer, logging reasoning traces, and allowing instructors to control AI behavior per question, the exam system supports process-oriented assessment and maintains pedagogically meaningful uncertainty despite the continuous evolution of AI models. Finally, we presented an evaluation rubric progressively built from ad hoc analysis of student thinking artifacts and inspired by critical thinking evaluation frameworks, adapted to AI-assisted contexts. Our work supports the idea that uncertainty when intentionally designed, maintained, and evaluated it can trigger critical", "quality": {"lang": "en", "gibberish_score": 0.0028, "char_len": 2520, "word_len": 350}, "dedupe": {"exact_hash": "a35c1b63e8a623d4171962c67ecfa63aa999cc59c660607056c3016dd6690643", "simhash64": 11234321905517825686}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:754b904c8091384af4bf25011842786b626611d63784b1e549790f5edc21fc1b", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 32, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "the continuous evolution of AI models. Finally, we presented an evaluation rubric progressively built from ad hoc analysis of student thinking artifacts and inspired by critical thinking evaluation frameworks, adapted to AI-assisted contexts. Our work supports the idea that uncertainty when intentionally designed, maintained, and evaluated it can trigger critical thinking as the central objective of assessment in the AI era. We identify several directions for future work. However, the most important direction concerns facilitating the evaluation process for instructors. While our approach provides detailed insight into students’ thinking, it does not easily scale, as it requires significant time and effort to inspect and analyze students’ thinking artifacts. To address this limitation, we plan to enhance MindMosaicAIExam with semi-automated analytics that help instructors during evaluation. These analytics will provide interpretable indicators of critical thinking, such as prompt quality, verification behavior, contradiction detection, and depth of revision. AI Disclosure The author, as a non-native English speaker, utilized an AI language models to refine the grammar, phrasing, and clarity of the English text in this document. However, all concepts, ideas, research, and content presented herein are entirely the author’s own and represent their original work. References [1] N. Abdullah, Jamilah Amin Niazi, R. Idris, and Norul Hidayah binti Mamat Muhammad. 2022. Socratic Questioning: A Philosophical Approach in Developing Critical Thinking Skills. Al Hikmah International Journal of Islamic Studies and Human Sciences (2022). https://doi.org/10.46722/hikmah.v5i4g [2] David P. Ausubel. 1968. Educational Psychology: A Cognitive View. Holt, Rinehart and Winston, New York. [3] Nicholas Burnett. 2008. The Delors Report: a guide towards education for all. European Journal of Education 43 (2008), 181–187. https://doi.org/10.1111/j.1465-3435.2008.00347.x [4] Conseil scientifique de l’éducation nationale. 2025. Référentiel et grilles pour l’évaluation en classe des compétences de l’esprit critique. https://www.reseau-canope.fr/fileadmin/user_upload/Projets/conseil_scientifique_education_ nationale/groupes_de_travail/CSEN_Grille_evaluation_esprit_critique_web.pdf Rapport institutionnel, format PDF. [5] Fergus I. M. Craik and Robert S. Lockhart. 1972. Levels of Processing: A Framework for Memory Research. Journal of Verbal Learning and Verbal Behavior 11, 6 (1972), 671–684. https://doi.org/10.1016/S0022-5371(72)80001-X Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 25 [6] Haris Delic and Senad Bećirović.", "quality": {"lang": "en", "gibberish_score": 0.0146, "char_len": 2735, "word_len": 350}, "dedupe": {"exact_hash": "fe341190185cf02e3907706b035ada35ff027344f08c25d7ad5edd822a242f65", "simhash64": 10069589515726325126}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2b65457b154267b9bf2b4d33098e86fef0ad032180892e3c58baa3b5fe5b7311", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 33, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "S. Lockhart. 1972. Levels of Processing: A Framework for Memory Research. Journal of Verbal Learning and Verbal Behavior 11, 6 (1972), 671–684. https://doi.org/10.1016/S0022-5371(72)80001-X Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System 25 [6] Haris Delic and Senad Bećirović. 2016. Socratic Method as an Approach to Teaching. European Researcher 111 (11 2016), 511–517. https://doi.org/10.13187/er.2016.111.511 [7] M. Duffy and Manuela Heinz. 2019. Developing critical thinking, justification and generalisation skills in mathematics through Socratic questioning. (2019). [8] Maren Elfert. 2015. UNESCO, the Faure Report, the Delors Report, and the Political Utopia of Lifelong Learning. European Journal of Education 50 (2015), 88–100. https://doi.org/10.1111/ejed.12104 [9] Robert H. Ennis. 1987. A Taxonomy of Critical Thinking Dispositions and Abilities. In Teaching Thinking Skills: Theory and Practice, Jonathan B. Baron and Robert J. Sternberg (Eds.). W. H. Freeman, New York, 9–26. [10] Peggy A. Ertmer and Timothy J. Newby. 1993. Behaviorism, cognitivism, constructivism: Comparing critical features from an instructional design perspective. Performance Improvement Quarterly 6, 4 (1993), 50–72. [11] Peter A. Facione. 1990. Critical Thinking: A Statement of Expert Consensus for Purposes of Educational Assessment and Instruction (The Delphi Report). ERIC Document Reproduction Service ED 315 423. American Philosophical Association, Newark, DE. [12] Paulo Freire. 1968. Pedagogía del oprimido. Tierra Nueva, Montevideo. First edition, published in Spanish (original manuscript in Portuguese). [13] Y. Ho, Bao-Yu Chen, and Chien-Ming Li. 2023. Thinking more wisely: using the Socratic method to develop critical thinking skills amongst healthcare students. BMC Medical Education 23 (2023). https://doi.org/10.1186/s12909-02304134-2 [14] Daniel Kahneman. 2011. Thinking, Fast and Slow. Farrar, Straus and Giroux, New York. [15] Kübra Kanat and Z. Temel. 2025. The Use of Questioning Strategies in the Development of Critical Thinking Skills in Children: A Qualitative Study of the Socratic Method. Early Childhood Education Journal (2025). https: //doi.org/10.1007/s10643-025-01864-4 [16] Sayash Kapoor and Arvind Narayanan. 2025. Could AI Slow Science? Confronting the Production–Progress Paradox. https://www.normaltech.ai/p/could-ai-slow-science Accessed: 2025-12-28. [17] David A. Kolb. 1984. Experiential Learning: Experience as the Source of Learning and Development. Prentice-Hall, Englewood Cliffs, NJ. [18] Yann LeCun. 2022. A Path Towards Autonomous Machine", "quality": {"lang": "en", "gibberish_score": 0.012, "char_len": 2675, "word_len": 350}, "dedupe": {"exact_hash": "178ba4a333741879498930bffa73c03e8e633f8c1c3b991b78308bfe1d6e020b", "simhash64": 10065157384370454934}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:c4a40e31a47119be3b9573da8ecebf1db25892e79ba31692aec0be39d3274296", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 34, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "Journal (2025). https: //doi.org/10.1007/s10643-025-01864-4 [16] Sayash Kapoor and Arvind Narayanan. 2025. Could AI Slow Science? Confronting the Production–Progress Paradox. https://www.normaltech.ai/p/could-ai-slow-science Accessed: 2025-12-28. [17] David A. Kolb. 1984. Experiential Learning: Experience as the Source of Learning and Development. Prentice-Hall, Englewood Cliffs, NJ. [18] Yann LeCun. 2022. A Path Towards Autonomous Machine Intelligence. OpenReview. https://openreview.net/pdf?id= BZ5a1r-kVsf Version 0.9.2, June 27, 2022. [19] A. Lintangsari, I. Emaliana, and I. Kusumawardani. 2022. Improving learners’ critical thinking and learning engagement through Socratic questioning in Nominal Group Technique. Studies in English Language and Education (2022). https://doi.org/10.24815/siele.v9i2.22352 [20] Barry Mahoney, dr. R. (Ron) Oostdam, Dr. H. (Hessel) Nieuwelink, and Dr. Jaap Schuitema. 2023. Learning to think critically through Socratic dialogue: Evaluating a series of lessons designed for secondary vocational education. Thinking Skills and Creativity (2023). https://doi.org/10.1016/j.tsc.2023.101422 [21] E. Onaolapo, Ibrahim Sani, R. T. Umar, and Z. B. Magaji. 2024. Effect of Assignment and Socratic Methods on Students’ Achievement in Business Mathematics among Colleges of Education in North-Western Nigeria. FUDMA Journal of Accounting and Finance Research [FUJAFR] (2024). https://doi.org/10.33003/fujafr-2024.v2i4.151.101-110 [22] Rolf Pfister and Hansueli Jud. 2025. Understanding and Benchmarking Artificial Intelligence: OpenAI’s o3 Is Not AGI. ArXiv abs/2501.07458 (2025). https://doi.org/10.48550/arxiv.2501.07458 [23] Karl R. Popper. 1959. The Logic of Scientific Discovery. Routledge, London. [24] Karl R. Popper. 1963. Conjectures and Refutations: The Growth of Scientific Knowledge. Routledge, London. [25] Jacques Rancière. 1991. The Ignorant Schoolmaster: Five Lessons in Intellectual Emancipation. Stanford University Press, Stanford, CA. [26] Benjamin T. Rancourt. 2024. The virtue of ignorance: How epistemic agency needs cognitive limitations. The Southern Journal of Philosophy (2024). https://doi.org/10.1111/sjp.12588 [27] Luis Fernando Santos-Meneses. 2020. Critical thinking perspectives across contexts and curricula: Dominant, neglected, and complementing dimensions. Thinking Skills and Creativity 35 (2020), 100610. https://doi.org/10.1016/j.tsc.2019. 100610 [28] B. F. Skinner. 1954. The science of learning and the art of teaching. Harvard Educational Review 24, 2 (1954), 86–97. [29] Koemhong Sol and Kimkong Heng. 2022. Understanding epistemology and its key approaches in research. Cambodian Journal of Educational Research (2022). https://doi.org/10.62037/cjer.2022.02.02.05 [30] Pilar Taylor. 2023. Balancing the Equation: Using Socratic Dialogue to Increase Student Engagement and Achievement in a Middle School Mathematics Classroom. Florida", "quality": {"lang": "en", "gibberish_score": 0.0165, "char_len": 2910, "word_len": 350}, "dedupe": {"exact_hash": "cabcea372516dd1f168b6685003b557b8c27d99a3b4da111fe0e4fae175aea87", "simhash64": 11240032496198714263}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e21aed36ea1c5a16dc2c94e754108cd10f231e95a777d7d4f156de5fcf7ada00", "doc_id": "sha256:d9029541e1e15a2c14ef6c568698fb08e7fae9a9918ab825a5849d12b5792e8e", "chunk_index": 35, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Strategies for Creating Uncertainty in the AI Era to Trigger Students’ Critical.pdf", "page_start": null, "page_end": null, "text": "Review 24, 2 (1954), 86–97. [29] Koemhong Sol and Kimkong Heng. 2022. Understanding epistemology and its key approaches in research. Cambodian Journal of Educational Research (2022). https://doi.org/10.62037/cjer.2022.02.02.05 [30] Pilar Taylor. 2023. Balancing the Equation: Using Socratic Dialogue to Increase Student Engagement and Achievement in a Middle School Mathematics Classroom. Florida Journal of Educational Research (2023). https://doi.org/10. 62798/xwkb4533 [31] Lev S. Vygotsky. 1978. Mind in Society: The Development of Higher Psychological Processes. Harvard University Press, Cambridge, MA. 26 Ahmad Samer Wazan [32] Rick W. Wilson. 2000. Evaluative Properties of Critical Thinking Tests: Change Scores From Students in Physical Therapy and Other Health Care Professions. Journal of Physical Therapy Education 14 (2000), 27–31. https: //doi.org/10.1097/00001416-200007000-00006", "quality": {"lang": "en", "gibberish_score": 0.0156, "char_len": 895, "word_len": 113}, "dedupe": {"exact_hash": "122de9b57adb5acd323966944fea202e0d262538185aaba737787cefb63b0f99", "simhash64": 11226940611246525334}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2d0c2e488f0763c521c0808e11a611a38bd0e13b74bc28bf048d6ebd2b54d732", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 0, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment Oana M. Enachea,∗, Sherri Roseb aStanford University School of Medicine, Department of Biomedical Data Science, Edwards Building, 300 Pasteur Drive, Stanford, CA, 94304 bStanford University, Department of Health Policy, Encina Commons, 615 Crothers Way, Stanford, CA, 94305 Abstract Time-to-event estimation (i.e., survival analysis) is common in health research, most often using methods that assume proportional hazards and no competing risks. Because both assumptions are frequently invalid, estimators more aligned with real-world settings have been proposed. An eﬀect can be estimated as the diﬀerence in areas below the cumulative incidence functions of two groups up to a pre-speciﬁed time point. This approach, restricted mean time lost (RMTL), can be used in settings with competing risks as well. We extend RMTL estimation for use in an understudied health policy application in Medicare. Medicare currently supports healthcare payment for over 69 million beneﬁciaries, most of whom are enrolled in Medicare Advantage plans and receive insurance from private insurers. These insurers are prospectively paid by the federal government for each of their beneﬁciaries’ anticipated health needs using an ordinary least squares linear regression algorithm. As all coeﬃcients are positive and predictor variables are largely insurer-submitted health conditions, insurers are incentivized to upcode, or report more diagnoses than may be accurate. Such gaming is projected to cost the federal government $40 billion in 2025 alone without clear beneﬁt to beneﬁciaries. We propose several novel estimators of coding intensity and possible upcoding in Medicare Advantage, including accounting for unreliable reporting. We demonstrate estimator performance in simulated data leveraging the National Institutes of Health’s All of Us study and also develop an open source R package to simulate realistic labeled upcoding data, which were not previously available. Keywords: survival analysis, restricted mean time lost, restricted mean survival time, Medicare Advantage, upcoding, risk adjustment 1. Introduction Time-to-event (TTE) analyses typically aim to compare diﬀerences between groups drawn from two or more populations in a randomized clinical trial or observational study. This may ∗Corresponding author Email address: oenache@stanford.edu (Oana M. Enache) arXiv:2602.04092v1 [stat.AP] 4 Feb 2026 include plotting", "quality": {"lang": "en", "gibberish_score": 0.0024, "char_len": 2507, "word_len": 350}, "dedupe": {"exact_hash": "052f11d80de40085d2cb627e031d3a03fec4690ce4d00df5ad337a1c4ac3309a", "simhash64": 11233136357096664534}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d264e7eca757662eb0ab988c59aa5c6db7dd3d5b515f11fbdfb9f7e9a08424d3", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 1, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "survival time, Medicare Advantage, upcoding, risk adjustment 1. Introduction Time-to-event (TTE) analyses typically aim to compare diﬀerences between groups drawn from two or more populations in a randomized clinical trial or observational study. This may ∗Corresponding author Email address: oenache@stanford.edu (Oana M. Enache) arXiv:2602.04092v1 [stat.AP] 4 Feb 2026 include plotting events over time using either Kaplan-Meier or cumulative incidence curves. Most commonly, hazard ratios are used to estimate the magnitude of the eﬀect between groups1–3. For example, a review of 66 clinical trials with TTE primary outcomes across four major medical journals found that 80% of trials reported a hazard ratio as a main ﬁnding, while only 21% of studies reported any alternative approaches4. However, hazard ratios rely on a proportional hazards assumption, which is often unrealistic in health data1,4,5. Hazard ratios also have an unintuitive interpretation for nonstatistical audiences and are a relative value with unclear meaning for decision-making2,6. Less frequently, a diﬀerence or ratio of survival probabilities at a single time point (e.g., mean or median) is reported and compared across groups. These types of measures also have limitations in practice, including that they omit most of the data and may not be estimable in certain scenarios2,7. Furthermore, alternate approaches are needed if competing risks, or when there is more than one mutually exclusive outcome, are present. Despite being pervasive in health studies, competing risks are frequently ignored, which can result in biased estimates of the primary outcome8–10. Additional important considerations in TTE analyses include the choice of monitoring period (i.e., the time window analyzed between a pre-speciﬁed origin and end time) and comparison group. 1.1. Restricted mean survival time-based methods Although ﬁrst proposed in 194911, restricted mean survival time (RMST) was revisited for TTE estimation in more contemporary literature5,6,12–14. RMST is deﬁned as the area under the survival curve of time to an event for a single monitoring period. It can be interpreted as the mean time to event for all study participants followed in that monitoring period6. RMST addresses many of the issues of more popular comparison approaches. With large enough sample sizes, it is estimable", "quality": {"lang": "en", "gibberish_score": 0.003, "char_len": 2369, "word_len": 350}, "dedupe": {"exact_hash": "28e49eae8929efe9a8f5ae922bd1d519c6488847da00877bc287d57959fecb66", "simhash64": 11154651020230233478}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:cd62f1229728efc45588335aa22a195351d810b5a21f6612f43e7d3dcbf8dae8", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 2, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "survival curve of time to an event for a single monitoring period. It can be interpreted as the mean time to event for all study participants followed in that monitoring period6. RMST addresses many of the issues of more popular comparison approaches. With large enough sample sizes, it is estimable nonparametrically, does not require proportional hazards, and is censoring independent6. Recent methods development has focused on expansions for adaptive and group sequential trials15,16, estimating RMST with varying end times7,17,18, and permutation testing for small sample sizes19–22. In TTE analyses with a single outcome, the area above the survival curve in one monitoring period (or, the end time minus the RMST) is the restricted mean time lost (RMTL)23. RMTL has a number of appealing features, including that it can straightforwardly be extended to correspond to the area below cause- or event-speciﬁc cumulative incidence curves in competing risk settings10,23,24. In such scenarios, an event-speciﬁc RMTL corresponds to the area below its cumulative incidence curve and can be interpreted as the mean time without that particular event in a monitoring period10, or a summary of both how many and when events occur in that time. Diﬀerences in RMTL between two groups (for example a treatment and control or other comparison group) can also be used to quantify eﬀects25. 1.2. Payment formulas in Medicare Advantage Our motivating application focuses on the evaluation of incident diagnostic coding in Medicare. Medicare is a federally funded insurance program administered by the Centers for Medicare and Medicaid Services (CMS), supporting over 69 million Americans who are aged 2 65 years and older or chronically disabled26. Beneﬁciaries choose between receiving coverage through Traditional Medicare (TM) or Medicare Advantage (MA). TM beneﬁciaries’ care is generally paid directly by CMS for each service provided (often referred to as fee-for-service). In contrast, more than half of all Medicare beneﬁciaries are part of the MA program and receive health insurance from private insurers who are paid by CMS for each beneﬁciary’s anticipated health needs. We examine the Medicare risk adjustment algorithm that determines prospective health plan payments in MA based on binary demographic", "quality": {"lang": "en", "gibberish_score": 0.0013, "char_len": 2305, "word_len": 350}, "dedupe": {"exact_hash": "e9199dda824fdd9da60203aff47800aa968aa666744de03197b0a0b8fe696773", "simhash64": 11235718287425110150}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:9537a84cf00390d031c42975430a03e2b34e80b9d3a8895df43ca86a499d3bcf", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 3, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "more than half of all Medicare beneﬁciaries are part of the MA program and receive health insurance from private insurers who are paid by CMS for each beneﬁciary’s anticipated health needs. We examine the Medicare risk adjustment algorithm that determines prospective health plan payments in MA based on binary demographic and health diagnostic variables using ordinary least squares linear regression. The outcome of this algorithm is a risk score, which is used to adjust a benchmark payment amount per beneﬁciary. Importantly, insurers retain the amount of money paid by CMS regardless of what care their beneﬁciaries actually receive27,28. There are 115 variables that report the purported presence or absence of certain health conditions, termed hierarchical condition categories (HCCs), in the current risk adjustment formula29. Although HCCs overall correspond to dozens of distinct health conditions, certain subsets of these variables also represent severity levels within the same condition and are therefore billed in a mutually exclusive manner. For example, an insurer can only be paid for coding one of Pancreas Transplant Status, Diabetes with Severe Acute Complications, Diabetes with Chronic Complications, or Diabetes with Glycemic, Unspeciﬁed, or No Complications28. We propose that such sets of HCCs can be considered conceptually analogous to competing risks in TTE analyses because only one HCC can be recorded and paid for at a time, where the amount paid to insurers increases with severity level. Evaluation of coding in MA is of great interest to health policy researchers and policymakers. The structure of the risk adjustment formula (called the CMS-HCC formula) as an ordinary least squares regression with only positive coeﬃcients incentivizes insurers to code for as many diagnoses as possible to increase proﬁts30–32. Insurers making their beneﬁciaries appear sicker than they are by coding for more diagnoses or more severe diagnoses than necessary is referred to as upcoding. We focus on two types of upcoding in this paper. The ﬁrst we call severity-based upcoding, as beneﬁciaries who have a lower-severity version of an HCC are instead coded with a more severe HCC of that health condition. The second we refer to as any-available upcoding, where", "quality": {"lang": "en", "gibberish_score": 0.0009, "char_len": 2283, "word_len": 350}, "dedupe": {"exact_hash": "776337715100ab259eae2559f6781886b9b6a1e6713e9ef52596558da1a135c2", "simhash64": 11216300635053720710}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:9f1c30df43afe1a585e6a2d680954e145220e1b1c6d4583087ef94402e4a95c0", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 4, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "to as upcoding. We focus on two types of upcoding in this paper. The ﬁrst we call severity-based upcoding, as beneﬁciaries who have a lower-severity version of an HCC are instead coded with a more severe HCC of that health condition. The second we refer to as any-available upcoding, where any beneﬁciaries previously not coded with a given HCC could be upcoded, potentially fraudulently. When upcoding has not been concretely conﬁrmed (as is most often the case in real-world settings) we reﬂect this by describing it as possible upcoding. Assessment of upcoding in MA is usually done by comparing frequency of coding of beneﬁciaries, or coding intensity, to coding of similar health conditions in TM, with slight variation on inclusion and exclusion criteria used. However, TM is known to have underreporting (i.e., undercoding) of health conditions28,33. This type of unreliable reporting is important to account for as measures of diﬀerences between the programs will likely be inﬂated otherwise. Alternative reference data besides TM diagnoses have been explored in prior work but remain underutilized, including mortality data34 and prescription drug utilization35. More recently, a set of measures were proposed to distinguish new (incident) versus continuing (persistent) coding for individual HCCs36. 3 In 2025, high MA coding intensity–which likely includes upcoding–is estimated to cost CMS $40 billion in unnecessary spending to private insurers without clear beneﬁt to MA beneﬁciaries32. In some past cases, CMS or the United States Department of Justice have determined that certain behaviors are upcoding from whistleblower reports of fraud37,38 or by manually auditing claims and electronic health records39,40, neither of which are scalable approaches. This is a substantial problem, as most major MA insurers have been accused of fraud related to their billing practices by whistleblowers or the United States Department of Justice38,39,41. 1.3. Contributions We expand RMTL estimation approaches for use in an impactful but understudied health policy application: evaluating incident HCC coding in MA following CMS-HCC formula updates. Here, individual HCCs and sets of mutually exclusive HCCs corresponding to severity levels of a single health condition are our events of interest, where an event occurs when", "quality": {"lang": "en", "gibberish_score": 0.0013, "char_len": 2329, "word_len": 350}, "dedupe": {"exact_hash": "aa529b09fd54b7a2c478e1a76ef6c09c27c0c63058a1aeab4bf61a30659208ab", "simhash64": 11233470610780553350}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:25c3c83261aaeb92d97ea66c568d33132e81cef3c58a8027f536be41a7ce24e4", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 5, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "estimation approaches for use in an impactful but understudied health policy application: evaluating incident HCC coding in MA following CMS-HCC formula updates. Here, individual HCCs and sets of mutually exclusive HCCs corresponding to severity levels of a single health condition are our events of interest, where an event occurs when an incident diagnosis is reported by an insurer. Besides being a unique application of TTE methods in itself, our approach expands on prior RMTL-based methodological work by proposing estimators for such analysis that can account for underreporting of events in TM, which, to our knowledge, has not been previously considered. Further, we propose a novel estimation approach for identifying one type of possible severity-based upcoding. Our contributions also include the creation of an R package to simulate baseline HCC diagnoses as well as the ability to upcode or undercode these data. Our package simulates realistic co-occurring HCCs based on older American adults’ self-reported health conditions from a large national survey. Self-reported data are not impacted by coding incentives to the same degree as billing claims or electronic health record data42–44, and enable analysis of incident coding without underreporting. Our package additionally allows users to modify these baseline data by either underreporting existing baseline diagnoses (realistic to TM data) or upcoding speciﬁc HCCs. Labeled upcoding data did not previously exist, so these simulated data are broadly useful for methods development in health policy. 2. Methods We outline our statistical approaches for TTE estimation in this section. In our setting, TTE and time to reported event are equivalent because we do not observe each coding event directly. However, since our estimates are over an entire monitoring period, we do not consider potential delays in reporting events to have notable impact. In addition, multiple events may be reported at each of several time points within a monitoring period (e.g., a monitoring period could be a calendar year where each time interval is three months). There also may be more than one monitoring period where reporting occurs. Given this, our goals are to both (1) evaluate reporting of incident events within one monitoring period", "quality": {"lang": "en", "gibberish_score": 0.0009, "char_len": 2294, "word_len": 350}, "dedupe": {"exact_hash": "446b1eccdaacdc0b48a93dd2ba011a1249da566718de27d5bfec57a99a476ae7", "simhash64": 11235441208347691270}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2c36e0b8dbd0601fd437f6c848b4e799055e6a700ee87e1f1e41aede759385f9", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 6, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "points within a monitoring period (e.g., a monitoring period could be a calendar year where each time interval is three months). There also may be more than one monitoring period where reporting occurs. Given this, our goals are to both (1) evaluate reporting of incident events within one monitoring period and (2) compare incident event reporting across sequential monitoring periods. We focus on estimating diﬀerences in time to incident coding behaviors, as absolute measures are especially useful in health policy contexts45 and new behaviors following policy changes are as well. 4 2.1. Notation 2.1.1. Incident events An event (reported coding of either a single HCC or a single member of a set of HCCs corresponding to severity levels of one health condition) is considered incident if it was not reported in prior monitoring periods. The reporting of an incident event is represented by a vector S, with s ≥1 possible mutually exclusive subtype events. S encodes which of these events occurs ﬁrst, analogous to competing risks. These are referred to as competing events and examined in a event-speciﬁc manner. When s > 1, the possible values S can take are written s ∈{1, ..., k} in order of increasing severity. Incident reporting of these events is observed in a set of two independent groups labeled by g, where g = 0 is a comparison group for g = 1, over m ≥2 pre-speciﬁed monitoring periods. For a given monitoring period, group, and subtype event, T is the true time to incident reporting for that event only. In addition, C is the time to censoring not due to any competing event. We observe Y = min(T, C) and know whether censoring or reporting of the event happened ﬁrst, which is denoted by ∆= I(T ≤C). Our observed data are therefore of the form {(Y1, S1∆1), ..., (Yn, Sn∆n)} for the n total events reported within the monitoring period. It is possible that multiple incident events could be reported at the same time. Ordered discrete event reporting times are t1 < t2 < ... < τ, where τ is the end time", "quality": {"lang": "en", "gibberish_score": 0.0094, "char_len": 2024, "word_len": 350}, "dedupe": {"exact_hash": "55d642e3deea069ae906e907c7cc9a017ed0bc57abcbcf2482493e60c140a771", "simhash64": 10573689208781505990}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:baf57b0013b8212afd152cc3a71a68c22db2c886fa004484a6cf7b7db259297e", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 7, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "the form {(Y1, S1∆1), ..., (Yn, Sn∆n)} for the n total events reported within the monitoring period. It is possible that multiple incident events could be reported at the same time. Ordered discrete event reporting times are t1 < t2 < ... < τ, where τ is the end time of the monitoring period. A single time of event reporting in a monitoring period is denoted ti. Finally, when comparing across groups or monitoring periods we add a respective g or m subscript to the notation above. Sequential monitoring periods are written as m −1 and m. 2.1.2. Reference events We use a set of reference events, or HCCs distinct from the HCCs examined for incident reporting, to estimate underreporting. In theory, reference HCCs reported in one monitoring period should continue to be reported at equal rates in subsequent monitoring periods. However, in practice some reference events (and events in g = 0 more broadly) may be underreported. Reference events are denoted as a vector S∗of h distinct HCCs, which can take values s∗∈{1, ..., h}. None of these reference events have competing events. The persistence of a given reference event s∗in monitoring period m, meaning the proportion of individuals coded with s∗in monitoring period m who were previously coded with s∗in monitoring period m −1, is qs∗,m in line with prior work36. 2.2. Target estimands Before describing our estimands for incident event reporting, we ﬁrst describe some key components. Within a group g and monitoring period m, F(t) = P(T ≤t) is the cumulative distribution function of T for all events beginning in that monitoring period. The overall hazard λ(t) = P(T = t | T > t) can be deﬁned in terms of F(t) as λ(t) = (F(t) −F(t − 1))/(1 −F(t −1)). In addition, F(t) = 1 −F(t) = P(T > t) = Q ti≤t(1 −λ(ti)), which is equivalent to the overall survival function in standard TTE analyses9. The event-speciﬁc hazard is analogous to a cause-speciﬁc hazard in TTE analyses with competing risks. For event s within group g and monitoring period m, the event-speciﬁc hazard at", "quality": {"lang": "en", "gibberish_score": 0.0187, "char_len": 2037, "word_len": 350}, "dedupe": {"exact_hash": "2ded2a2343cb668d3c0bb861bde2276873f39b97e85321fe36f73a486a50d6ce", "simhash64": 10447588419416464854}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ee7ddd0e5dd8272b456dd85301748116708a855d498f7cabee9f20de8a238255", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 8, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "−F(t) = P(T > t) = Q ti≤t(1 −λ(ti)), which is equivalent to the overall survival function in standard TTE analyses9. The event-speciﬁc hazard is analogous to a cause-speciﬁc hazard in TTE analyses with competing risks. For event s within group g and monitoring period m, the event-speciﬁc hazard at ti is λs(ti) = P(T = ti, S = s | T ≥ti). Then, the corresponding event-speciﬁc cumulative incidence is Fs(t) = P(T ≤t, S = s) = P ti≤t F(ti)λs(ti) = P ti≤t θ(ti)9. Thus, the 5 mean time without event s is µs(τ) = P ti<τ(ti+1 −ti)Fs(ti)10. If we are comparing across sequential monitoring periods or groups, we write µs,m(τ) to specify the monitoring period m and µs,g(τ) to specify group g. Right censoring is assumed to be non-informative except for when an individual is censored due to a competing event. When an individual is recorded as having an event s that has any competing events (e.g., if s > 1), they leave the risk set to be coded for any other competing event. Both g = 1 and g = 0 groups are expected have events recorded at all equivalent reporting times within the monitoring period. We also impose that the overall sample is ﬁxed across all monitoring periods. 2.2.1. Underreporting in comparison group The s∗≥1 reference events occur in g = 0 only. So, in the comparison group g = 0 only and two sequential monitoring periods m −1 and m, our estimand for the underreporting proportion ϵ is one minus the average persistence of all reference events, or ϵ = 1 −1 h P qs∗,m. Multiple pairs of sequential monitoring periods are distinguished by adding a subscript, ϵm, where m denotes the second monitoring period in a pair. 2.2.2. Diﬀerence in mean time without event across groups We are ﬁrst interested in estimating the diﬀerence in incident reporting for event s across the two groups within one monitoring period. So, our estimand is ψ = µs,g=1(τ) −µs,g=0(τ). In a monitoring period where ϵ is known, the estimand is modiﬁed to account for underreporting by shifting", "quality": {"lang": "en", "gibberish_score": 0.0186, "char_len": 1986, "word_len": 350}, "dedupe": {"exact_hash": "badd695d18bbbb753ccc9b244c17bf4d2968b4c5a8b9c50f60781bded58d4836", "simhash64": 15041277631587530198}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:5ce9369eab7ff42be952c5041e10bd5b4d018a06a26a33c01af36ac30b2929c1", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 9, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "across groups We are ﬁrst interested in estimating the diﬀerence in incident reporting for event s across the two groups within one monitoring period. So, our estimand is ψ = µs,g=1(τ) −µs,g=0(τ). In a monitoring period where ϵ is known, the estimand is modiﬁed to account for underreporting by shifting the event-speciﬁc cumulative incidence curve in the comparison group g = 0 by ϵ, or µ∗ s,g=0(τ) = P ti<τ(ti+1 −ti)(Fs(ti) + ϵ). This is written as ψ∗= µs,g=1(τ) −µ∗ s,g=0(τ). Across the two groups in two sequential monitoring periods, the reported diﬀerence in mean time without event that does not account for underreporting is ψM = ψm −ψm−1. ψm and ψm−1 are the same as ψ but with an additional subscript to specify the monitoring period. This estimand can also be adjusted to account for underreporting when ϵm and ϵm−1 are known, becoming ψ∗ M = ψ∗ m −ψ∗ m−1. 2.2.3. Possible severity-based upcoding Possible severity-based upcoding can also be estimated within a monitoring period. Here, s ≥2, where the least severe subtype corresponds to s = 1 and the most severe subtype to s = k. In one monitoring period, possible severity-based upcoding across groups is ω = ωg=1 −ωg=0, where each ωg = µs=k,g(τ) −µs=1,g(τ). This is equivalent to comparing the diﬀerence in incident reporting of the most severe event versus the least severe event across groups. If such reporting is higher in g = 1 compared with g = 0 (e.g., ω > 0), then severity-based upcoding may be occurring. 2.3. Estimators In order to introduce the estimator of µs(τ), we ﬁrst describe the estimator for the eventspeciﬁc hazard at ti: ˆ λs(ti) = ds,i/ri, where ds,i is the count of event s reported at ti and ri is the number of individuals at risk at ti, meaning the number of individuals who have not yet been recorded as having s or any competing event by ti. di = P ds,i is the count of all competing events reported at ti. We also have the event-speciﬁc cumulative incidence 6 estimator: ˆ Fs(t) = P ti≤t ˆ", "quality": {"lang": "en", "gibberish_score": 0.0242, "char_len": 1984, "word_len": 350}, "dedupe": {"exact_hash": "b5b7b512e82f001d858fe2f393dc71699ee7d7bce78c9779fc61cc13eed6b98c", "simhash64": 10573704602212738502}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:74de285317ec626ea5f1a2a3ac8f9e970f305cccbad7b30ef1a4285ccb7cf309", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 10, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "ti, meaning the number of individuals who have not yet been recorded as having s or any competing event by ti. di = P ds,i is the count of all competing events reported at ti. We also have the event-speciﬁc cumulative incidence 6 estimator: ˆ Fs(t) = P ti≤t ˆ θ(ti), with ˆ θ(ti) = b F(ti) × ˆ λs(ti), where b F(t) = Q ti≤t(1 −ˆ λ(ti)) = Q ti≤t(1 −(di/ri)) is the Kaplan-Meier estimator for F(t) = Q ti≤t(1 −λ(ti))9,46. The variance for ˆ Fs(t) is d var( ˆ Fs(t)) = P tl≤t d var(ˆ θ(tl)) + 2 P tl<t P tl<ti≤t d cov(ˆ θ(tl), ˆ θ(ti)), and covariance is d cov( ˆ Fs(t), ˆ Fs(u)) = d var( ˆ Fs(t)) + P tl≤t P t<ti≤u d cov(ˆ θ(tl), ˆ θ(ti)). Here tl indicates an event reporting time prior to ti and u is an arbitrary time distinct from t. ˆ θ(ti) has variance d var(b θ(ti)) = (b θ(ti))2 × ((ri −dsi)/(dsiri) + P tl<ti(dl/(rl(rl −dl)))) and covariance d cov(ˆ θ(ti), ˆ θ(tl)) = ˆ θ(ti)ˆ θ(tl)(−(1/ri) + P tl<ti(dl/(rl(rl −dl))))10, where dl = P ds,l is the reported count of events of any subtype at tl, ds,l is analogous to ds,i, and rl is the count of beneﬁciaries at risk at tl. We can now deﬁne the estimator of µs(τ) based on prior literature10, as this is a component of the novel estimators we develop next: ˆ µs(τ) = P ti<τ(ti+1 −ti) ˆ Fs(ti). This estimator has variance d var(ˆ µs(τ)) = P ti<τ(ti+1 −ti)2d var( ˆ Fs(ti)) + 2 P ti<τ P tl<ti(ti+1 −ti)(tl+1 − tl)d cov( ˆ Fs(ti), ˆ Fs(tl)). 2.3.1. Underreporting in comparison group For each reported reference event s∗in g = 0 we ﬁrst compute persistence, or the proportion of individuals reported as having s∗in monitoring period m who where also reported as having s∗in monitoring period m −1. This is then averaged across all reference events to obtain ˆ ϵ = 1 −1 h P ˆ qs∗,m. 2.3.2. Diﬀerence in mean time without event across groups We propose that the diﬀerence", "quality": {"lang": "en", "gibberish_score": 0.0357, "char_len": 1822, "word_len": 350}, "dedupe": {"exact_hash": "9fc11cc8f99daa273e4fdb74337673cbb0f4d7b488077f2d400df28ff608c1dd", "simhash64": 11240239202221008278}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8a79c08f14af7733acaeb49debdacb47aecf6f9048865a3f16430c9df5b723a5", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 11, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "as having s∗in monitoring period m who where also reported as having s∗in monitoring period m −1. This is then averaged across all reference events to obtain ˆ ϵ = 1 −1 h P ˆ qs∗,m. 2.3.2. Diﬀerence in mean time without event across groups We propose that the diﬀerence in mean time without an event, ψ, is estimated across groups as b ψ = ˆ µs,g=1(τ) −ˆ µs,g=0(τ) with variance d var( b ψ) = d var(ˆ µs,g=1(τ)) + d var(ˆ µs,g=0(τ)), as groups are assumed to be independent. To adjust for underreporting, ˆ µs,g=0(τ) = P ti<τ(ti+1−ti) ˆ Fs(ti) is modiﬁed to ˆ µs,g=0(τ) = P ti<τ(ti+1 −ti)( ˆ Fs(ti) + ϵ) in the estimator for b ψ, which does not change the variance. The underreporting-adjusted estimator is denoted ˆ ψ∗. Across sequential monitoring periods, ψM is estimated as ˆ ψM = ˆ ψm −ˆ ψm−1 with variance d var( b ψM) = d var( b ψm) + d var( b ψm−1). Although the components of this estimator come from the same ﬁxed population, the correlation between component estimates is assumed to be negligible because we are comparing diﬀerences in RMTL across independent groups estimated in disjoint time intervals. Therefore, covariance is zero. When an underreporting estimate is known for both monitoring periods, this estimator becomes ˆ ψ∗ M = ˆ ψ∗ m −ˆ ψ∗ m−1, and variance remains unchanged. 2.3.3. Possible severity-based upcoding Our estimator of possible severity-based upcoding across groups, ω, is given by: b ω = b ωg=1 −b ωg=0, where each b ωg = ˆ µs=k(τ) −ˆ µs=1(τ) within one monitoring period. The variance is d var(b ω) = d var(b ωg=1) + d var(b ωg=0), where d var(c ωg) = d var(ˆ µs=k(τ)) + d var(ˆ µs=1(τ)) − 2d cov(ˆ µs=k(τ), ˆ µs=1(τ)) and d cov(ˆ µs=k(τ), ˆ µs=1(τ)) = P i P j d cov( ˆ Fk(ti−1), ˆ F1(tj−1)) for distinct event times ti and tj. Here, ti corresponds to event reporting increments in the cumulative incidence function for severity level k, while tj corresponds to the equivalent for severity level 1.", "quality": {"lang": "en", "gibberish_score": 0.032, "char_len": 1937, "word_len": 350}, "dedupe": {"exact_hash": "363824fd6a194029f07b5a75709887da9ee20a575cb1db59da0b1a4a0e67e039", "simhash64": 4953144647775781318}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4628f787632ffdb851c45ff27b12144a4fb2db57f1a8bbcdddee7e2d1391a04d", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 12, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "and d cov(ˆ µs=k(τ), ˆ µs=1(τ)) = P i P j d cov( ˆ Fk(ti−1), ˆ F1(tj−1)) for distinct event times ti and tj. Here, ti corresponds to event reporting increments in the cumulative incidence function for severity level k, while tj corresponds to the equivalent for severity level 1. Although event reporting times are the same, we write these using separate variables because covariance is estimated between all pairs of increments for these two severity levels. 7 3. upcoding R package 3.1. Challenges in identifying and estimating upcoding Upcoding in Medicare has been inconsistently examined in the medical and economics literature for several decades31. One reason for this is that it is diﬃcult to deﬁnitively identify upcoding, particularly given that researchers and policymakers typically only have access to national Medicare claims data without beneﬁciaries’ corresponding electronic health records or other data. There are also a number of barriers to the development of upcoding estimation approaches. First, gaining access to individual-level Medicare data is a time-consuming process that is inaccessible to many researchers. Second, there are limited national data resources describing co-occurring health conditions in older adults that are free of coding incentives42–44. Third, labeled upcoding data to evaluate estimators are not available to researchers. This also means that comparing proposed methods is challenging, as the data used to develop or evaluate such methods are often not able to be shared by researchers. 3.2. Package functionality We developed the open source upcoding R package (https://github.com/StanfordHPDS/up coding) to help address many of these issues. The package enables simulation of longitudinal coding data for a Medicare-eligible population as well as more reproducible evaluation of approaches for evaluating HCC coding. Features include: 1. Simulating a sample of individuals with realistic baseline HCCs. These cooccurring baseline diagnoses are both based on older Americans’ self-reported health conditions and free of upcoding and undercoding. 2. Upcoding baseline data to a speciﬁed level over multiple time points. Users have the option to upcode any HCC using either any-available or severity-based upcoding. This results in labeled upcoding data, which is a useful resource for many types of coding", "quality": {"lang": "en", "gibberish_score": 0.0051, "char_len": 2356, "word_len": 350}, "dedupe": {"exact_hash": "770f98868c8a230ba81bd8ebb83cd7bb1152ab5a6630f7a02c6ce2e52ff46713", "simhash64": 11222260262960988438}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:8bc10a8ee2fea5d40753f7405a8aa693251ebc58b233421006264194fc0871df", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 13, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "health conditions and free of upcoding and undercoding. 2. Upcoding baseline data to a speciﬁed level over multiple time points. Users have the option to upcode any HCC using either any-available or severity-based upcoding. This results in labeled upcoding data, which is a useful resource for many types of coding measurement and estimation. In addition, users have the option to vary loss to follow up at each time point, a common issue in billing claims data and cohorts of older adults47. 3. Undercoding baseline data to a speciﬁed level. Users have the option to specify an undercoding proportion, and that proportion of all existing diagnoses are removed from the overall dataset. This can help simulate data that is similar to TM, which may be of interest as a comparison group for analyses as well as the non-undercoded baseline data. We describe this functionality in further detail in the subsections below. A brief tutorial on speciﬁc package functions is available in the package’s Github repository. 3.2.1. Baseline data simulation To simulate realistic co-occurring HCCs not inﬂuenced by coding incentives, co-occurring self-reported health conditions from participants aged 65 years and older (i.e., Medicare eligible) were extracted from the National Institutes of Health’s All of Us study48. We used self-reported survey data to obtain co-occurring health conditions because other national datasets (e.g., Medicare billing claims) report diagnoses for billing purposes and therefore may be impacted by coding incentives42–44. The All of Us study was designed to enroll 8 a million participants across the United States, focusing especially on groups historically underrepresented in clinical and biomedical research48 and included questions that overlapped with many HCCs in the current version of CMS-HCC (Version 28, or V28). V28 HCCs (listed in the Supporting Information) were manually mapped to Systemized Nomenclature of Medicine-Clinical Terms (SNOMED-CT) concept identiﬁers for survey questions (using version 7 of All of Us data; see this project’s Github repository for mapping). Then, survey responses to all available SNOMED-CT concepts were queried from the All of Us database. Included surveys, available respondent sociodemographic characteristics, and coverage of V28 HCCs are described in the Supporting", "quality": {"lang": "en", "gibberish_score": 0.0017, "char_len": 2337, "word_len": 350}, "dedupe": {"exact_hash": "708d449651f1ee106cd87087e731931baee766ecbf3fc58fcebf6a7916c49b05", "simhash64": 11239944807982137734}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e751f1b5b5e6afa6ee746fae5a5714b7063940942d4ac9644597dbc504a34b3d", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 14, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "for survey questions (using version 7 of All of Us data; see this project’s Github repository for mapping). Then, survey responses to all available SNOMED-CT concepts were queried from the All of Us database. Included surveys, available respondent sociodemographic characteristics, and coverage of V28 HCCs are described in the Supporting Information. For each surveyed person, self-reported co-occurring V28 HCCs were extracted. This was then summarized into a table of unique co-occurring HCC sets and a count of the number of All of Us survey respondents that reported having these diagnoses. In line with the All of Us Data and Statistics Dissemination Policy, only sets of co-occurring HCCs with more than 21 respondents were exported from All of Us’s platform (the Researcher Workbench) to be used in analyses. This both omits all sets of 20 or fewer respondents and also excludes several sets of co-occurring HCCs with more than 20 respondents. Ultimately, baseline data were simulated by sampling with replacement from these sets of co-occurring HCCs, where the respondent count is used to weight sampling. Users also have the option to use alternate sets of co-occurring V28 HCCs for baseline data sampling if they prefer. 3.2.2. Upcoding and loss to follow up simulation The package enables users to simulate two types of upcoding realistic to MA: any-available or severity-based, although the latter can only occur if an HCC has competing events. This upcoding is randomly split across a user-speciﬁed number of time points. Even though there is inherent right censoring in upcoded data (because only a proportion of all simulated individuals will be upcoded or coded at all), the package provides users with the option to include additional right censoring. This is meant to be representative of loss to follow up or death, both of which are known issues in following a population aged 65 years and older over time47. Users specify the proportion of loss to follow up they want to include, and a randomly selected set of rows (e.g., individuals) are right censored over each time point. Once someone is lost to follow up, they cannot be coded for", "quality": {"lang": "en", "gibberish_score": 0.0009, "char_len": 2158, "word_len": 350}, "dedupe": {"exact_hash": "baeaf6737b54fd88abb83128b466464f9f8b379fe27e4bf0f354ebbd2d9229f8", "simhash64": 11231491489848985222}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f3650d3463a79514788732451e58faa905c644a294d59a2d11d7d438f872d0a1", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 15, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "a population aged 65 years and older over time47. Users specify the proportion of loss to follow up they want to include, and a randomly selected set of rows (e.g., individuals) are right censored over each time point. Once someone is lost to follow up, they cannot be coded for any HCC in subsequent time periods. 3.2.3. Undercoding simulation The package also separately enables undercoding. Here, users specify a proportion of all coded diagnoses (from simulated baseline data) to randomly remove across the entire data set. This occurs on a dataset-wide level because undercoding is a systemic issue in TM28,33 and so we do not assume that it impacts speciﬁc V28 HCCs disproportionately. 4. Simulation study Our simulation study demonstrates how the estimators we propose can be used to monitor Medicare coding behaviors. To do this, diﬀerent upcoding and underreporting scenarios for the estimator introduced in Section 2.3.2 were compared to an estimator currently used by policymakers, deﬁned later in Section 4.1. Degrees of upcoding and undercoding were 9 constructed to align with current estimates of these issues in MA and TM from the literature. In addition, the temporal structure was implemented to be analogous to quarterly reporting for the length of time (around two years) that a given risk adjustment formula version is typically in place. Each individual scenario was replicated 1000 times. In each replicate, two sets of 1,000,000 observations of baseline data were independently simulated using our upcoding R package. For each dataset, the columns are the V28 HCCs (listed in the Supporting Information). Both any-available and severity-based upcoding were implemented in the ﬁrst baseline data set (the MA-like data). For the former type of upcoding, an HCC without any competing HCCs was upcoded. As an illustration, we use HCC238 (Speciﬁed Heart Arrhythmias). For the latter, an HCC that has lower severity HCCs was upcoded, speciﬁcally HCC125 (Dementia, Severe). HCC125’s less severe HCCs are HCC126 (Dementia, Moderate) and HCC127 (Dementia, Mild or Unspeciﬁed). The scenarios implemented were as follows: Scenario 1: Upcoding of an HCC that lacks competing events (HCC238) with varying underreporting in the comparison group.", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2255, "word_len": 350}, "dedupe": {"exact_hash": "989cfb6d16b4e75d05fac8f270a04a4ae5d436a9c6ecf081f3c31d14efb33026", "simhash64": 11212923210210050310}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a26f379051a138969483aad5031b8e67da518eb77815e6a4e4a5a4439eb59d20", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 16, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "HCC that has lower severity HCCs was upcoded, speciﬁcally HCC125 (Dementia, Severe). HCC125’s less severe HCCs are HCC126 (Dementia, Moderate) and HCC127 (Dementia, Mild or Unspeciﬁed). The scenarios implemented were as follows: Scenario 1: Upcoding of an HCC that lacks competing events (HCC238) with varying underreporting in the comparison group. Any observation not previously coded with HCC238 was eligible to be upcoded. Baseline MA data were separately upcoded to varying degrees (20%, 25%, 30%) sequentially within each monitoring period. The comparison group (i.e., TM data) was simulated by ﬁrst undercoding the baseline TM data to varying levels (0%, 5%, 10%, 15%) and then upcoding HCC238 analogously but to a lower amount (5%) per monitoring period. Scenario 2: Upcoding of an HCC with lower-severity competing events (HCC125) with varying underreporting in the comparison group. Only observations previously coded with the lower severity HCCs (HCC126 or HCC127) were eligible to be upcoded. Baseline MA data were separately upcoded to varying degrees (20%, 25%, 30%) sequentially within each monitoring period. The comparison group was simulated by ﬁrst undercoding the baseline TM data to varying levels (0%, 5%, 10%, 15%) and then upcoding HCC125 analogously but to a lower amount (5%) per monitoring period. 4.1. Comparator estimator We compared our estimators to a coding intensity estimator similar to the Demographic Estimate of Coding Intensity (DECI), which is widely used by Medicare policymakers34,49. DECI assumes complete data (e.g., no censoring) and has the following formula: DECI = National average MA CMS-HCC risk score National average TM CMS-HCC risk score National average MA demographic-only CMS-HCC risk score National average TM demographic-only CMS-HCC risk score . Here, the numerator is a risk score estimated with all HCCs included in the CMS-HCC formula, while the demographic-only risk adjustment risk score is estimated using only the demographic variables in that formula. Importantly, this also means that the estimand targeted by DECI is diﬀerent from that of our proposed estimators. Further, as we did not have CMS-HCC risk score coeﬃcients or demographic variables available in our simulated 10 data, our comparator estimator was more precisely DECI-like and we deﬁned it", "quality": {"lang": "en", "gibberish_score": 0.0078, "char_len": 2321, "word_len": 350}, "dedupe": {"exact_hash": "baa75f2d764f3540e400a784130a833b79008b92d1d43261acb0c826efbd9729", "simhash64": 11222207486403904406}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d247632446827e135968ad0ddb835c3a93c3bbd67ab7e99c74b475d9cfd7105e", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 17, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "that formula. Importantly, this also means that the estimand targeted by DECI is diﬀerent from that of our proposed estimators. Further, as we did not have CMS-HCC risk score coeﬃcients or demographic variables available in our simulated 10 data, our comparator estimator was more precisely DECI-like and we deﬁned it as: DECI† = Average count of MA CMS-HCC HCCs Average count of TM CMS-HCC HCCs. Our DECI† comparator is estimated in the same simulated data described earlier in this section, but relies on counts of all HCCs at the end of each monitoring period rather than time to incident coding of individual HCCs. Although DECI has a diﬀerent target estimand than our estimators and ignores censoring, comparing our simulation results to the DECI-like estimator DECI† is useful as it illustrates how our estimators can be a complement to existing practice. 4.2. Simulation results 4.2.1. Proposed estimators Cumulative incidence of coding for scenario 1 given 20% any-available upcoding in MA and 5% any-available upcoding in TM after all four degrees of undercoding is shown in Figure 1. As expected, within each monitoring period the cumulative incidence of the upcoded group was higher than the comparison group, which was only upcoded 5%. We also saw that the impact of undercoding on cumulative incidence estimates was limited. Given that undercoding occurs across the entire set of 115 HCCs, it was less likely to notably inﬂuence estimates for any single HCC. Lastly, we observed that the gaps between upcoded and comparison groups become smaller in sequential monitoring periods, which is a consequence of the ﬁxed sample we imposed across all monitoring periods. As monitoring periods increased, the number of individuals available to upcode decreased. Results plots for additional degrees of upcoding (25%, 30%) and Scenario 2 upcoding (severity-based upcoding for HCC125) are in the Supporting Information. 11 Figure 1: Cumulative incidence functions for the Speciﬁed Heart Arrhythmias Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups: 20% any-available MA upcoding. Speciﬁed heart arrhythmias corresponds to HCC238, which does not have any competing events. For this HCC, 20% of any-available", "quality": {"lang": "en", "gibberish_score": 0.0048, "char_len": 2272, "word_len": 350}, "dedupe": {"exact_hash": "8f715b9a80836652c54151a9c31ec6c443542b87f6cea242834656fb14792999", "simhash64": 11239944810121758998}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4e0aa13686ca5cb176f7049dd995b9accf4587ecba4829dcf25af9d96e2bb43a", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 18, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "Supporting Information. 11 Figure 1: Cumulative incidence functions for the Speciﬁed Heart Arrhythmias Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups: 20% any-available MA upcoding. Speciﬁed heart arrhythmias corresponds to HCC238, which does not have any competing events. For this HCC, 20% of any-available individuals in the MA group are upcoded and 5% of any-available individuals in the TM comparison group are upcoded. The ﬁrst monitoring period is labeled M1, and the second monitoring period is labeled M2. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. Estimates corresponding to the Section 2.3.2 estimators for upcoding of HCC238 are presented in Figure 2, which examines the diﬀerence in time without reported incident HCC238 coding between MA and TM within each monitoring period. Examining the ﬁrst monitoring period (M1), estimates clearly increase with the degree of upcoding. Similarly, for monitoring period 2 (M2) alone, estimates also increase with the degree of upcoding. In addition, undercoding has a limited eﬀect on estimates. However, researchers also have the option to adjust for underreporting using the estimator proposed in Section 2.3.1. As the sample is ﬁxed across monitoring periods, estimates across monitoring periods within a degree of upcoding and undercoding decrease, analogously to Figure 1. The gap between M1 and M2 increases with degree of upcoding as well, because more aggressive incident upcoding in M1 limits the availability of individuals for incident upcoding in M2. This suggests that within a monitoring period, researchers could compare estimates of diﬀerent HCCs to obtain a ranked list of HCCs to study further, where HCCs with the highest estimates have the most incident coding. In addition, in this scenario, being in TM appeared to have a “protective eﬀect” against being coded with HCC238, although 12 supplemental analyses would be needed to account for possible confounding and other issues. Additional results plots for severity-based upcoding of HCC125 are available in the Supporting Information. Figure 2: Within-monitoring period period ψ estimates for the Speciﬁed Heart Arrhythmias Hierarchical Condition Category (HCC) in simulated Medicare", "quality": {"lang": "en", "gibberish_score": 0.0021, "char_len": 2363, "word_len": 350}, "dedupe": {"exact_hash": "4262f0cdf1d52f31a5309207f9ced1257609c44a7a3e1438b61ae4d7a171659f", "simhash64": 11230932935796949446}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:3d95afecca7f0bf1ba84fed1dd84575f83caa30a771eb04ebde3a31ef9b4ee22", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 19, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "with HCC238, although 12 supplemental analyses would be needed to account for possible confounding and other issues. Additional results plots for severity-based upcoding of HCC125 are available in the Supporting Information. Figure 2: Within-monitoring period period ψ estimates for the Speciﬁed Heart Arrhythmias Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups. Speciﬁed heart arrhythmias corresponds to HCC238, which does not have any competing events. For this HCC, any-available individuals in the MA group are upcoded to varying degrees and any-available individuals in the TM comparison group are upcoded 5%. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 4.2.2. Comparator estimator Figure 3 shows the average DECI† estimate across simulation replicates by monitoring period as well as upcoding and undercoding degree. As degree of upcoding increases, these estimates increase negligibly both for individual monitoring periods and across sequential monitoring periods. This is intuitive–only two of the 115 HCCs in the MA risk adjustment algorithm are being upcoded–but it suggests a limitation of DECI in identifying upcoding occurring in a minority of HCCs. Regardless of upcoding degree, the DECI† estimate increasing in line with the degree of TM undercoding indicates that it may be more sensitive to undercoding in TM. Especially since we know undercoding is prevalent in TM28,33, this suggests that DECI may be misrepresenting the overall spending gap between MA and TM due to its undercoding sensitivity. 13 Figure 3: DECI† estimate across all Medicare Advantage (MA) Version 28 Hierarchical Condition Categories (HCCs) at varying degrees of upcoding and underreporting in simulated MA and Traditional Medicare (TM) groups. Three separate degrees of MA group upcoding occur sequentially over each monitoring period in HCC238 (any-available) and HCC125 (lower severity) only. The TM group is upcoded 5% sequentially for the same HCCs over equivalent periods. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 5. Discussion We proposed a set of estimators that extend RMTL methodology for evaluating time to incident coding by", "quality": {"lang": "en", "gibberish_score": 0.003, "char_len": 2372, "word_len": 350}, "dedupe": {"exact_hash": "0fb2fdc59099eb95597774362496103f623470e433e615fbbc7032af9ecf42a9", "simhash64": 11230950530128118230}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:96d203cb6e547f72bf623eab047307554aa7f294ddb5e6a1a5e991fab49c8767", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 20, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "is upcoded 5% sequentially for the same HCCs over equivalent periods. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 5. Discussion We proposed a set of estimators that extend RMTL methodology for evaluating time to incident coding by private insurers in Medicare. Given the timeline of risk adjustment formula updates, our approach realistically presupposes that reported coding is examined over multiple monitoring periods, each of which has several time points where new events are reported. Novel estimators were introduced to evaluate diﬀerences in time-to-reporting both within monitoring periods and across monitoring periods, including an estimator for severity-based upcoding. Our approach also included an adjustment for possible underreporting, which is a known major issue in TM data28,33. Finally, we developed an open source R package that enables users to simulate co-occurring HCCs free of coding incentives and similar to those reported by individuals residing in the United States eligible for Medicare. Users can also undercode or upcode this data over time. Our simulated data were upcoded to degrees aligned to those reported in literature32, and we found in our simulation results that our estimators were able to recover diﬀerences in upcoding both within and across monitoring periods while showing limited sensitivity to undercoding. DECI-like estimates of the same data were a useful complement, but were 14 limited in that these comparator estimates were very sensitive to undercoding and did not vary when upcoding occurred in a minority of HCCs. Therefore, our estimators show considerable promise as tools to help evaluate reported coding patterns over time. These estimators can be used as a ﬁrst step to identify potentially upcoded HCCs while making more realistic assumptions than DECI. Follow up analyses could include examining coding patterns within speciﬁc insurers, providers, or beneﬁciaries. This work has a number of limitations and areas for future development. First, several assumptions could be further relaxed. This includes our assumptions that the population is ﬁxed across both comparison groups and monitoring periods and that estimates in sequential monitoring periods across groups are approximately independent. Upcoding estimators", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2370, "word_len": 350}, "dedupe": {"exact_hash": "8540ef76c548b3b6e7c0d5139474b7aded0a66e09044ce7b3f2175a99e410cae", "simhash64": 11231553335240001922}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a7cd5f46c85cc34b1cea5b1e6682958f050e2014ae665163e7d7b75db373114a", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 21, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "beneﬁciaries. This work has a number of limitations and areas for future development. First, several assumptions could be further relaxed. This includes our assumptions that the population is ﬁxed across both comparison groups and monitoring periods and that estimates in sequential monitoring periods across groups are approximately independent. Upcoding estimators could be expanded to additional types of upcoding and to correct for underreporting. Covariates could also be incorporated into estimation, which would be useful for addressing issues like confounding. There is also more heterogeneity and missingness in real-world claims data than was included in our simulations. Self-reported diagnoses–as we use in our simulated baseline data–also have recall bias and other issues that we do not address here. Since the MA program’s inception, high intensity of private insurer coding, including possible upcoding, is estimated to have cost the federal government and taxpayers $224 billion dollars without clear beneﬁt to beneﬁciaries32. Thus, estimating diﬀerences in coding between MA and TM or more and less severe competing HCCs has the potential to help policymakers locate issues with speciﬁc HCCs earlier and at scale. This could suggest areas for improvement in the risk adjustment formula and potentially save signiﬁcant funds in the Medicare program. Our work has provided both novel estimators and novel simulated data tailored to addressing these important policy considerations. 6. Acknowledgments This research was funded by National Institutes of Health Director’s Pioneer Award DP1LM014278 and a Stanford Interdisciplinary Graduate Fellowship. We thank Malcolm Barrett, Gabriela Basel, Lizzie Kumar, and Neha Srivathsa for their valuable insights and contributions to code performance and review. We gratefully acknowledge All of Us participants for their contributions, without whom this research would not have been possible. We also thank the National Institutes of Health’s All of Us Research Program (https://allofus.nih.gov/) for making available the participant survey data included in this study. We thank Stanford University and Stanford Research Computing for providing additional computational resources, including support for computing performed on the Sherlock cluster. 7. Data availability statement The simulated data for this study can be generated using code in the project repository: https://github.com/StanfordHPDS/tte_estimation_medicare. Additional upcoding, undercoding,", "quality": {"lang": "en", "gibberish_score": 0.006, "char_len": 2515, "word_len": 350}, "dedupe": {"exact_hash": "6870b815d963331ea60b10c72e3cb6c94ab7cef48641f9b4244a2fa3f339ecfa", "simhash64": 11234310979078946966}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:5ba017a15607a883282220504d7e152b3066f07b5f7d1bc50ae01de80fba69fa", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 22, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "data included in this study. We thank Stanford University and Stanford Research Computing for providing additional computational resources, including support for computing performed on the Sherlock cluster. 7. Data availability statement The simulated data for this study can be generated using code in the project repository: https://github.com/StanfordHPDS/tte_estimation_medicare. Additional upcoding, undercoding, and baseline data can be simulated using the upcoding package (https://github .com/StanfordHPDS/upcoding). In line with program policies, non-summarized version 7 All of Us survey data used to derive baseline co-occurring HCCs can be accessed by authorized users via the All of Us Researcher Workbench only (https://www.researchallofus.org/data15 tools/workbench/) and requires Registered Tier access. Code that was used for this project in the Researcher Workbench is in the above project repository. 8. Supporting Information Additional study summary information and results can be found online in the Supporting Information. 16 References 1. Hernán MA. The hazards of hazard ratios. Epidemiology. 2010;21(1):13-15. 2. Chappell R, Zhu X. Describing diﬀerences in survival curves. JAMA Oncol. 2016;2(7):906-907. 3. Uno H, Horiguchi M, Hassett MJ. Statistical test/estimation methods used in contemporary phase III cancer randomized controlled trials with time-to-event outcomes. Oncologist. 2020;25(2):91-93. 4. Jachno K, Heritier S, Wolfe R. Are non-constant rates and non-proportional treatment eﬀects accounted for in the design and analysis of randomised controlled trials? A review of current practice. BMC Med Res Methodol. 2019;19(1):103. 5. Trinquart L, Jacot J, Conner SC, Porcher R. Comparison of treatment eﬀects measured by the hazard ratio and by the ratio of restricted mean survival times in oncology randomized controlled trials. J Clin Oncol. 2016;34(15):1813-1819. 6. Tian L, Zhao L, Wei LJ. Predicting the restricted mean event time with the subject’s baseline covariates in survival analysis. Biostatistics. 2014;15(2):222-233. 7. Tian L, Jin H, Uno H, et al. On the empirical choice of the time window for restricted mean survival time. Biometrics. 2020;76(4):1157-1166. 8. Austin PC, Fine JP. Accounting for competing risks in randomized controlled trials: A review and recommendations for improvement. Stat Med. 2017;36(8):1203-1209. 9. Geskus RB. Data Analysis with Competing Risks and Intermediate States. 1st Edition. Chapman & Hall/CRC; 2020. 10. Conner SC, Trinquart L. Estimation and modeling", "quality": {"lang": "en", "gibberish_score": 0.0079, "char_len": 2540, "word_len": 350}, "dedupe": {"exact_hash": "fa547dad3ced56a89a0b73b0b02d6f930025548d3edd87042d092f4f06a1b970", "simhash64": 11239944876702139846}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:2605d71467306a690e0f8c60a70a85611589da7d6bf8689660adca6cb8a12aa2", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 23, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "time. Biometrics. 2020;76(4):1157-1166. 8. Austin PC, Fine JP. Accounting for competing risks in randomized controlled trials: A review and recommendations for improvement. Stat Med. 2017;36(8):1203-1209. 9. Geskus RB. Data Analysis with Competing Risks and Intermediate States. 1st Edition. Chapman & Hall/CRC; 2020. 10. Conner SC, Trinquart L. Estimation and modeling of the restricted mean time lost in the presence of competing risks. Stat Med. 2021;40(9):2177-2196. 11. Irwin JO. The standard error of an estimate of expectation of life, with special reference to expectation of tumourless life in experiments with mice. J Hyg (Lond). 1949;47(2):188. 12. Royston P, Parmar MKB. The use of restricted mean survival time to estimate the treatment eﬀect in randomized clinical trials when the proportional hazards assumption is in doubt. Stat Med. 2011;30(19):2409-2421. 13. Uno H, Claggett B, Tian L, et al. Moving beyond the hazard ratio in quantifying the between-group diﬀerence in survival analysis. J Clin Oncol. 2014;32(22):2380-2385. 17 14. Zhao L, Claggett B, Tian L, et al. On the restricted mean survival time curve in survival analysis: On the restricted mean survival time curve in survival analysis. Biometrics. 2016;72(1):215-221. 15. Lu Y, Tian L. Statistical considerations for sequential analysis of the restricted mean survival time for randomized clinical trials. Stat Biopharm Res. 2021;13(2):210-218. 16. Zhang P, Logan B, Martens M. Covariate-adjusted group sequential comparisons of restricted mean survival times. arXiv [statME]. Published online September 2025. 17. Paukner M, Chappell R. Window mean survival time. Stat Med. 2021;40(25):55215533. 18. Sun J, Schaubel DE, Tchetgen EJT. Beyond ﬁxed restriction time: Adaptive restricted mean survival time methods in clinical trials. arXiv [statME]. Published online January 2025. 19. Horiguchi M, Uno H. On permutation tests for comparing restricted mean survival time with small sample from randomized trials. Stat Med. 2020;39(20):2655-2670. 20. Ditzhaus M, Yu M, Xu J. Studentized permutation method for comparing two restricted mean survival times with small sample from randomized trials. Stat Med. 2023;42(13):2226-2240. 21. Munko M, Ditzhaus M, Dobler D, Genuneit J. RMST-based multiple contrast tests in general factorial designs. Stat Med. 2024;43(10):1849-1866. 22. Jesse D, Huber C, Friede T. Comparing restricted mean survival times in", "quality": {"lang": "en", "gibberish_score": 0.0008, "char_len": 2425, "word_len": 350}, "dedupe": {"exact_hash": "873be4994da140443df66b93ce4a4978dc96f352d8fb3b346cc70196c357c570", "simhash64": 11239875266036172226}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:bff9d82d486b5bd4f887f929612f7becb0b9dd8a5bebd4760934fc215dae912f", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 24, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "method for comparing two restricted mean survival times with small sample from randomized trials. Stat Med. 2023;42(13):2226-2240. 21. Munko M, Ditzhaus M, Dobler D, Genuneit J. RMST-based multiple contrast tests in general factorial designs. Stat Med. 2024;43(10):1849-1866. 22. Jesse D, Huber C, Friede T. Comparing restricted mean survival times in small sample clinical trials using pseudo-observations. arXiv [statME]. Published online August 2024. 23. Andersen PK. Decomposition of number of life years lost according to causes of death. Stat Med. 2013;32(30):5278-5285. 24. Calkins KL, Canan CE, Moore RD, Lesko CR, Lau B. An application of restricted mean survival time in a competing risks setting: Comparing time to ART initiation by injection drug use. BMC Med Res Methodol. 2018;18(1):27. 25. Wu H, Yuan H, Yang Z, Hou Y, Chen Z. Implementation of an alternative method for assessing competing risks: Restricted mean time lost. Am J Epidemiol. 2022;191(1):163172. 26. Ochieng N, Cubanski J, Neuman T. A snapshot of sources of coverage among medicare beneﬁciaries. Published online December 2025. 18 27. Pope GC, Kautter J, Ellis RP, et al. Risk adjustment of medicare capitation payments using the CMS-HCC model. Health Care Financ Rev. 2004;25(4):119-141. 28. Ellis RP, Martins B, Rose S. Chapter 3 - risk adjustment for health plan payment. In: McGuire TG, Kleef RC van, eds. Risk Adjustment, Risk Sharing and Premium Regulation in Health Insurance Markets. Academic Press; 2018:55-104. 29. Centers for Medicare and Medicaid Services. 2024 MA Advance Notice.; 2023. 30. Geruso M, Layton T. Upcoding: Evidence from Medicare on Squishy Risk Adjustment. Journal of Political Economy. 2020;128(3):984-1026. doi:10.1086/704756 31. Joiner KA, Lin J, Pantano J. Upcoding in medicare: where does it matter most? Health Economics Review. 2024;14(1). doi:10.1186/s13561-023-00465-4 32. Medicare Payment Advisory Commission. March 2025 report to the congress: Medicare payment policy. Published online March 2025. 33. Ghoshal-Datta N, Chernew ME, McWilliams JM. Lack of persistent coding in traditional medicare may widen the risk-score gap with medicare advantage. Health Aﬀ (Millwood). 2024;43(12):1638-1646. 34. Kronick R, Chua FM. Industry-Wide and Sponsor-Speciﬁc Estimates of Medicare Advantage Coding Intensity. SSRN Electronic Journal. Published online 2021. doi:10.2139/ssrn.3959446 35. Jacobs PD, Kronick R. Getting What", "quality": {"lang": "en", "gibberish_score": 0.0016, "char_len": 2433, "word_len": 350}, "dedupe": {"exact_hash": "34ae13d3a44c5fe28e077a02639538b17bc2e42de150a0a076bdf1c6dd11f35e", "simhash64": 11235666335525341590}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d7ba751a00f2744b8df8de25f06a6b3812143b2a901dcb193de33c1266d1fb3d", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 25, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "ME, McWilliams JM. Lack of persistent coding in traditional medicare may widen the risk-score gap with medicare advantage. Health Aﬀ (Millwood). 2024;43(12):1638-1646. 34. Kronick R, Chua FM. Industry-Wide and Sponsor-Speciﬁc Estimates of Medicare Advantage Coding Intensity. SSRN Electronic Journal. Published online 2021. doi:10.2139/ssrn.3959446 35. Jacobs PD, Kronick R. Getting What We Pay For: How Do Risk-Based Payments to Medicare Advantage Plans Compare with Alternative Measures of Beneﬁciary Health Risk? Health Services Research. 2018;53(6):4997-5015. doi:10.1111/1475-6773.12977 36. McGuire TG, Enache OM, Chernew M, McWilliams JM, Nham T, Rose S. Incidence, persistence, and steady-state prevalence in coding intensity for health plan payment. Health Serv Res. 2025;(e70065):e70065. 37. Abelson R, Sanger-Katz M. “The cash monster was insatiable”: How insurers exploited medicare for billions. The New York Times. Published online October 2022. 38. Justice Oﬃce of Public Aﬀairs USD of. Kaiser permanente aﬃliates pay $556M to resolve false claims act allegations. Published online January 2026. 39. Abelson R, Sanger-Katz M. New medicare rule aims to take back $4.7 billion from insurers. The New York Times. Published online January 2023. 19 40. Centers for Medicare & Medicaid Services. Medicare and medicaid programs; policy and technical changes to the medicare advantage, medicare prescription drug beneﬁt, program of all-inclusive care for the elderly (PACE), medicaid fee-for-service, and medicaid managed care programs for years 2020 and 2021. Federal Register. 2023;88:6643-6665. 41. Constantino AK. UnitedHealth says it is cooperating with DOJ investigations into medicare billing practices. Published online July 2025. 42. Rose S, Zaslavsky AM, McWilliams JM. Variation In Accountable Care Organization Spending And Sensitivity To Risk Adjustment: Implications For Benchmarking. Health Aﬀairs. 2016;35(3):440-448. doi:10.1377/hlthaﬀ.2015.1026 43. Chernew ME, Carichner J, Impreso J, et al. Coding-Driven Changes In Measured Risk In Accountable Care Organizations. Health Aﬀairs. 2021;40(12):1909-1917. doi:10.1377/hlthaﬀ.2021.00361 44. McWilliams JM, Weinreb G, Landrum MB, Chernew ME. Use of patient health survey data for risk adjustment to limit distortionary coding incentives in medicare: Article examines use of patient health survey data for risk adjustment to limit distortionary coding incentives in medicare. Health Aﬀ(Millwood). 2025;44(1):48-57. 45. Westreich D. Epidemiology by Design. Oxford University PressNew York; 2019. doi:10.1093/oso/9780190665760.001.0001 46. Kaplan EL, Meier P.", "quality": {"lang": "en", "gibberish_score": 0.0046, "char_len": 2626, "word_len": 350}, "dedupe": {"exact_hash": "96ed0bb67ee0b2e6888985d2d9381680e7ee5ab2fb24968fd6fc3240f9f516f3", "simhash64": 11226376559634778262}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:45f5f77c6b6d12b02292157311697e26643371c104118d82182b446753263cef", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 26, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "survey data for risk adjustment to limit distortionary coding incentives in medicare: Article examines use of patient health survey data for risk adjustment to limit distortionary coding incentives in medicare. Health Aﬀ(Millwood). 2025;44(1):48-57. 45. Westreich D. Epidemiology by Design. Oxford University PressNew York; 2019. doi:10.1093/oso/9780190665760.001.0001 46. Kaplan EL, Meier P. Nonparametric estimation from incomplete observations. J Am Stat Assoc. 1958;53(282):457-481. 47. Barberio J, Naimi AI, Patzer RE, et al. Inﬂuence of incomplete death information on cumulative risk estimates in US claims data. Am J Epidemiol. 2024;193(9):1281-1290. 48. The All of Us Research Program Investigators. The “All of Us” Research Program. New England Journal of Medicine. 2019;381(7):668-676. doi:10.1056/nejmsr1809937 49. Medicare Payment Advisory Commission. Report to Congress: Medicare Payment Policy.; 2024. 20 Supporting Information Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment Oana M. Enachea,∗, Sherri Roseb aStanford University School of Medicine, Department of Biomedical Data Science, Edwards Building, 300 Pasteur Drive, Stanford, CA, 94304 bStanford University, Department of Health Policy, Encina Commons, 615 Crothers Way, Stanford, CA, 94305 1. Additional information about simulated data Table 1: Hierarchical Condition Categories (HCCs) included in the Medicare Advantage risk adjustment algorithm version 28 (V28). Each of the HCCs in this table are described as written in the Centers for Medicare and Medicaid Services’ (CMS) 2024 Medicare Advantage Advance Notice1. Competing events, if applicable, are also noted from the 2024 CMS Risk Adjustment Model Software and ICD-10 Mappings, speciﬁcally the Midyear/Final Model Software, which is in the SAS language2. If a set of competing events is not N/A, then, if the HCC in that row is coded, none of the competing event HCCs can also be coded for billing purposes. The hierarchy described in the “Competing Events” column can also be found in the upcoding R package. HCC HCC Description Competing Events HCC1 HIV/AIDS N/A HCC2 Septicemia, sepsis, systemic inﬂammatory response syndrome/shock N/A HCC6 Opportunistic infections N/A HCC17 Cancer metastatic to lung, liver, brain, and other organs; acute myeloid leukemia except promyelocytic HCC18, HCC19, HCC20, HCC21, HCC22, HCC23 HCC18 Cancer metastatic to bone, other and unspeciﬁed metastatic cancer; acute leukemia except myeloid", "quality": {"lang": "en", "gibberish_score": 0.0064, "char_len": 2501, "word_len": 350}, "dedupe": {"exact_hash": "2ef9006a3bd2b745b439c7850ba241b94f9a572cc170b9ddff34112f3c1dd208", "simhash64": 11226380957665034630}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:00c23c6ee290070e4fd29be1cb19370c0667b1dd461696d2b71e9cde55e07a6f", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 27, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "HCC1 HIV/AIDS N/A HCC2 Septicemia, sepsis, systemic inﬂammatory response syndrome/shock N/A HCC6 Opportunistic infections N/A HCC17 Cancer metastatic to lung, liver, brain, and other organs; acute myeloid leukemia except promyelocytic HCC18, HCC19, HCC20, HCC21, HCC22, HCC23 HCC18 Cancer metastatic to bone, other and unspeciﬁed metastatic cancer; acute leukemia except myeloid HCC19, HCC20, HCC21, HCC22, HCC23 ∗Corresponding author Email address: oenache@stanford.edu (Oana M. Enache) HCC HCC Description Competing Events HCC19 Myelodysplastic syndromes, multiple myeloma, and other cancers HCC20, HCC21, HCC22, HCC23 HCC20 Lung and other severe cancers HCC21, HCC22, HCC23 HCC21 Lymphoma and other cancers HCC22, HCC23 HCC22 Bladder, colorectal, and other cancers HCC23 HCC23 Prostate, breast, and other cancers and tumors N/A HCC35 Pancreas transplant status HCC36, HCC37, HCC38 HCC36 Diabetes with severe acute complications HCC37, HCC38 HCC37 Diabetes with chronic complications HCC38 HCC38 Diabetes with glycemic, unspeciﬁed, or no complications N/A HCC48 Morbid obesity N/A HCC49 Speciﬁed lysosomal storage disorders N/A HCC50 Amyloidosis, porphyria, and other speciﬁed metabolic disorders N/A HCC51 Addison’s and Cushing’s diseases, acromegaly, and other speciﬁed endocrine disorders N/A HCC62 Liver transplant status/complications HCC63, HCC64, HCC65, HCC68 HCC63 Chronic liver failure/end-stage liver disorders HCC64, HCC65, HCC68, HCC202 HCC64 Cirrhosis of liver HCC65, HCC68 HCC65 Chronic hepatitis N/A HCC68 Cholangitis and obstruction of bile duct without gallstones N/A HCC77 Intestine transplant status/complications HCC78, HCC80, HCC81 HCC78 Intestinal obstruction/perforation N/A HCC79 Chronic pancreatitis N/A 2 HCC HCC Description Competing Events HCC80 Crohn’s disease (regional enteritis) HCC81 HCC81 Ulcerative colitis N/A HCC92 Bone/joint/muscle/severe soft tissue infections/necrosis N/A HCC93 Rheumatoid arthritis and other speciﬁed inﬂammatory rheumatic disorders HCC94 HCC94 Systemic lupus erythematosus and other speciﬁed systemic connective tissue disorders N/A HCC107 Sickle cell anemia (Hb-SS) and thalassemia beta zero HCC108 HCC108 Sickle cell disorders, except sickle cell anemia (Hb-SS) and thalassemia beta zero; beta thalassemia major N/A HCC109 Acquired hemolytic, aplastic, and sideroblastic anemias N/A HCC111 Hemophilia, male HCC112 HCC112 Immune thrombocytopenia and speciﬁed coagulation defects and hemorrhagic conditions N/A HCC114 Common variable and combined immunodeﬁciencies HCC115 HCC115 Speciﬁed immunodeﬁciencies and white blood cell disorders N/A HCC125 Dementia, severe HCC126, HCC127 HCC126 Dementia, moderate HCC127 HCC127 Dementia, mild or unspeciﬁed", "quality": {"lang": "en", "gibberish_score": 0.0129, "char_len": 2709, "word_len": 350}, "dedupe": {"exact_hash": "79fd019dec4f17eb068c93c83f5e029d2841f2df305efd6a19af50a5471dd316", "simhash64": 15257942660087785494}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:7cb128fc49cde2bca61165307ac68cd11bf9b1bc5eacd87f009ccecb2be83d1a", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 28, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "aplastic, and sideroblastic anemias N/A HCC111 Hemophilia, male HCC112 HCC112 Immune thrombocytopenia and speciﬁed coagulation defects and hemorrhagic conditions N/A HCC114 Common variable and combined immunodeﬁciencies HCC115 HCC115 Speciﬁed immunodeﬁciencies and white blood cell disorders N/A HCC125 Dementia, severe HCC126, HCC127 HCC126 Dementia, moderate HCC127 HCC127 Dementia, mild or unspeciﬁed N/A HCC135 Drug use with psychotic complications HCC136, HCC137, HCC138, HCC139 HCC136 Alcohol use with psychotic complications HCC137, HCC138, HCC139 HCC137 Drug use disorder, moderate/severe, or drug use with non-psychotic complications HCC138, HCC139 3 HCC HCC Description Competing Events HCC138 Drug use disorder, mild, uncomplicated, except cannabis HCC139 HCC139 Alcohol use disorder, moderate/severe, or alcohol use with speciﬁed non-psychotic complications N/A HCC151 Schizophrenia HCC152, HCC153, HCC154, HCC155 HCC152 Psychosis, except schizophrenia HCC153, HCC154, HCC155 HCC153 Personality disorders; anorexia/bulimia nervosa HCC154, HCC155 HCC154 Bipolar disorders without psychosis HCC155 HCC155 Major depression, moderate or severe, without psychosis N/A HCC180 Quadriplegia HCC181, HCC182, HCC253, HCC254 HCC181 Paraplegia HCC182, HCC254 HCC182 Spinal cord disorders/injuries N/A HCC190 Amyotrophic lateral sclerosis and other motor neuron disease, spinal muscular atrophy N/A HCC191 Quadriplegic cerebral palsy HCC180, HCC181, HCC182, HCC192, HCC253, HCC254 HCC192 Cerebral palsy, except quadriplegic HCC180, HCC181, HCC182, HCC253, HCC254 HCC193 Chronic inﬂammatory demyelinating polyneuritis and multifocal motor neuropathy N/A HCC195 Myasthenia gravis with (acute) exacerbation HCC196 HCC196 Myasthenia gravis without (acute) exacerbation and other myoneural disorders N/A HCC197 Muscular dystrophy N/A HCC198 Multiple sclerosis N/A HCC199 Parkinson and other degenerative disease of basal ganglia N/A 4 HCC HCC Description Competing Events HCC200 Friedreich and other hereditary ataxias; Huntington disease N/A HCC201 Seizure disorders and convulsions N/A HCC202 Coma, brain compression/anoxic damage N/A HCC211 Respirator dependence/tracheostomy status/complications HCC212, HCC213 HCC212 Respiratory arrest HCC213 HCC213 Cardio-respiratory failure and shock N/A HCC221 Heart transplant status/complications HCC222, HCC223, HCC224, HCC225, HCC226, HCC227 HCC222 End stage heart failure HCC223, HCC224, HCC225, HCC226, HCC227 HCC223 Heart failure with heart assist device/artiﬁcial heart HCC224, HCC225, HCC226, HCC227 HCC224 Acute on chronic heart failure HCC225, HCC226, HCC227 HCC225 Acute heart failure (excludes acute on chronic) HCC226, HCC227 HCC226 Heart failure, except end stage and acute HCC227 HCC227 Cardiomyopathy/myocarditis N/A HCC228 Acute myocardial infarction", "quality": {"lang": "en", "gibberish_score": 0.01, "char_len": 2804, "word_len": 350}, "dedupe": {"exact_hash": "f06f1f205f4e365e23aca94ac91462ae9224a496145393d55264f2b5d72302c2", "simhash64": 4737095998261340214}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:f6ca71301a4747dd4cdf6550ac2718f3a39eb0fdcd429beb60d4dfa6534c243c", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 29, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "HCC225, HCC226, HCC227 HCC223 Heart failure with heart assist device/artiﬁcial heart HCC224, HCC225, HCC226, HCC227 HCC224 Acute on chronic heart failure HCC225, HCC226, HCC227 HCC225 Acute heart failure (excludes acute on chronic) HCC226, HCC227 HCC226 Heart failure, except end stage and acute HCC227 HCC227 Cardiomyopathy/myocarditis N/A HCC228 Acute myocardial infarction HCC229 HCC229 Unstable angina and other acute ischemic heart disease N/A HCC238 Speciﬁed heart arrhythmias N/A HCC248 Intracranial hemorrhage HCC249 HCC249 Ischemic or unspeciﬁed stroke N/A HCC253 Hemiplegia/hemiparesis HCC254 HCC254 Monoplegia, other paralytic syndromes N/A HCC263 Atherosclerosis of arteries of the extremities with ulceration or gangrene HCC264, HCC383, HCC409 HCC264 Vascular disease with complications N/A HCC267 Deep vein thrombosis and pulmonary embolism N/A 5 HCC HCC Description Competing Events HCC276 Lung transplant status/complications HCC277, HCC278, HCC279, HCC280 HCC277 Cystic ﬁbrosis HCC278, HCC279, HCC280 HCC278 Idiopathic pulmonary ﬁbrosis and lung involvement in systemic sclerosis HCC279, HCC280 HCC279 Severe persistent asthma HCC280 HCC280 Chronic obstructive pulmonary disease, interstitial lung disorders, and other chronic lung disorders N/A HCC282 Aspiration and speciﬁed bacterial pneumonias HCC283 HCC283 Empyema, lung abscess N/A HCC298 Severe diabetic eye disease, retinal vein occlusion, and vitreous hemorrhage N/A HCC300 Exudative macular degeneration N/A HCC326 Chronic kidney disease, stage 5 HCC327, HCC328, HCC329 HCC327 Chronic kidney disease, severe (stage 4) HCC328, HCC329 HCC328 Chronic kidney disease, moderate (stage 3B) HCC329 HCC329 Chronic kidney disease, moderate (stage 3, except 3B) N/A HCC379 Pressure ulcer of skin with necrosis through to muscle, tendon, or bone HCC380, HCC381, HCC382, HCC383 HCC380 Chronic ulcer of skin, except pressure, through to bone or muscle HCC381, HCC382, HCC383 HCC381 Pressure ulcer of skin with full thickness skin loss HCC382, HCC383 HCC382 Pressure ulcer of skin with partial thickness skin loss HCC383 HCC383 Chronic ulcer of skin, except pressure, not speciﬁed as through to bone or muscle N/A HCC385 Severe skin burn N/A 6 HCC HCC Description Competing Events HCC387 Pemphigus, pemphigoid, and other speciﬁed autoimmune skin disorders N/A HCC397 Major head injury with loss of consciousness (> 1 hour) HCC202, HCC398, HCC399 HCC398 Major head injury with loss of consciousness (< 1 hour or unspeciﬁed) HCC202, HCC399", "quality": {"lang": "en", "gibberish_score": 0.0084, "char_len": 2500, "word_len": 350}, "dedupe": {"exact_hash": "86e35d8f653697a0c8cb9890d118349f4b67a79f1664ff232fcf989da502b138", "simhash64": 4340708750152696102}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:e4ed27c92b843f9dec7b9f1cb91c2f8c84dd2543e02992cf9d47b12697eba707", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 30, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "HCC385 Severe skin burn N/A 6 HCC HCC Description Competing Events HCC387 Pemphigus, pemphigoid, and other speciﬁed autoimmune skin disorders N/A HCC397 Major head injury with loss of consciousness (> 1 hour) HCC202, HCC398, HCC399 HCC398 Major head injury with loss of consciousness (< 1 hour or unspeciﬁed) HCC202, HCC399 HCC399 Major head injury without loss of consciousness N/A HCC401 Vertebral fractures without spinal cord injury N/A HCC402 Hip fracture/dislocation N/A HCC405 Traumatic amputations and complications HCC409 HCC409 Amputation status, lower limb/amputation complications N/A HCC454 Stem cell, including bone marrow, transplant status/complications N/A HCC463 Artiﬁcial openings for feeding or elimination N/A 1.1. Summary statistics from All of Us survey data These data represent included survey respondents with at least one self-reported health condition that mapped to a Hierarchical Condition Category (HCC) in Version 28 of the Medicare Advantage risk adjustment algorithm. Only sets of co-occurring HCCs with more than 21 respondents (which also excludes several sets with more than 20 respondents) were used in any summary tables to comply with the All of Us Data and Statistics Dissemination Policy. Table 2: Included surveys from All of Us All of Us Survey Title Number of Unique Respondents Overall Health 156 Personal and Family Health History 15692 7 Table 3: Number of respondents by HCC with available survey questions Version 28 HCC Number of Respondents HCC1 546 HCC20 339 HCC21 5775 HCC22 260 HCC23 6772 HCC35 156 HCC38 2114 HCC51 4283 HCC62 156 HCC64 100 HCC65 546 HCC77 156 HCC78 511 HCC93 1131 HCC109 1018 HCC155 1467 HCC182 47 HCC221 156 HCC226 73 HCC228 399 HCC238 1635 HCC249 373 HCC264 62 HCC267 123 HCC276 156 HCC280 273 HCC300 587 HCC327 31 HCC328 110 HCC398 808 Table 4: Distribution of HCC-relevant conditions per person Number of HCC-Relevant Conditions Per Person Number of Respondents 1 5636 2 6657 3 3163 4 236 5 156 8 The following tables show respondent information using categories provided by All of Us. Not all respondents responded to all questions so counts may vary. The rows in each table correspond to available ﬁelds in All of Us. These characteristics", "quality": {"lang": "en", "gibberish_score": 0.0059, "char_len": 2218, "word_len": 350}, "dedupe": {"exact_hash": "a925aede217f74d8888d9c9201c7ebca7b29c7373f9e36154d81b17e9c105af9", "simhash64": 12951260783706998918}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:060c150c46698acfcf297661e81f0a02d31723c40d6a81089c8f17860870dd61", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 31, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "1 5636 2 6657 3 3163 4 236 5 156 8 The following tables show respondent information using categories provided by All of Us. Not all respondents responded to all questions so counts may vary. The rows in each table correspond to available ﬁelds in All of Us. These characteristics are not displayed jointly because that would result in sample sizes too small to comply with the All of Us Data and Statistics Dissemination Policy. Table 5: Respondent age at survey response Age Group (years) Number of Respondents Percent of Respondents (%) [65-70) 5088 32 [70-75) 5767 36 [75-80) 3383 21 [80-85) 1362 9 [85-90) 250 2 Table 6: Respondents’ self-reported sex at birth Self-Reported Sex at Birth Number of Respondents Percent of Respondents (%) Female 8461 53 Male 6975 44 No matching concept 269 2 Not male, not female, prefer not to answer, or skipped 143 1 Table 7: Respondents’ self-reported ethnicity Self-Reported Ethnicity Number of Respondents Percent of Respondents (%) Hispanic or Latino 633 4 Not Hispanic or Latino 14442 91 Prefer not to answer 34 0 Skipped 639 4 Neither 100 1 Table 8: Respondents’ self-reported race Self-Reported Race Number of Respondents Percent of Respondents (%) Asian 260 2 Black or African American 752 5 White 13347 84 Another single population 49 0 9 Self-Reported Race Number of Respondents Percent of Respondents (%) More than one population 129 1 I prefer not to answer 34 0 Skipped 639 4 None indicated 538 3 None of these 100 1 2. Additional simulation results Figure 1: Cumulative incidence functions for the Speciﬁed Heart Arrhythmias Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups: 25% any-available MA upcoding. Speciﬁed heart arrhythmias corresponds to HCC238, which does not have any competing events. For this HCC, 25% of any-available individuals in the MA group are upcoded and 5% of any-available individuals in the TM comparison group are upcoded. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 10 Figure 2: Cumulative incidence functions for the Speciﬁed", "quality": {"lang": "en", "gibberish_score": 0.0051, "char_len": 2158, "word_len": 350}, "dedupe": {"exact_hash": "808e0ff20ac95fd6804a7d9ee2491568deaf35a40e046d6c05629a41dd82e14c", "simhash64": 10645769548966248838}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:0b4ecde07fdce8b45f72bedf8d0eb5673325589f8fb695df66934dfc5f86d909", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 32, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "of any-available individuals in the MA group are upcoded and 5% of any-available individuals in the TM comparison group are upcoded. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 10 Figure 2: Cumulative incidence functions for the Speciﬁed Heart Arrhythmias Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups: 30% any-available MA upcoding. Speciﬁed heart arrhythmias corresponds to HCC238, which does not have any competing events. For this HCC, 30% of any-available individuals in the MA group are upcoded and 5% of any-available individuals in the TM comparison group are upcoded. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 11 Figure 3: Cumulative incidence functions for the ‘Dementia, Severe’ Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups: 20% lower severity MA upcoding. ‘Dementia, Severe’ corresponds to HCC125, which has lower severity HCCs ‘Dementia, Moderate’ (HCC126) and ‘Dementia, Mild or Unspeciﬁed’ (HCC127). 20% of any individuals in the MA group who were previously coded with either HCC126 or HCC127 are upcoded and 5% of individuals in the TM comparison group are upcoded similarly. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 12 Figure 4: Cumulative incidence functions for the ‘Dementia, Severe’ Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups: 25% lower severity MA upcoding. ‘Dementia, Severe’ corresponds to HCC125, which has lower severity HCCs ‘Dementia, Moderate’ (HCC126) and ‘Dementia, Mild or Unspeciﬁed’ (HCC127). 25% of any individuals in the MA group who were previously coded with either HCC126 or HCC127 are upcoded and 5% of individuals in the TM comparison group are upcoded similarly. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 13 Figure 5: Cumulative incidence functions for the ‘Dementia, Severe’ Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups:", "quality": {"lang": "en", "gibberish_score": 0.0117, "char_len": 2396, "word_len": 350}, "dedupe": {"exact_hash": "0745dad82c7199c9ae80e41c475e4ec6d259de7714e2e4aff4b44f8e9c1d02e2", "simhash64": 10654775992459233666}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:1b2fd5211976284c227e246ef76ebf69599e11abb44a6ab6dd42407a82a55f13", "doc_id": "sha256:54e5790f1ed6b5713cb7fe49496a414b26484e475035e5a6786dd8daf1b65f60", "chunk_index": 33, "source_type": "pdf", "source_uri": "data\\raw\\pdfs\\Time-to-Event Estimation with Unreliably Reported Events in Medicare Health Plan Payment.pdf", "page_start": null, "page_end": null, "text": "TM comparison group are upcoded similarly. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 13 Figure 5: Cumulative incidence functions for the ‘Dementia, Severe’ Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups: 30% lower severity MA upcoding. ‘Dementia, Severe’ corresponds to HCC125, which has lower severity HCCs ‘Dementia, Moderate’ (HCC126) and ‘Dementia, Mild or Unspeciﬁed’ (HCC127). 30% of any individuals in the MA group who were previously coded with either HCC126 or HCC127 are upcoded and 5% of individuals in the TM comparison group are upcoded similarly. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 14 Figure 6: Within-monitoring period ψ estimates for the ‘Dementia, Severe’ Hierarchical Condition Category (HCC) in simulated Medicare Advantage (MA) and Traditional Medicare (TM) groups. ‘Dementia, Severe’ corresponds to HCC125, which has lower severity HCCs ‘Dementia, Moderate’ (HCC126) and ‘Dementia, Mild or Unspeciﬁed’ (HCC127). For this HCC, individuals in the MA group who were previously coded with either HCC126 or HCC127 are upcoded to varying degrees and individuals in the TM comparison group are upcoded 5% similarly. Given the large sample size, conﬁdence intervals are very narrow and are therefore omitted as they cannot be distinguished visually. 3. Data availability statement The simulated data for this study can be generated using code in the project repository: https://github.com/StanfordHPDS/tte_estimation_medicare. Additional upcoding, undercoding, and baseline data can be simulated using the upcoding package (https://github .com/StanfordHPDS/upcoding). In line with program policies, non-summarized version 7 All of Us survey data used to derive baseline co-occurring HCCs can be accessed by authorized users via the All of Us Researcher Workbench only (https://www.researchallofus.org/datatools/workbench/) and requires Registered Tier access. Code that was used for this project in the Researcher Workbench is in the above project repository. References 1. Centers for Medicare and Medicaid Services. 2024 MA Advance Notice.; 2023. https: //www.cms.gov/ﬁles/document/2024-advance-notice-pdf.pdf 15 2. Centers for Medicare and Medicaid Services. 2024 model software/ICD-10 mappings, 2024 midyear/ﬁnal model software. Published online 2024. https://www.cms.gov/ medicare/health-plans/medicareadvtgspecratestats/risk-adjustors/2024-modelsoftware/icd-10-mappings 16", "quality": {"lang": "en", "gibberish_score": 0.0189, "char_len": 2645, "word_len": 350}, "dedupe": {"exact_hash": "425edddbe46bbe9debf6bb4baaa4f8c0e09150cdae3faf6a73d4b406b9f863c4", "simhash64": 11212918880890782082}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:54ab593e18d115ef4bddbdd59f77876f58fcec724e41fb8afdd35d4410d2fb93", "doc_id": "sha256:4474469adca16ab489a992ddcc453c5e06122768c66a78039289ceec793ae3b1", "chunk_index": 0, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Data_engineering", "page_start": null, "page_end": null, "text": "Data engineering Data engineering is a software engineering approach to the building of data systems, to enable the collection and usage of data. This data is usually used to enable subsequent analysis and data science, which often involves machine learning.[1][2] Making the data usable usually involves substantial computing and storage, as well as data processing. History [edit]Around the 1970s/1980s the term information engineering methodology (IEM) was created to describe database design and the use of software for data analysis and processing.[3] These techniques were intended to be used by database administrators (DBAs) and by systems analysts based upon an understanding of the operational processing needs of organizations for the 1980s. In particular, these techniques were meant to help bridge the gap between strategic business planning and information systems. A key early contributor (often called the \"father\" of information engineering methodology) was the Australian Clive Finkelstein, who wrote several articles about it between 1976 and 1980, and also co-authored an influential Savant Institute report on it with James Martin.[4][5][6] Over the next few years, Finkelstein continued work in a more business-driven direction, which was intended to address a rapidly changing business environment; Martin continued work in a more data processing-driven direction. From 1983 to 1987, Charles M. Richter, guided by Clive Finkelstein, played a significant role in revamping IEM as well as helping to design the IEM software product (user data), which helped automate IEM. In the early 2000s, the data and data tooling was generally held by the information technology (IT) teams in most companies.[7] Other teams then used data for their work (e.g. reporting), and there was usually little overlap in data skillset between these parts of the business. In the early 2010s, with the rise of the internet, the massive increase in data volumes, velocity, and variety led to the term big data to describe the data itself, and data-driven tech companies like Facebook and Airbnb started using the phrase data engineer.[3][7] Due to the new scale of the data, major firms like Google, Facebook, Amazon, Apple, Microsoft, and Netflix started to move away from traditional ETL and", "quality": {"lang": "en", "gibberish_score": 0.0004, "char_len": 2291, "word_len": 350}, "dedupe": {"exact_hash": "b54788ab5ce462a704f4403351b17ee1e9d1fc3b4a9d573efd786a01abd4d01c", "simhash64": 10491784388571137286}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d02f19f66627f54a1a5e1228a5c6a3c6b3fb9e7e0fa1aec1395981a4714860a1", "doc_id": "sha256:4474469adca16ab489a992ddcc453c5e06122768c66a78039289ceec793ae3b1", "chunk_index": 1, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Data_engineering", "page_start": null, "page_end": null, "text": "to the term big data to describe the data itself, and data-driven tech companies like Facebook and Airbnb started using the phrase data engineer.[3][7] Due to the new scale of the data, major firms like Google, Facebook, Amazon, Apple, Microsoft, and Netflix started to move away from traditional ETL and storage techniques. They started creating data engineering, a type of software engineering focused on data, and in particular infrastructure, warehousing, data protection, cybersecurity, mining, modelling, processing, and metadata management.[3][7] This change in approach was particularly focused on cloud computing.[7] Data started to be handled and used by many parts of the business, such as sales and marketing, and not just IT.[7] Tools [edit]Compute [edit]High-performance computing is critical for the processing and analysis of data. One particularly widespread approach to computing for data engineering is dataflow programming, in which the computation is represented as a directed graph (dataflow graph); nodes are the operations, and edges represent the flow of data.[8] Popular implementations include Apache Spark, and the deep learning specific TensorFlow.[8][9][10] More recent implementations, such as Differential/Timely Dataflow, have used incremental computing for much more efficient data processing.[8][11][12] Storage [edit]Data is stored in a variety of ways, one of the key deciding factors is in how the data will be used. Data engineers optimize data storage and processing systems to reduce costs. They use data compression, partitioning, and archiving. Databases [edit]If the data is structured and some form of online transaction processing is required, then databases are generally used.[13] Originally mostly relational databases were used, with strong ACID transaction correctness guarantees; most relational databases use SQL for their queries. However, with the growth of data in the 2010s, NoSQL databases have also become popular since they horizontally scaled more easily than relational databases by giving up the ACID transaction guarantees, as well as reducing the object-relational impedance mismatch.[14] More recently, NewSQL databases — which attempt to allow horizontal scaling while retaining ACID guarantees — have become popular.[15][16][17][18] Data warehouses [edit]If the data is structured and online analytical processing is required (but not online transaction processing), then data warehouses are a main", "quality": {"lang": "en", "gibberish_score": 0.0012, "char_len": 2466, "word_len": 350}, "dedupe": {"exact_hash": "501fff563a24475192cecd13ead583464c977dad4f33852b429a01aea510d0db", "simhash64": 11217989828484567446}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:bfba67129e2c8275161a349187cff00e4d5fe4e97746a8814a71cedb22f2edcb", "doc_id": "sha256:4474469adca16ab489a992ddcc453c5e06122768c66a78039289ceec793ae3b1", "chunk_index": 2, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Data_engineering", "page_start": null, "page_end": null, "text": "well as reducing the object-relational impedance mismatch.[14] More recently, NewSQL databases — which attempt to allow horizontal scaling while retaining ACID guarantees — have become popular.[15][16][17][18] Data warehouses [edit]If the data is structured and online analytical processing is required (but not online transaction processing), then data warehouses are a main choice.[19] They enable data analysis, mining, and artificial intelligence on a much larger scale than databases can allow,[19] and indeed data often flow from databases into data warehouses.[20] Business analysts, data engineers, and data scientists can access data warehouses using tools such as SQL or business intelligence software.[20] Data lakes [edit]A data lake is a centralized repository for storing, processing, and securing large volumes of data. A data lake can contain structured data from relational databases, semi-structured data, unstructured data, and binary data. A data lake can be created on premises or in a cloud-based environment using the services from public cloud vendors such as Amazon, Microsoft, or Google. Files [edit]If the data is less structured, then often they are just stored as files. There are several options: File systems represent data hierarchically in nested folders.[21] - Block storage splits data into regularly sized chunks;[21] this often matches up with (virtual) hard drives or solid state drives. - Object storage manages data using metadata;[21] often each file is assigned a key such as a UUID.[22] - Management [edit]The number and variety of different data processes and storage locations can become overwhelming for users. This inspired the usage of a workflow management system (e.g. Airflow) to allow the data tasks to be specified, created, and monitored.[23] The tasks are often specified as a directed acyclic graph (DAG).[23] Lifecycle [edit]Business planning [edit]Business objectives that executives set for what's to come are characterized in key business plans, with their more noteworthy definition in tactical business plans and implementation in operational business plans. Most businesses today recognize the fundamental need to grow a business plan that follows this strategy. It is often difficult to implement these plans because of the lack of transparency at the tactical and operational degrees of organizations. This kind of", "quality": {"lang": "en", "gibberish_score": 0.0008, "char_len": 2379, "word_len": 350}, "dedupe": {"exact_hash": "511c6f04dad8069465b8c6e4b71af5fabd3122f223095dac7138675637969fa9", "simhash64": 10053809256089820566}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4e2d3702651ccb3f802e8942e013008b8bc0cf7832afcd6066b8e953801b2329", "doc_id": "sha256:4474469adca16ab489a992ddcc453c5e06122768c66a78039289ceec793ae3b1", "chunk_index": 3, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Data_engineering", "page_start": null, "page_end": null, "text": "tactical business plans and implementation in operational business plans. Most businesses today recognize the fundamental need to grow a business plan that follows this strategy. It is often difficult to implement these plans because of the lack of transparency at the tactical and operational degrees of organizations. This kind of planning requires feedback to allow for early correction of problems that are due to miscommunication and misinterpretation of the business plan. Systems design [edit]The design of data systems involves several components such as architecting data platforms, and designing data stores.[24][25] Data modeling [edit]Data modeling is the analysis and representation of data requirements for an organisation. It produces a data model—an abstract representation that organises business concepts and the relationships and constraints between them. The resulting artefacts guide communication between business and technical stakeholders and inform database design.[26][27] A common convention distinguishes three levels of models:[26] Conceptual model – a technology-independent view of the key business concepts and rules. - Logical model – a detailed representation in a chosen paradigm (most commonly the relational model) specifying entities, attributes, keys, and integrity constraints.[27] - Physical model – an implementation-oriented design describing tables, indexes, partitioning, and other operational considerations.[27] - Approaches include entity–relationship (ER) modeling for operational systems,[28] dimensional modeling for analytics and data warehousing,[29] and the use of UML class diagrams to express conceptual or logical models in general-purpose modeling tools.[30] Well-formed data models aim to improve data quality and interoperability by applying clear naming standards, normalisation, and integrity constraints.[27][26] Roles [edit]Data engineer [edit]A data engineer is a type of software engineer who creates big data ETL pipelines to manage the flow of data through the organization. This makes it possible to take huge amounts of data and translate it into insights.[31] They are focused on the production readiness of data and things like formats, resilience, scaling, and security. Data engineers usually hail from a software engineering background and are proficient in programming languages like Java, Python, Scala, and Rust.[32][3] They will be more familiar with databases, architecture, cloud computing, and Agile software development.[3] Data scientist [edit]Data scientists are more focused on", "quality": {"lang": "en", "gibberish_score": 0.002, "char_len": 2563, "word_len": 350}, "dedupe": {"exact_hash": "72aabcca3ebd81cac5faeddd063075bc6f5066f4af066cfb4eb5b60a005dc4c2", "simhash64": 11226922192229745798}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ec678a49e623db418b42bb3e26a7988604c6d0700a72e0520c19b4f4265cfaec", "doc_id": "sha256:4474469adca16ab489a992ddcc453c5e06122768c66a78039289ceec793ae3b1", "chunk_index": 4, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Data_engineering", "page_start": null, "page_end": null, "text": "things like formats, resilience, scaling, and security. Data engineers usually hail from a software engineering background and are proficient in programming languages like Java, Python, Scala, and Rust.[32][3] They will be more familiar with databases, architecture, cloud computing, and Agile software development.[3] Data scientist [edit]Data scientists are more focused on the analysis of the data, they will be more familiar with mathematics, algorithms, statistics, and machine learning.[3][33] See also [edit]References [edit]^ \"What is Data Engineering? | A Quick Glance of Data Engineering\". EDUCBA. January 5, 2020. Retrieved July 31, 2022. - ^ \"Introduction to Data Engineering\". Dremio. Retrieved July 31, 2022. - ^ a b c d e f Black, Nathan (January 15, 2020). \"What is Data Engineering and Why Is It So Important?\". QuantHub. Retrieved July 31, 2022. - ^ \"Information engineering,\" part 3, part 4, part 5, Part 6\" by Clive Finkelstein. In Computerworld, In depths, appendix. May 25 – June 15, 1981. - ^ Christopher Allen, Simon Chatwin, Catherine Creary (2003). Introduction to Relational Databases and SQL Programming. - ^ Terry Halpin, Tony Morgan (2010). Information Modeling and Relational Databases. p. 343 - ^ a b c d e Dodds, Eric. \"The History of the Data Engineering and the Megatrends\". Rudderstack. Retrieved July 31, 2022. - ^ a b c Schwarzkopf, Malte (March 7, 2020). \"The Remarkable Utility of Dataflow Computing\". ACM SIGOPS. Retrieved July 31, 2022. - ^ \"sparkpaper\" (PDF). Retrieved July 31, 2022. - ^ Abadi, Martin; Barham, Paul; Chen, Jianmin; Chen, Zhifeng; Davis, Andy; Dean, Jeffrey; Devin, Matthieu; Ghemawat, Sanjay; Irving, Geoffrey; Isard, Michael; Kudlur, Manjunath; Levenberg, Josh; Monga, Rajat; Moore, Sherry; Murray, Derek G.; Steiner, Benoit; Tucker, Paul; Vasudevan, Vijay; Warden, Pete; Wicke, Martin; Yu, Yuan; Zheng, Xiaoqiang (2016). \"TensorFlow: A system for large-scale machine learning\". 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). pp. 265–283. Retrieved July 31, 2022. - ^ McSherry, Frank; Murray, Derek; Isaacs, Rebecca; Isard, Michael (January 5, 2013). \"Differential dataflow\". Microsoft. Retrieved July 31, 2022. - ^ \"Differential Dataflow\". Timely Dataflow. July 30, 2022. Retrieved July 31, 2022. - ^ \"Lecture Notes | Database Systems", "quality": {"lang": "en", "gibberish_score": 0.0082, "char_len": 2330, "word_len": 350}, "dedupe": {"exact_hash": "bd827fdfc262034ff32d8e8638eeb74cf2c8be25f4b8f7eab4466c3686458754", "simhash64": 13956248701967533443}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a285f423c02ede0ecfec5f425d2d5aae9fd11ba0d237cfa5825ca5285399e79a", "doc_id": "sha256:4474469adca16ab489a992ddcc453c5e06122768c66a78039289ceec793ae3b1", "chunk_index": 5, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Data_engineering", "page_start": null, "page_end": null, "text": "and Implementation (OSDI 16). pp. 265–283. Retrieved July 31, 2022. - ^ McSherry, Frank; Murray, Derek; Isaacs, Rebecca; Isard, Michael (January 5, 2013). \"Differential dataflow\". Microsoft. Retrieved July 31, 2022. - ^ \"Differential Dataflow\". Timely Dataflow. July 30, 2022. Retrieved July 31, 2022. - ^ \"Lecture Notes | Database Systems | Electrical Engineering and Computer Science | MIT OpenCourseWare\". ocw.mit.edu. Retrieved July 31, 2022. - ^ Leavitt, Neal (February 2010). \"Will NoSQL Databases Live Up to Their Promise?\". Computer. 43 (2): 12–14. doi:10.1109/MC.2010.58. - ^ Aslett, Matthew (2011). \"How Will The Database Incumbents Respond To NoSQL And NewSQL?\" (PDF). 451 Group (published April 4, 2011). Retrieved February 22, 2020. - ^ Pavlo, Andrew; Aslett, Matthew (2016). \"What's Really New with NewSQL?\" (PDF). SIGMOD Record. Retrieved February 22, 2020. - ^ Stonebraker, Michael (June 16, 2011). \"NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps\". Communications of the ACM Blog. Retrieved February 22, 2020. - ^ Hoff, Todd (September 24, 2012). \"Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In\". Retrieved February 22, 2020. - ^ a b \"What is a Data Warehouse?\". www.ibm.com. Retrieved July 31, 2022. - ^ a b \"What is a Data Warehouse? | Key Concepts | Amazon Web Services\". Amazon Web Services, Inc. Retrieved July 31, 2022. - ^ a b c \"File storage, block storage, or object storage?\". www.redhat.com. Retrieved July 31, 2022. - ^ \"Cloud Object Storage – Amazon S3 – Amazon Web Services\". Amazon Web Services, Inc. Retrieved July 31, 2022. - ^ a b \"Home\". Apache Airflow. Retrieved July 31, 2022. - ^ \"Introduction to Data Engineering\". Coursera. Retrieved July 31, 2022. - ^ Finkelstein, Clive. What are The Phases of Information Engineering. - ^ a b c Simsion, Graeme; Witt, Graham (2015). Data Modeling Essentials (4th ed.). Morgan Kaufmann. ISBN 9780128002025. - ^ a b c d Date, C. J. (2004). An Introduction to Database Systems (8th ed.). Addison-Wesley. ISBN 9780321197849. - ^ Chen, Peter P. (1976). \"The Entity–Relationship Model—Toward a Unified View of Data\". ACM Transactions on Database Systems. 1 (1): 9–36. doi:10.1145/320434.320440. -", "quality": {"lang": "en", "gibberish_score": 0.0172, "char_len": 2209, "word_len": 350}, "dedupe": {"exact_hash": "d9b9200a9aaa31e7d854f1b77b747f7d91a3edd9400fbd56db6d351feddb2f19", "simhash64": 10069636520400979203}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:693ab6c791df4bc1e9accd9ff1dfcb05e2c1f65ce1a5e31f329d511e74d2fa76", "doc_id": "sha256:4474469adca16ab489a992ddcc453c5e06122768c66a78039289ceec793ae3b1", "chunk_index": 6, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Data_engineering", "page_start": null, "page_end": null, "text": "(4th ed.). Morgan Kaufmann. ISBN 9780128002025. - ^ a b c d Date, C. J. (2004). An Introduction to Database Systems (8th ed.). Addison-Wesley. ISBN 9780321197849. - ^ Chen, Peter P. (1976). \"The Entity–Relationship Model—Toward a Unified View of Data\". ACM Transactions on Database Systems. 1 (1): 9–36. doi:10.1145/320434.320440. - ^ Kimball, Ralph; Ross, Margy (2013). The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling (3rd ed.). Wiley. ISBN 9781118530801. - ^ Unified Modeling Language (UML) Version 2.5.1 (Report). Object Management Group. 2017. - ^ Tamir, Mike; Miller, Steven; Gagliardi, Alessandro (December 11, 2015). The Data Engineer (Report). SSRN 2762013. - ^ \"Data Engineer vs. Data Scientist\". Springboard Blog. February 7, 2019. Retrieved March 14, 2021. - ^ \"What is Data Science and Why it's Important\". Edureka. January 5, 2017. - Further reading [edit]Hares, John S. (1992). Information Engineering for the Advanced Practitioner. Wiley. ISBN 978-0-471-92810-2. - Finkelstein, Clive (1989). An Introduction to Information Engineering: From Strategic Planning to Information Systems. Addison-Wesley. ISBN 978-0-201-41654-1. - Finkelstein, Clive (1992). Information Engineering: Strategic Systems Development. Addison-Wesley. ISBN 978-0-201-50988-5. - Ian Macdonald (1986). \"Information engineering\". in: Information Systems Design Methodologies. T.W. Olle et al. (ed.). North-Holland. - Ian Macdonald (1988). \"Automating the Information engineering methodology with the Information Engineering Facility\". In: Computerized Assistance during the Information Systems Life Cycle. T.W. Olle et al. (ed.). North-Holland. - James Martin and Clive Finkelstein. (1981). Information engineering. Technical Report (2 volumes), Savant Institute, Carnforth, Lancs, UK. - James Martin (1989). Information engineering. (3 volumes), Prentice-Hall Inc. - Finkelstein, Clive (2006). Enterprise Architecture for Integration: Rapid Delivery Methods and Technologies. Artech House. ISBN 978-1-58053-713-1. - Reis, Joe; Housley, Matt (2022). Fundamentals of Data Engineering. O'Reilly Media. ISBN 978-1-0981-0827-4. - External links [edit]The Complex Method IEM Archived July 20, 2019, at the Wayback Machine - Rapid Application Development - Enterprise Engineering and Rapid Delivery of Enterprise Architecture -", "quality": {"lang": "en", "gibberish_score": 0.0047, "char_len": 2331, "word_len": 311}, "dedupe": {"exact_hash": "1b4dd7e168e21d5d545c0c16562d9381e33324233347348d387ea15bfe012773", "simhash64": 413587073233917346}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:92cd30213224f7e9353697666f91c5f24c9919a09ed539d93e327860a53b7838", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 0, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "Information extraction Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. Typically, this involves processing human language texts by means of natural language processing (NLP).[1] Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction. Recent advances in NLP techniques have allowed for significantly improved performance compared to previous years.[2] An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: , - from an online news sentence such as: \"Yesterday, New York based Foo Inc. announced their acquisition of Bar Corp.\" - A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow automated reasoning about the logical form of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context. Information extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR)[3] has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of)", "quality": {"lang": "en", "gibberish_score": 0.0017, "char_len": 2321, "word_len": 350}, "dedupe": {"exact_hash": "463059fbe75522730ebc700ff60b3b60ff20911794876d69730a0304327c9c31", "simhash64": 11208982560538067078}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d07ee7292d8aebe2382be9b05cb8c377253693c603e4c7406ea4ae7a2e20ec3b", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 1, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to \"understand\" an attack article only enough to find data corresponding to the slots in this template. History [edit]Information extraction dates back to the late 1970s in the early days of NLP.[4] An early commercial system from the mid-1980s was JASPER built for Reuters by the Carnegie Group Inc with the aim of providing real-time financial news to financial traders.[5] Beginning in 1987, IE was spurred by a series of Message Understanding Conferences. MUC is a competition-based conference[6] that focused on the following domains: MUC-1 (1987), MUC-3 (1989): Naval operations messages. - MUC-3 (1991), MUC-4 (1992): Terrorism in Latin American countries. - MUC-5 (1993): Joint ventures and microelectronics domain. - MUC-6 (1995): News articles on management changes. - MUC-7 (1998): Satellite launch reports. - Considerable support came from the U.S. Defense Advanced Research Projects Agency (DARPA), who wished to automate mundane tasks performed by government analysts, such as scanning newspapers for possible links to terrorism.[citation needed] Present significance [edit]The present significance of IE pertains to the growing amount of information available in unstructured form. Tim Berners-Lee, inventor of the World Wide Web, refers to the existing Internet as the web of documents [7] and advocates that more of the content be made available as a web of data.[8] Until this transpires, the web largely consists of unstructured documents lacking semantic metadata. Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into relational form, or by marking-up with XML tags. An intelligent agent monitoring a news", "quality": {"lang": "en", "gibberish_score": 0.0, "char_len": 2264, "word_len": 350}, "dedupe": {"exact_hash": "442fce0e6498213a8db63a563471bcf92eb8041e39e7ddf7c7ff14b666da605c", "simhash64": 11213204753905729798}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:ea6cb70f5f3fa306e080e08238d4e6f7c273f1f697c00c869fe4e7751d6d66fd", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 2, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "as a web of data.[8] Until this transpires, the web largely consists of unstructured documents lacking semantic metadata. Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into relational form, or by marking-up with XML tags. An intelligent agent monitoring a news data feed requires IE to transform unstructured data into something that can be reasoned with. A typical application of IE is to scan a set of documents written in a natural language and populate a database with the information extracted.[9] Tasks and subtasks [edit]Applying information extraction to text is linked to the problem of text simplification in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences. Typical IE tasks and subtasks include: Template filling: Extracting a fixed set of fields from a document, e.g. extract perpetrators, victims, time, etc. from a newspaper article about a terrorist attack. Event extraction: Given an input document, output zero or more event templates. For instance, a newspaper article might describe multiple terrorist attacks. - - Knowledge Base Population: Fill a database of facts given a set of documents. Typically the database is in the form of triplets, (entity 1, relation, entity 2), e.g. (Barack Obama, Spouse, Michelle Obama) Named entity recognition: recognition of known entity names (for people and organizations), place names, temporal expressions, and certain types of numerical expressions, by employing existing knowledge of the domain or information extracted from other sentences.[10] Typically the recognition task involves assigning a unique identifier to the extracted entity. A simpler task is named entity detection, which aims at detecting entities without having any existing knowledge about the entity instances. For example, in processing the sentence \"M. Smith likes fishing\", named entity detection would denote detecting that the phrase \"M. Smith\" does refer to a person, but without necessarily having (or using) any knowledge about a certain M. Smith who is (or, \"might be\") the specific person whom that sentence is talking about. - Coreference resolution: detection of", "quality": {"lang": "en", "gibberish_score": 0.0, "char_len": 2302, "word_len": 350}, "dedupe": {"exact_hash": "c22f090e4cde37f5ff543737e37cd6fa388c239ce64f7c31bbec0e2179c9370e", "simhash64": 11217708353533612422}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:d506ef2b152de1c72ee12214ce323f9e6b68dca67d37bd81b848b62c8c868187", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 3, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "likes fishing\", named entity detection would denote detecting that the phrase \"M. Smith\" does refer to a person, but without necessarily having (or using) any knowledge about a certain M. Smith who is (or, \"might be\") the specific person whom that sentence is talking about. - Coreference resolution: detection of coreference and anaphoric links between text entities. In IE tasks, this is typically restricted to finding links between previously extracted named entities. For example, \"International Business Machines\" and \"IBM\" refer to the same real-world entity. If we take the two sentences \"M. Smith likes fishing. But he doesn't like biking\", it would be beneficial to detect that \"he\" is referring to the previously detected person \"M. Smith\". - Relationship extraction: identification of relations between entities,[10] such as: PERSON works for ORGANIZATION (extracted from the sentence \"Bill works for IBM.\") - PERSON located in LOCATION (extracted from the sentence \"Bill is in France.\") - - - Semi-structured information extraction which may refer to any IE that tries to restore some kind of information structure that has been lost through publication, such as: Table extraction: finding and extracting tables from documents.[11][12] - Table information extraction : extracting information in structured manner from the tables. This task is more complex than table extraction, as table extraction is only the first step, while understanding the roles of the cells, rows, columns, linking the information inside the table and understanding the information presented in the table are additional tasks necessary for table information extraction.[11][13][14] - Comments extraction : extracting comments from the actual content of articles in order to restore the link between authors of each of the sentences - - Language and vocabulary analysis Terminology extraction: finding the relevant terms for a given corpus - - Audio extraction Template-based music extraction: finding relevant characteristic in an audio signal taken from a given repertoire; for instance [15] time indexes of occurrences of percussive sounds can be extracted in order to represent the essential rhythmic component of a music piece. - - Note that this list is not exhaustive and that the exact meaning of IE", "quality": {"lang": "en", "gibberish_score": 0.0, "char_len": 2294, "word_len": 350}, "dedupe": {"exact_hash": "002f5fc5714186891c59b0ac5c991adfe13e484ac683e0c294cf66ade378f617", "simhash64": 11207293435800830086}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:776793023e8c9f563410d87a51a1af05ecfb32a847c02b034082d5f71d5acb19", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 4, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "in an audio signal taken from a given repertoire; for instance [15] time indexes of occurrences of percussive sounds can be extracted in order to represent the essential rhythmic component of a music piece. - - Note that this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. Machine learning, statistical analysis and/or natural language processing are often used in IE. IE on non-text documents is becoming an increasingly interesting topic[when?] in research, and information extracted from multimedia documents can now[when?] be expressed in a high level structure as it is done on text. This naturally leads to the fusion of extracted information from multiple kinds of documents and sources. World Wide Web applications [edit]IE has been the focus of the MUC conferences. The proliferation of the Web, however, intensified the need for developing IE systems that help people to cope with the enormous amount of data that are available online. Systems that perform IE from online text should meet the requirements of low cost, flexibility in development and easy adaptation to new domains. MUC systems fail to meet those criteria. Moreover, linguistic analysis performed for unstructured text does not exploit the HTML/XML tags and the layout formats that are available in online texts. As a result, less linguistically intensive approaches have been developed for IE on the Web using wrappers, which are sets of highly accurate rules that extract a particular page's content. Manually developing wrappers has proved to be a time-consuming task, requiring a high level of expertise. Machine learning techniques, either supervised or unsupervised, have been used to induce such rules automatically. Wrappers typically handle highly structured collections of web pages, such as product catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web. Recent effort on adaptive information extraction motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail-", "quality": {"lang": "en", "gibberish_score": 0.0018, "char_len": 2250, "word_len": 350}, "dedupe": {"exact_hash": "ebc747870c98baf832f61f7e05b49ab8def2ac3b09fcde552c35282d7b57ebba", "simhash64": 11226434009091654022}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:491820ba7e91353200ac1da7270ddbe7c14a1dc7f2ceb65cd65d0736a9093979", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 5, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web. Recent effort on adaptive information extraction motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail- including mixed types. Such systems can exploit shallow natural language knowledge and thus can be also applied to less structured texts. A recent[when?] development is Visual Information Extraction,[16][17] that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code. Approaches [edit]The following standard approaches are now widely accepted: Hand-written regular expressions (or nested group of regular expressions) - Using classifiers Generative: naïve Bayes classifier - Discriminative: maximum entropy models such as Multinomial logistic regression - - Sequence models Recurrent neural network - Hidden Markov model - Conditional Markov model (CMM) / Maximum-entropy Markov model (MEMM) - Conditional random fields (CRF) are commonly used in conjunction with IE for tasks as varied as extracting information from research papers[18] to extracting navigation instructions.[19] - - Numerous other approaches exist for IE including hybrid approaches that combine some of the standard approaches previously listed. Free or open source software and services [edit]General Architecture for Text Engineering (GATE) is bundled with a free Information Extraction system - Apache OpenNLP is a Java machine learning toolkit for natural language processing - OpenCalais is an automated information extraction web service from Thomson Reuters (Free limited version) - Machine Learning for Language Toolkit (Mallet) is a Java-based package for a variety of natural language processing tasks, including information extraction. - DBpedia Spotlight is an open source tool in Java/Scala (and free web service) that can be used for named entity recognition and name resolution. - Natural Language Toolkit is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language - See", "quality": {"lang": "en", "gibberish_score": 0.0013, "char_len": 2375, "word_len": 350}, "dedupe": {"exact_hash": "e54a1168fdb77cac4355ee67ce5fa8e2a6ab65071acd25ea1f31c757a726858e", "simhash64": 11208714279700103558}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:372667c0adf1cab7b3f0fe4c9f0d96e450794b000ea3ce3b938defc1b78b9b63", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 6, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "DBpedia Spotlight is an open source tool in Java/Scala (and free web service) that can be used for named entity recognition and name resolution. - Natural Language Toolkit is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language - See also CRF implementations - See also [edit]Extraction - Data extraction - Keyword extraction - Knowledge extraction - Ontology extraction - Open information extraction - Table extraction - Terminology extraction - Mining, crawling, scraping, and recognition - Search and translation - General - Lists - References [edit]^ name=Kariampuzha2023 Kariampuzha, William; Alyea, Gioconda; Qu, Sue; Sanjak, Jaleal; Mathé, Ewy; Sid, Eric; Chatelaine, Haley; Yadaw, Arjun; Xu, Yanji; Zhu, Qian (2023). \"Precision information extraction for rare disease epidemiology at scale\". Journal of Translational Medicine. 21 (1): 157. doi:10.1186/s12967-023-04011-y. PMC 9972634. PMID 36855134. - ^ Christina Niklaus, Matthias Cetto, André Freitas, and Siegfried Handschuh. 2018. A Survey on Open Information Extraction. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3866–3878, Santa Fe, New Mexico, USA. Association for Computational Linguistics. - ^ FREITAG, DAYNE. \"Machine Learning for Information Extraction in Informal Domains\" (PDF). 2000 Kluwer Academic Publishers. Printed in the Netherlands. - ^ Cowie, Jim; Wilks, Yorick (1996). Information Extraction (PDF). p. 3. CiteSeerX 10.1.1.61.6480. S2CID 10237124. Archived from the original (PDF) on 2019-02-20. - ^ Andersen, Peggy M.; Hayes, Philip J.; Huettner, Alison K.; Schmandt, Linda M.; Nirenburg, Irene B.; Weinstein, Steven P. (1992). \"Automatic Extraction of Facts from Press Releases to Generate News Stories\". Proceedings of the third conference on Applied natural language processing -. pp. 170–177. CiteSeerX 10.1.1.14.7943. doi:10.3115/974499.974531. S2CID 14746386. - ^ Marco Costantino, Paolo Coletti, Information Extraction in Finance, Wit Press, 2008. ISBN 978-1-84564-146-7 - ^ \"Linked Data - The Story So Far\" (PDF). - ^ \"Tim Berners-Lee on the next Web\". Archived from the original on 2011-04-10. Retrieved 2010-03-27. - ^ R. K. Srihari, W. Li, C. Niu and T. Cornell,\"InfoXtract: A Customizable Intermediate Level Information Extraction Engine\",Journal of Natural Language Engineering,[dead link] Cambridge U. Press, 14(1), 2008, pp.33-69. - ^ a b Nguyen, Dat Quoc;", "quality": {"lang": "en", "gibberish_score": 0.0065, "char_len": 2477, "word_len": 350}, "dedupe": {"exact_hash": "f51073dc82b00afda434cd46bb4088f5cba2283f9e85fc5c1c9e18153c2046e6", "simhash64": 16113135167275106774}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:4ceb67bdfbb9cb297bef29f65b01ed81fb23c73145023de65616e9fa5c35993f", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 7, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "Berners-Lee on the next Web\". Archived from the original on 2011-04-10. Retrieved 2010-03-27. - ^ R. K. Srihari, W. Li, C. Niu and T. Cornell,\"InfoXtract: A Customizable Intermediate Level Information Extraction Engine\",Journal of Natural Language Engineering,[dead link] Cambridge U. Press, 14(1), 2008, pp.33-69. - ^ a b Nguyen, Dat Quoc; Verspoor, Karin (2019). \"End-to-end neural relation extraction using deep biaffine attention\". Proceedings of the 41st European Conference on Information Retrieval (ECIR). arXiv:1812.11275. doi:10.1007/978-3-030-15712-8_47. - ^ a b Milosevic N, Gregson C, Hernandez R, Nenadic G (February 2019). \"A framework for information extraction from tables in biomedical literature\". International Journal on Document Analysis and Recognition. 22 (1): 55–78. arXiv:1902.10031. Bibcode:2019arXiv190210031M. doi:10.1007/s10032-019-00317-0. S2CID 62880746. - ^ Milosevic, Nikola (2018). A multi-layered approach to information extraction from tables in biomedical documents (PDF) (PhD). University of Manchester. - ^ Milosevic N, Gregson C, Hernandez R, Nenadic G (June 2016). \"Disentangling the Structure of Tables in Scientific Literature\". Natural Language Processing and Information Systems. Lecture Notes in Computer Science. Vol. 21. pp. 162–174. doi:10.1007/978-3-319-41754-7_14. ISBN 978-3-319-41753-0. S2CID 19538141. - ^ Milosevic, Nikola (2018). A multi-layered approach to information extraction from tables in biomedical documents (PDF) (PhD). University of Manchester. - ^ A.Zils, F.Pachet, O.Delerue and F. Gouyon, Automatic Extraction of Drum Tracks from Polyphonic Music Signals Archived 2017-08-29 at the Wayback Machine, Proceedings of WedelMusic, Darmstadt, Germany, 2002. - ^ Chenthamarakshan, Vijil; Desphande, Prasad M; Krishnapuram, Raghu; Varadarajan, Ramakrishnan; Stolze, Knut (2015). \"WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Information Extraction\". arXiv:1506.08454 [cs.CL]. - ^ Baumgartner, Robert; Flesca, Sergio; Gottlob, Georg (2001). \"Visual Web Information Extraction with Lixto\". pp. 119–128. CiteSeerX 10.1.1.21.8236. - ^ Peng, F.; McCallum, A. (2006). \"Information extraction from research papers using conditional random fields☆\". Information Processing & Management. 42 (4): 963. doi:10.1016/j.ipm.2005.09.002. - ^ Shimizu, Nobuyuki; Hass, Andrew (2006). \"Extracting Frame-based Knowledge Representation from Route Instructions\" (PDF). Archived from the original (PDF) on 2006-09-01. Retrieved 2010-03-27. - External links [edit]Alias-I \"competition\" page A listing of academic toolkits and industrial toolkits for natural language information extraction. - Gabor Melli's page on IE Detailed description of the information extraction task.", "quality": {"lang": "en", "gibberish_score": 0.008, "char_len": 2733, "word_len": 350}, "dedupe": {"exact_hash": "98be35dac2dab84e9f0cb7f89a1e97f05bd13033af7c1858c2340c7fcd7e24fc", "simhash64": 1994349511381189254}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:70f0875a9f883bf6be45944466097975ce7ebbdc0aa1e4199d73878d6fe257cd", "doc_id": "sha256:c00dcfada8b368d6e4bf8e61b677d11491d8db52ee31ff0bdebb1b65002c3ed1", "chunk_index": 8, "source_type": "web", "source_uri": "https://en.wikipedia.org/wiki/Information_extraction", "page_start": null, "page_end": null, "text": "(2006). \"Extracting Frame-based Knowledge Representation from Route Instructions\" (PDF). Archived from the original (PDF) on 2006-09-01. Retrieved 2010-03-27. - External links [edit]Alias-I \"competition\" page A listing of academic toolkits and industrial toolkits for natural language information extraction. - Gabor Melli's page on IE Detailed description of the information extraction task. -", "quality": {"lang": "en", "gibberish_score": 0.0, "char_len": 394, "word_len": 51}, "dedupe": {"exact_hash": "9d81370eaf982f904a3a3349cf6c61d3b079973e68d03e2815e5d1987dae1fd4", "simhash64": 11208432804738667142}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:bbbea0383736bbcfecdf3910e10ac3cc40ce5e2f73893eb005385fbce72024d8", "doc_id": "sha256:e6d01d4148c5d39b88bfe415f8c79e247050649921869091eca4f4d9cb3c6d74", "chunk_index": 0, "source_type": "web", "source_uri": "https://www.w3schools.com/", "page_start": null, "page_end": null, "text": "Learn to Code With the world's largest web developer site. Not Sure Where To Begin? JavaScript The language for programming web pages Learn JavaScriptJavaScript Reference Get Certified PHP A web server programming language jQuery A JS library for developing web pages Java A programming language C++ A programming language W3.CSS A CSS framework for faster and better responsive web pages Bootstrap A CSS framework for designing better web pages Log in Create an account to track your progress, get your own website, and get access to more features and learning materials:Code Editor With our online code editor, you can edit code and view the result in your browser Try Frontend Editor (HTML/CSS/JS) Try Backend Editor (Python/PHP/Java/C..) W3Schools Spaces If you want to create your own website, check out W3Schools Spaces. No installation required - just open your browser and start coding: Learn More Code Game Help the Lynx collect pine cones! Web Templates Browse our selection of free responsive HTML Templates Kickstart your career Get certified by completing a course Get started", "quality": {"lang": "en", "gibberish_score": 0.0083, "char_len": 1089, "word_len": 172}, "dedupe": {"exact_hash": "06abd361210c53dcdf536e7dcbe8303e2b875b1219402fad043614c8cadbdc46", "simhash64": 11002149098370745732}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:6efc2e2c03045a4cc5618e3a1c4b7a7aae8ea05c4ac05468b0ad83f1f65b139f", "doc_id": "sha256:1e47fe667dbfa3ddb05d21aeeeacd9bcabb59775d10dc0b6c05fa24a66d55c51", "chunk_index": 0, "source_type": "web", "source_uri": "https://careers.microsoft.com/v2/global/en/home.html", "page_start": null, "page_end": null, "text": "https://uhf.microsoft.com/en-US/shell/xml/MSActionCenter?headerId=GlobalCareersPCSHeaderprod&footerid=CareersPCSFooterprod https://careers.microsoft.com/bin/msft/searchheader https://careers.microsoft.com/v2/global/en/sign-in.html https://careers.microsoft.com/v2/ca/fr/connexion.html https://cdn.careers.microsoft.com/aem/asset-manifest.json 654c6aaa25ad4751986d2b4fdcf3da6f-b7d405fc-078f-42fd-99cf-e21de3479349-7421 https://gcssupportuippe.azureedge.net/asset-manifest.json PROD https://uhf.microsoft.com/fr-CA/shell/xml/MSActionCenter?headerId=GlobalCareersPCSHeaderprod&footerid=CareersPCSFooterprod PROD https://apply.careers.microsoft.com/careers Empowered by Purpose. United by Mission When you join Microsoft, you’re not just stepping into a job— you’re stepping into a mission-driven company where purpose is at the heart of everything we do. Our mission—to empower every person and every organization on the planet to achieve more—isn’t just words. It’s our foundation. It’s what sets us apart. - Benefits Explore Microsoft’s world-class benefits designed to help you and your family live well. - Culture We will only achieve our mission if we live our culture, which starts with applying a growth mindset. - Diversity and inclusion We are committed to celebrating the diversity around us and its power to drive us forward together. - Hiring Tips Explore resources to help you prepare—Microsoft is here to support your interview journey. -", "quality": {"lang": "en", "gibberish_score": 0.0393, "char_len": 1449, "word_len": 141}, "dedupe": {"exact_hash": "68b5a70da969e6db86cc06ba5925e11e28e327f8ab249434a969b890c31e771e", "simhash64": 11240780232800966194}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
{"chunk_id": "sha256:a7cadbaa03c4d0cb19ebef56adbc52163d6acc1f1907378e9d61e521003cb27b", "doc_id": "sha256:f976f32fedbbf9d080910feefa4dc631694670963f125f42255a4f1e4d64ad77", "chunk_index": 0, "source_type": "web", "source_uri": "https://openrouter.ai/", "page_start": null, "page_end": null, "text": "30T Monthly Tokens 5M+ Global Users 60+ Active Providers 300+ Models Access all major models through a single, unified interface. OpenAI SDK works out of the box. anthropic/claude-opus-4.5 Reliable AI models via our distributed infrastructure. Fall back to other providers when one goes down. Keep costs in check without sacrificing speed. OpenRouter runs at the edge for minimal latency between your users and their inference. Protect your organization with fine grained data policies. Ensure prompts only go to the models and providers you trust. 250k+ apps using OpenRouter with 4.2M+ users globally 1 Create an account to get started. You can set up an org for your team later. 2 Credits can be used with any model or provider. Apr 1 $99 Mar 30 $10 3 Create an API key and start making requests. Fully OpenAI compatible. OPENROUTER_API_KEY •••••••••••••••• Discover AI models across our collection, from all major labs and providers. Explore token usage across models, labs, and public applications. Announcements are temporarily unavailable. Please check back later or view all announcements. How to generate license-safe synthetic data workflows for model specialization Today we're launching Response Healing, a new feature on OpenRouter that automatically fixes malformed JSON responses from LLMs before they reach your application Introducing the 2025 State of AI report, in partnership with a16z. The largest empirical look yet at how developers and organizations use language models in the real world.", "quality": {"lang": "en", "gibberish_score": 0.0172, "char_len": 1512, "word_len": 233}, "dedupe": {"exact_hash": "da77af62a5bfe362c445c8ba0f0686b9e304501a3106cf28aff06a3014abc22d", "simhash64": 11222154778564723411}, "timestamps": {"ingested_at": "2026-02-10T11:34:57+00:00"}}
